{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hopsworks Client","text":"<p>hopsworks is the python API for interacting with a Hopsworks cluster. Don't have a Hopsworks cluster just yet? Register an account on Hopsworks Serverless and get started for free. Once connected to your project, you can:</p> <ul> <li>Insert dataframes into the online or offline Store, create training datasets or serve real-time feature vectors in the Feature Store via the Feature Store API. Already have data somewhere you want to import, checkout our Storage Connectors documentation.</li> <li>register ML models in the model registry and deploy them via model serving via the Machine Learning API.</li> <li>manage environments, executions, kafka topics and more once you deploy your own Hopsworks cluster, either on-prem or in the cloud. Hopsworks is open-source and has its own Community Edition.</li> </ul> <p>Our tutorials cover a wide range of use cases and example of what you can build using Hopsworks.</p>"},{"location":"#getting-started-on-hopsworks","title":"Getting Started On Hopsworks","text":"<p>Once you created a project on Hopsworks Serverless and created a new Api Key, just use your favourite virtualenv and package manager to install the library:</p> <pre><code>pip install \"hopsworks[python]\"\n</code></pre> <p>Fire up a notebook and connect to your project, you will be prompted to enter your newly created API key:</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n</code></pre>"},{"location":"#feature-store-api","title":"Feature Store API","text":"<p>Access the Feature Store of your project to use as a central repository for your feature data. Use your favourite data engineering library (pandas, polars, Spark, etc...) to insert data into the Feature Store, create training datasets or serve real-time feature vectors. Want to predict likelyhood of e-scooter accidents in real-time? Here's how you can do it:</p> <pre><code>fs = project.get_feature_store()\n\n# Write to Feature Groups\nbike_ride_fg = fs.get_or_create_feature_group(\n  name=\"bike_rides\",\n  version=1,\n  primary_key=[\"ride_id\"],\n  event_time=\"activation_time\",\n  online_enabled=True,\n)\n\nfg.insert(bike_rides_df)\n\n# Read from Feature Views\nprofile_fg = fs.get_feature_group(\"user_profile\", version=1)\n\nbike_ride_fv = fs.get_or_create_feature_view(\n  name=\"bike_rides_view\",\n  version=1,\n  query=bike_ride_fg.select_except([\"ride_id\"]).join(profile_fg.select([\"age\", \"has_license\"]), on=\"user_id\")\n)\n\nbike_rides_Q1_2021_df = bike_ride_fv.get_batch_data(\n  start_date=\"2021-01-01\",\n  end_date=\"2021-01-31\"\n)\n\n# Create a training dataset\nversion, job = bike_ride_fv.create_train_test_split(\n    test_size=0.2,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n\n# Predict the probability of accident in real-time using new data + context data\nbike_ride_fv.init_serving()\n\nwhile True:\n    new_ride_vector = poll_ride_queue()\n    feature_vector = bike_ride_fv.get_online_feature_vector(\n      {\"user_id\": new_ride_vector[\"user_id\"]},\n      passed_features=new_ride_vector\n    )\n    accident_probability = model.predict(feature_vector)\n</code></pre> <p>The API enables interaction with the Hopsworks Feature Store. It makes creating new features, feature groups and training datasets easy.</p> <p>The API is environment independent and can be used in two modes:</p> <ul> <li> <p>Spark mode: For data engineering jobs that create and write features into the feature store or generate training datasets. It requires a Spark environment such as the one provided in the Hopsworks platform or Databricks. In Spark mode, HSFS provides bindings both for Python and JVM languages.</p> </li> <li> <p>Python mode: For data science jobs to explore the features available in the feature store, generate training datasets and feed them in a training pipeline. Python mode requires just a Python interpreter and can be used both in Hopsworks from Python Jobs/Jupyter Kernels, Amazon SageMaker or KubeFlow.</p> </li> </ul> <p>Scala API is also available, here is a short sample of it:</p> <pre><code>import com.logicalclocks.hsfs._\nval connection = HopsworksConnection.builder().build()\nval fs = connection.getFeatureStore();\nval attendances_features_fg = fs.getFeatureGroup(\"games_features\", 1);\nattendances_features_fg.show(1)\n</code></pre>"},{"location":"#machine-learning-api","title":"Machine Learning API","text":"<p>Or you can use the Machine Learning API to interact with the Hopsworks Model Registry and Model Serving. The API makes it easy to export, manage and deploy models. For example, to register models and deploy them for serving you can do:</p> <pre><code>mr = project.get_model_registry()\n# or\nms = connection.get_model_serving()\n\n# Create a new model:\nmodel = mr.tensorflow.create_model(name=\"mnist\",\n                                   version=1,\n                                   metrics={\"accuracy\": 0.94},\n                                   description=\"mnist model description\")\nmodel.save(\"/tmp/model_directory\") # or /tmp/model_file\n\n# Download a model:\nmodel = mr.get_model(\"mnist\", version=1)\nmodel_path = model.download()\n\n# Delete the model:\nmodel.delete()\n\n# Get the best-performing model\nbest_model = mr.get_best_model('mnist', 'accuracy', 'max')\n\n# Deploy the model:\ndeployment = model.deploy()\ndeployment.start()\n\n# Make predictions with a deployed model\ndata = { \"instances\": [ model.input_example ] }\npredictions = deployment.predict(data)\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Usage data is collected for improving quality of the library. It is turned on by default if the backend is Hopsworks Serverless. To turn it off, use one of the following ways: <pre><code># use environment variable\nimport os\nos.environ[\"ENABLE_HOPSWORKS_USAGE\"] = \"false\"\n\n# use `disable_usage_logging`\nimport hopsworks\nhopsworks.disable_usage_logging()\n</code></pre></p> <p>The corresponding source code is in <code>python/hopsworks_common/usage.py</code>.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Need more inspiration or want to learn more about the Hopsworks platform? Check out our tutorials.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Documentation is available at Hopsworks Documentation.</p>"},{"location":"#issues","title":"Issues","text":"<p>For general questions about the usage of Hopsworks and the Feature Store please open a topic on Hopsworks Community.</p> <p>Please report any issue using Github issue tracking and attach the client environment from the output below to your issue:</p> <pre><code>import hopsworks\nhopsworks.login()\nprint(hopsworks.get_sdk_info())\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>If you would like to contribute to this library, please see the Contribution Guidelines.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#python-development-setup","title":"Python development setup","text":"<ul> <li> <p>Fork and clone the repository</p> </li> <li> <p>Create a new Python environment with your favourite environment manager (e.g. virtualenv or conda) and Python 3.9 (newer versions will return a library conflict in <code>auto_doc.py</code>)</p> </li> <li> <p>Install repository in editable mode with development dependencies:</p> </li> </ul> <pre><code>cd python\npip install -e \".[dev]\"\n</code></pre> <ul> <li>Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The library uses pre-commit to ensure code-style and code formatting through ruff. Run the following commands from the <code>python</code> directory:</li> </ul> <pre><code>cd python\npip install --user pre-commit\npre-commit install\n</code></pre> <p>Afterwards, pre-commit will run whenever you commit.</p> <ul> <li>To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use <code>ruff</code>, or run it via the command line:</li> </ul> <pre><code># linting\nruff check python --fix\n# formatting\nruff format python\n</code></pre>"},{"location":"CONTRIBUTING/#python-documentation","title":"Python documentation","text":"<p>We follow a few best practices for writing the Python documentation:</p> <ol> <li>Use the Google docstring style:</li> </ol> <pre><code>\"\"\"[One Line Summary]\n\n[Extended Summary]\n\n[!!! example\n    import xyz\n]\n\n# Arguments\n    arg1: Type[, optional]. Description[, defaults to `default`]\n    arg2: Type[, optional]. Description[, defaults to `default`]\n\n# Returns\n    Type. Description.\n\n# Raises\n    Exception. Description.\n\"\"\"\n</code></pre> <p>If Python 3 type annotations are used, they are inserted automatically.</p> <ol> <li>Hopsworks entity engine methods (e.g. ExecutionEngine etc.) only require a single line docstring.</li> <li>Private REST API implementations (e.g. FeatureGroupApi etc.) should be fully documented with docstrings without defaults.</li> <li>Public API such as metadata objects and public REST API implementations should be fully documented with defaults.</li> </ol>"},{"location":"CONTRIBUTING/#setup-and-build-documentation","title":"Setup and Build Documentation","text":"<p>We use <code>mkdocs</code> together with <code>mike</code> (for versioning) to build the documentation and a plugin called <code>keras-autodoc</code> to auto generate Python API documentation from docstrings.</p> <p>Background about <code>mike</code>: <code>mike</code> builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, <code>mike</code> maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like <code>dev</code> or <code>latest</code>, to indicate stable and unstable releases.</p> <ol> <li>Install Hopsworks with <code>requirements-docs.txt</code>:</li> </ol> <pre><code>pip install -r requirements-docs.txt\npip install -e \"python[dev]\"\n</code></pre> <ol> <li>To build the docs, first run the auto doc script:</li> </ol> <pre><code>python python/auto_doc.py\n</code></pre>"},{"location":"CONTRIBUTING/#option-1-build-only-current-version-of-docs","title":"Option 1: Build only current version of docs","text":"<ol> <li>Either build the docs, or serve them dynamically:</li> </ol> <p>Note: Links and pictures might not resolve properly later on when checking with this build.    The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and    therefore another level is added to all paths, e.g. <code>docs.hopsworks.ai/[version-or-alias]</code>.    Using relative links should not be affected by this, however, building the docs with version    (Option 2) is recommended.</p> <pre><code>mkdocs build\n# or\nmkdocs serve\n</code></pre>"},{"location":"CONTRIBUTING/#option-2-preferred-build-multi-version-doc-with-mike","title":"Option 2 (Preferred): Build multi-version doc with <code>mike</code>","text":""},{"location":"CONTRIBUTING/#versioning-on-docshopsworksai","title":"Versioning on docs.hopsworks.ai","text":"<p>On docs.hopsworks.ai we implement the following versioning scheme:</p> <ul> <li>current master branches (e.g. of hopsworks corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 4.0.0-SNAPSHOT [dev], where <code>dev</code> is an alias to indicate that this is an unstable version.</li> <li>the latest release: rendered with full current version, e.g. 3.8.0 [latest] with <code>latest</code> alias to indicate that this is the latest stable release.</li> <li>previous stable releases: rendered without alias, e.g. 3.4.4.</li> </ul>"},{"location":"CONTRIBUTING/#build-instructions","title":"Build Instructions","text":"<ol> <li> <p>For this you can either checkout and make a local copy of the <code>upstream/gh-pages</code> branch, where <code>mike</code> maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating:</p> <p>Building one branch:</p> <p>Checkout your dev branch with modified docs:</p> <pre><code>git checkout [dev-branch]\n</code></pre> <p>Generate API docs if necessary:</p> <pre><code>python auto_doc.py\n</code></pre> <p>Build docs with a version and alias</p> <pre><code>mike deploy [version] [alias] --update-alias\n\n# for example, if you are updating documentation to be merged to master,\n# which will become the new SNAPSHOT version:\nmike deploy 4.0.0-SNAPSHOT dev --update-alias\n\n# if you are updating docs of the latest stable release branch\nmike deploy [version] latest --update-alias\n\n# if you are updating docs of a previous stable release branch\nmike deploy [version]\n</code></pre> <p>If no gh-pages branch existed in your local repository, this will have created it.</p> <p>Important: If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows</p> <pre><code>mike set-default [version-or-alias]\n</code></pre> <p>You can now checkout the gh-pages branch and serve:</p> <pre><code>git checkout gh-pages\nmike serve\n</code></pre> <p>You can also list all available versions/aliases:</p> <pre><code>mike list\n</code></pre> <p>Delete and reset your local gh-pages branch:</p> <pre><code>mike delete --all\n\n# or delete single version\nmike delete [version-or-alias]\n</code></pre> </li> </ol>"},{"location":"CONTRIBUTING/#adding-new-api-documentation","title":"Adding new API documentation","text":"<p>To add new documentation for APIs, you need to add information about the method/class to document to the <code>auto_doc.py</code> script:</p> <pre><code>PAGES = {\n    \"connection.md\": [\n        \"hopsworks.connection.Connection.connection\"\n    ]\n    \"new_template.md\": [\n            \"module\",\n            \"xyz.asd\"\n    ]\n}\n</code></pre> <p>Now you can add a template markdown file to the <code>docs/templates</code> directory with the name you specified in the auto-doc script. The <code>new_template.md</code> file should contain a tag to identify the place at which the API documentation should be inserted:</p> <pre><code>## The XYZ package\n\n{{module}}\n\nSome extra content here.\n\n!!! example\n    ```python\n    import xyz\n    ```\n\n{{xyz.asd}}\n</code></pre> <p>Finally, run the <code>auto_doc.py</code> script, as decribed above, to update the documentation.</p> <p>For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation.</p>"},{"location":"generated/api/connection/","title":"Connection","text":"<p>[source]</p>"},{"location":"generated/api/connection/#connection_1","title":"connection","text":"<pre><code>Connection.connection(\n    host=None,\n    port=443,\n    project=None,\n    engine=None,\n    hostname_verification=False,\n    trust_store_path=None,\n    cert_folder=\"/tmp\",\n    api_key_file=None,\n    api_key_value=None,\n)\n</code></pre> <p>Connection factory method, accessible through <code>hopsworks.connection()</code>.</p> <p>This class provides convenience classmethods accessible from the <code>hopsworks</code>-module:</p> <p>Connection factory</p> <p>For convenience, <code>hopsworks</code> provides a factory method, accessible from the top level module, so you don't have to import the <code>Connection</code> class manually:</p> <pre><code>import hopsworks\nconn = hopsworks.connection()\n</code></pre> <p>Save API Key as File</p> <p>To get started quickly, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to Hopsworks.</p> <p>You can then connect by simply passing the path to the key file when instantiating a connection:</p> <pre><code>    import hopsworks\n    conn = hopsworks.connection(\n        'my_instance',                      # DNS of your Hopsworks instance\n        443,                                # Port to reach your Hopsworks instance, defaults to 443\n        api_key_file='hopsworks.key',       # The file containing the API key generated above\n        hostname_verification=True)         # Disable for self-signed certificates\n    )\n    project = conn.get_project(\"my_project\")\n</code></pre> <p>Clients in external clusters need to connect to the Hopsworks using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" scope to be able to access a project. For more information, see the integration guides.</p> <p>Arguments</p> <ul> <li>host <code>str | None</code>: The hostname of the Hopsworks instance in the form of <code>[UUID].cloud.hopsworks.ai</code>,     defaults to <code>None</code>. Do not use the url including <code>https://</code> when connecting     programatically.</li> <li>port <code>int</code>: The port on which the Hopsworks instance can be reached,     defaults to <code>443</code>.</li> <li>project <code>str | None</code>: The name of the project to connect to. When running on Hopsworks, this     defaults to the project from where the client is run from.     Defaults to <code>None</code>.</li> <li>engine <code>str | None</code>: Which engine to use, <code>\"spark\"</code>, <code>\"python\"</code> or <code>\"training\"</code>. Defaults to <code>None</code>,     which initializes the engine to Spark if the environment provides Spark, for     example on Hopsworks and Databricks, or falls back to Python if Spark is not     available, e.g. on local Python environments or AWS SageMaker. This option     allows you to override this behaviour. <code>\"training\"</code> engine is useful when only     feature store metadata is needed, for example training dataset location and label     information when Hopsworks training experiment is conducted.</li> <li>hostname_verification <code>bool</code>: Whether or not to verify Hopsworks' certificate, defaults     to <code>True</code>.</li> <li>trust_store_path <code>str | None</code>: Path on the file system containing the Hopsworks certificates,     defaults to <code>None</code>.</li> <li>cert_folder <code>str</code>: The directory to store retrieved HopsFS certificates, defaults to     <code>\"/tmp\"</code>. Only required when running without a Spark environment.</li> <li>api_key_file <code>str | None</code>: Path to a file containing the API Key, defaults to <code>None</code>.</li> <li>api_key_value <code>str | None</code>: API Key as string, if provided, <code>api_key_file</code> will be ignored,     however, this should be used with care, especially if the used notebook or     job script is accessible by multiple parties. Defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>Connection</code>. Connection handle to perform operations on a     Hopsworks project.</p>"},{"location":"generated/api/connection/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/connection/#api_key_file","title":"api_key_file","text":"<p>[source]</p>"},{"location":"generated/api/connection/#api_key_value","title":"api_key_value","text":"<p>[source]</p>"},{"location":"generated/api/connection/#cert_folder","title":"cert_folder","text":"<p>[source]</p>"},{"location":"generated/api/connection/#host","title":"host","text":"<p>[source]</p>"},{"location":"generated/api/connection/#hostname_verification","title":"hostname_verification","text":"<p>[source]</p>"},{"location":"generated/api/connection/#port","title":"port","text":"<p>[source]</p>"},{"location":"generated/api/connection/#project","title":"project","text":"<p>[source]</p>"},{"location":"generated/api/connection/#trust_store_path","title":"trust_store_path","text":""},{"location":"generated/api/connection/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/connection/#close","title":"close","text":"<pre><code>Connection.close()\n</code></pre> <p>Close a connection gracefully.</p> <p>This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker.</p> <p>Usage is optional.</p> <p>Example</p> <pre><code>import hopsworks\nconn = hopsworks.connection()\nconn.close()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/connection/#connect","title":"connect","text":"<pre><code>Connection.connect()\n</code></pre> <p>Instantiate the connection.</p> <p>Creating a <code>Connection</code> object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the <code>close()</code> method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call <code>connect()</code> again to reopen the connection.</p> <p>Example</p> <pre><code>import hopsworks\nconn = hopsworks.connection()\nconn.close()\nconn.connect()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/connection/#connection_2","title":"connection","text":"<pre><code>Connection.connection(\n    host=None,\n    port=443,\n    project=None,\n    engine=None,\n    hostname_verification=False,\n    trust_store_path=None,\n    cert_folder=\"/tmp\",\n    api_key_file=None,\n    api_key_value=None,\n)\n</code></pre> <p>Connection factory method, accessible through <code>hopsworks.connection()</code>.</p> <p>This class provides convenience classmethods accessible from the <code>hopsworks</code>-module:</p> <p>Connection factory</p> <p>For convenience, <code>hopsworks</code> provides a factory method, accessible from the top level module, so you don't have to import the <code>Connection</code> class manually:</p> <pre><code>import hopsworks\nconn = hopsworks.connection()\n</code></pre> <p>Save API Key as File</p> <p>To get started quickly, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to Hopsworks.</p> <p>You can then connect by simply passing the path to the key file when instantiating a connection:</p> <pre><code>    import hopsworks\n    conn = hopsworks.connection(\n        'my_instance',                      # DNS of your Hopsworks instance\n        443,                                # Port to reach your Hopsworks instance, defaults to 443\n        api_key_file='hopsworks.key',       # The file containing the API key generated above\n        hostname_verification=True)         # Disable for self-signed certificates\n    )\n    project = conn.get_project(\"my_project\")\n</code></pre> <p>Clients in external clusters need to connect to the Hopsworks using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" scope to be able to access a project. For more information, see the integration guides.</p> <p>Arguments</p> <ul> <li>host <code>str | None</code>: The hostname of the Hopsworks instance in the form of <code>[UUID].cloud.hopsworks.ai</code>,     defaults to <code>None</code>. Do not use the url including <code>https://</code> when connecting     programatically.</li> <li>port <code>int</code>: The port on which the Hopsworks instance can be reached,     defaults to <code>443</code>.</li> <li>project <code>str | None</code>: The name of the project to connect to. When running on Hopsworks, this     defaults to the project from where the client is run from.     Defaults to <code>None</code>.</li> <li>engine <code>str | None</code>: Which engine to use, <code>\"spark\"</code>, <code>\"python\"</code> or <code>\"training\"</code>. Defaults to <code>None</code>,     which initializes the engine to Spark if the environment provides Spark, for     example on Hopsworks and Databricks, or falls back to Python if Spark is not     available, e.g. on local Python environments or AWS SageMaker. This option     allows you to override this behaviour. <code>\"training\"</code> engine is useful when only     feature store metadata is needed, for example training dataset location and label     information when Hopsworks training experiment is conducted.</li> <li>hostname_verification <code>bool</code>: Whether or not to verify Hopsworks' certificate, defaults     to <code>True</code>.</li> <li>trust_store_path <code>str | None</code>: Path on the file system containing the Hopsworks certificates,     defaults to <code>None</code>.</li> <li>cert_folder <code>str</code>: The directory to store retrieved HopsFS certificates, defaults to     <code>\"/tmp\"</code>. Only required when running without a Spark environment.</li> <li>api_key_file <code>str | None</code>: Path to a file containing the API Key, defaults to <code>None</code>.</li> <li>api_key_value <code>str | None</code>: API Key as string, if provided, <code>api_key_file</code> will be ignored,     however, this should be used with care, especially if the used notebook or     job script is accessible by multiple parties. Defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>Connection</code>. Connection handle to perform operations on a     Hopsworks project.</p> <p>[source]</p>"},{"location":"generated/api/connection/#create_project","title":"create_project","text":"<pre><code>Connection.create_project(name, description=None, feature_store_topic=None)\n</code></pre> <p>Create a new project.</p> <p>Example for creating a new project</p> <p><pre><code>import hopsworks\n\nconnection = hopsworks.connection()\n\nconnection.create_project(\"my_hopsworks_project\", description=\"An example Hopsworks project\")\n</code></pre> Arguments</p> <ul> <li>name <code>str</code>: The name of the project.</li> <li>description <code>str</code>: optional description of the project</li> <li>feature_store_topic <code>str</code>: optional feature store topic name</li> </ul> <p>Returns</p> <p><code>Project</code>. A project handle object to perform operations on.</p> <p>[source]</p>"},{"location":"generated/api/connection/#get_feature_store","title":"get_feature_store","text":"<pre><code>Connection.get_feature_store(name=None)\n</code></pre> <p>Get a reference to a feature store to perform operations on.</p> <p>Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required.</p> <p>Arguments</p> <ul> <li>name <code>str | None</code>: The name of the feature store, defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>FeatureStore</code>. A feature store handle object to perform operations on.</p> <p>[source]</p>"},{"location":"generated/api/connection/#get_model_registry","title":"get_model_registry","text":"<pre><code>Connection.get_model_registry(project=None)\n</code></pre> <p>Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the <code>project</code> argument.</p> <p>Arguments</p> <ul> <li>project <code>str</code>: The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>ModelRegistry</code>. A model registry handle object to perform operations on.</p> <p>[source]</p>"},{"location":"generated/api/connection/#get_model_serving","title":"get_model_serving","text":"<pre><code>Connection.get_model_serving()\n</code></pre> <p>Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> <p>Returns</p> <p><code>ModelServing</code>. A model serving handle object to perform operations on.</p> <p>[source]</p>"},{"location":"generated/api/connection/#get_project","title":"get_project","text":"<pre><code>Connection.get_project(name=None)\n</code></pre> <p>Get an existing project.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: The name of the project.</li> </ul> <p>Returns</p> <p><code>Project</code>. A project handle object to perform operations on.</p> <p>[source]</p>"},{"location":"generated/api/connection/#get_projects","title":"get_projects","text":"<pre><code>Connection.get_projects()\n</code></pre> <p>Get all projects.</p> <p>Returns</p> <p><code>List[Project]</code>: List of Project objects</p> <p>[source]</p>"},{"location":"generated/api/connection/#get_secrets_api","title":"get_secrets_api","text":"<pre><code>Connection.get_secrets_api()\n</code></pre> <p>Get the secrets api.</p> <p>Returns</p> <p><code>SecretsApi</code>: The Secrets Api handle</p> <p>[source]</p>"},{"location":"generated/api/connection/#project_exists","title":"project_exists","text":"<pre><code>Connection.project_exists(name)\n</code></pre> <p>Check if a project exists.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: The name of the project.</li> </ul> <p>Returns</p> <p><code>bool</code>. True if project exists, otherwise False</p>"},{"location":"generated/api/datasets/","title":"Datasets API","text":""},{"location":"generated/api/datasets/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/api/datasets/#get_dataset_api","title":"get_dataset_api","text":"<pre><code>Project.get_dataset_api()\n</code></pre> <p>Get the dataset api for the project.</p> <p>Returns</p> <p><code>DatasetApi</code>: The Datasets Api handle</p>"},{"location":"generated/api/datasets/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/datasets/#copy","title":"copy","text":"<pre><code>DatasetApi.copy(source_path, destination_path, overwrite=False)\n</code></pre> <p>Copy a file or directory in the Hopsworks Filesystem.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\ndirectory_path = dataset_api.copy(\"Resources/myfile.txt\", \"Logs/myfile.txt\")\n</code></pre> Arguments</p> <ul> <li>source_path <code>str</code>: the source path to copy</li> <li>destination_path <code>str</code>: the destination path</li> <li>overwrite <code>bool</code>: overwrite destination if exists</li> </ul> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to perform the copy</li> </ul> <p>[source]</p>"},{"location":"generated/api/datasets/#download","title":"download","text":"<pre><code>DatasetApi.download(path, local_path=None, overwrite=False)\n</code></pre> <p>Download file from Hopsworks Filesystem to the current working directory.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\ndownloaded_file_path = dataset_api.download(\"Resources/my_local_file.txt\")\n</code></pre> Arguments</p> <ul> <li>path <code>str</code>: path in Hopsworks filesystem to the file</li> <li>local_path <code>str</code>: path where to download the file in the local filesystem</li> <li>overwrite <code>bool</code>: overwrite local file if exists</li> </ul> <p>Returns</p> <p><code>str</code>: Path to downloaded file</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to download the file</li> </ul> <p>[source]</p>"},{"location":"generated/api/datasets/#exists","title":"exists","text":"<pre><code>DatasetApi.exists(path)\n</code></pre> <p>Check if a file exists in the Hopsworks Filesystem.</p> <p>Arguments</p> <ul> <li>path <code>str</code>: path to check</li> </ul> <p>Returns</p> <p><code>bool</code>: True if exists, otherwise False</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to check existence for the path</li> </ul> <p>[source]</p>"},{"location":"generated/api/datasets/#list_files","title":"list_files","text":"<pre><code>DatasetApi.list_files(path, offset, limit)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/datasets/#mkdir","title":"mkdir","text":"<pre><code>DatasetApi.mkdir(path)\n</code></pre> <p>Create a directory in the Hopsworks Filesystem.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\ndirectory_path = dataset_api.mkdir(\"Resources/my_dir\")\n</code></pre> Arguments</p> <ul> <li>path <code>str</code>: path to directory</li> </ul> <p>Returns</p> <p><code>str</code>: Path to created directory</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to create the directory</li> </ul> <p>[source]</p>"},{"location":"generated/api/datasets/#move","title":"move","text":"<pre><code>DatasetApi.move(source_path, destination_path, overwrite=False)\n</code></pre> <p>Move a file or directory in the Hopsworks Filesystem.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\ndirectory_path = dataset_api.move(\"Resources/myfile.txt\", \"Logs/myfile.txt\")\n</code></pre> Arguments</p> <ul> <li>source_path <code>str</code>: the source path to move</li> <li>destination_path <code>str</code>: the destination path</li> <li>overwrite <code>bool</code>: overwrite destination if exists</li> </ul> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to perform the move</li> </ul> <p>[source]</p>"},{"location":"generated/api/datasets/#read_content","title":"read_content","text":"<pre><code>DatasetApi.read_content(path, dataset_type=\"DATASET\")\n</code></pre> <p>[source]</p>"},{"location":"generated/api/datasets/#remove","title":"remove","text":"<pre><code>DatasetApi.remove(path)\n</code></pre> <p>Remove a path in the Hopsworks Filesystem.</p> <p>Arguments</p> <ul> <li>path <code>str</code>: path to remove</li> </ul> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to remove the path</li> </ul> <p>[source]</p>"},{"location":"generated/api/datasets/#upload","title":"upload","text":"<pre><code>DatasetApi.upload(\n    local_path,\n    upload_path,\n    overwrite=False,\n    chunk_size=1048576,\n    simultaneous_uploads=3,\n    max_chunk_retries=1,\n    chunk_retry_interval=1,\n)\n</code></pre> <p>Upload a file to the Hopsworks filesystem.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ndataset_api = project.get_dataset_api()\n\nuploaded_file_path = dataset_api.upload(\"my_local_file.txt\", \"Resources\")\n</code></pre> Arguments</p> <ul> <li>local_path <code>str</code>: local path to file to upload</li> <li>upload_path <code>str</code>: path to directory where to upload the file in Hopsworks Filesystem</li> <li>overwrite <code>bool</code>: overwrite file if exists</li> <li>chunk_size: upload chunk size in bytes. Default 1048576 bytes</li> <li>simultaneous_uploads: number of simultaneous chunks to upload. Default 3</li> <li>max_chunk_retries: maximum retry for a chunk. Default is 1</li> <li>chunk_retry_interval: chunk retry interval in seconds. Default is 1sec</li> </ul> <p>Returns</p> <p><code>str</code>: Path to uploaded file</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to upload the file</li> </ul> <p>[source]</p>"},{"location":"generated/api/datasets/#upload_feature_group","title":"upload_feature_group","text":"<pre><code>DatasetApi.upload_feature_group(feature_group, path, dataframe)\n</code></pre>"},{"location":"generated/api/embedding_feature_api/","title":"EmbeddingFeature","text":"<p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#embeddingfeature_1","title":"EmbeddingFeature","text":"<pre><code>hsfs.embedding.EmbeddingFeature(\n    name=None,\n    dimension=None,\n    similarity_function_type=\"l2_norm\",\n    model=None,\n    feature_group=None,\n    embedding_index=None,\n)\n</code></pre> <p>Represents an embedding feature.</p> <p>Arguments</p> <ul> <li>name <code>str | None</code>: The name of the embedding feature.</li> <li>dimension <code>int | None</code>: The dimensionality of the embedding feature.</li> <li>similarity_function_type <code>hsfs.embedding.SimilarityFunctionType</code>: The type of similarity function used for the embedding feature.   Available functions are <code>L2</code>, <code>COSINE</code>, and <code>DOT_PRODUCT</code>.   (default is <code>SimilarityFunctionType.L2</code>).</li> <li>model: <code>hsml.model.Model</code> A Model in hsml.</li> <li>feature_group: The feature group object that contains the embedding feature.</li> <li>embedding_index: <code>EmbeddingIndex</code> The index for managing embedding features.</li> </ul>"},{"location":"generated/api/embedding_feature_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#dimenstion","title":"dimenstion","text":"<p>int: The dimensionality of the embedding feature.</p> <p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#embedding_index","title":"embedding_index","text":"<p>EmbeddingIndex: The index for managing embedding features.</p> <p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#feature_group","title":"feature_group","text":"<p>FeatureGroup: The feature group object that contains the embedding feature.</p> <p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#model","title":"model","text":"<p>hsml.model.Model: The Model in hsml.</p> <p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#name","title":"name","text":"<p>str: The name of the embedding feature.</p> <p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#similarity_function_type","title":"similarity_function_type","text":"<p>SimilarityFunctionType: The type of similarity function used for the embedding feature.</p>"},{"location":"generated/api/embedding_index_api/","title":"EmbeddingIndex","text":"<p>[source]</p>"},{"location":"generated/api/embedding_index_api/#embeddingindex_1","title":"EmbeddingIndex","text":"<pre><code>hsfs.embedding.EmbeddingIndex(index_name=None, features=None, col_prefix=None)\n</code></pre> <p>Represents an index for managing embedding features.</p> <p>Arguments</p> <ul> <li>index_name <code>str | None</code>: The name of the embedding index. The name of the project index is used if not provided.</li> <li>features <code>List[hsfs.embedding.EmbeddingFeature] | None</code>: A list of <code>EmbeddingFeature</code> objects for the features that     contain embeddings that should be indexed for similarity search.</li> <li>col_prefix <code>str | None</code>: The prefix to be added to column names when using project index.     It is managed by Hopsworks and should not be provided.</li> </ul> <p>Example</p> <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=256)\nembeddings = embedding_index.get_embeddings()\n</code></pre>"},{"location":"generated/api/embedding_index_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/embedding_index_api/#col_prefix","title":"col_prefix","text":"<p>str: The prefix to be added to column names.</p> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#feature_group","title":"feature_group","text":"<p>FeatureGroup: The feature group object that contains the embedding feature.</p> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#index_name","title":"index_name","text":"<p>str: The name of the embedding index.</p>"},{"location":"generated/api/embedding_index_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/embedding_index_api/#add_embedding","title":"add_embedding","text":"<pre><code>EmbeddingIndex.add_embedding(name, dimension, similarity_function_type=\"l2_norm\", model=None)\n</code></pre> <p>Adds a new embedding feature to the index.</p> <p>Example: <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=256)\n\n# Attach a hsml model to the embedding feature\nembedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=256, model=hsml_model)\n</code></pre></p> <p>Arguments</p> <ul> <li>name <code>str</code>: The name of the embedding feature.</li> <li>dimension <code>int</code>: The dimensionality of the embedding feature.</li> <li>similarity_function_type <code>hsfs.embedding.SimilarityFunctionType | None</code>: The type of similarity function to be used.</li> <li>model (hsml.model.Model, optional): The hsml model used to generate the embedding.     Defaults to None.</li> </ul> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#count","title":"count","text":"<pre><code>EmbeddingIndex.count(options=None)\n</code></pre> <p>Count the number of records in the feature group.</p> <p>Arguments</p> <ul> <li>options <code>map | None</code>: The options used for the request to the vector database.     The keys are attribute values of the <code>hsfs.core.opensearch.OpensearchRequestOption</code> class.</li> </ul> <p>Returns</p> <p>int: The number of records in the feature group.</p> <p>Raises:</p> <p>ValueError: If the feature group is not initialized. FeaturestoreException: If an error occurs during the count operation.</p> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#get_embedding","title":"get_embedding","text":"<pre><code>EmbeddingIndex.get_embedding(name)\n</code></pre> <p>Returns the <code>hsfs.embedding.EmbeddingFeature</code> object associated with the feature name.</p> <p>Arguments</p> <ul> <li>name (str): The name of the embedding feature.</li> </ul> <p>Returns</p> <p><code>hsfs.embedding.EmbeddingFeature</code> object</p> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#get_embeddings","title":"get_embeddings","text":"<pre><code>EmbeddingIndex.get_embeddings()\n</code></pre> <p>Returns the list of <code>hsfs.embedding.EmbeddingFeature</code> objects associated with the index.</p> <p>Returns</p> <p>A list of <code>hsfs.embedding.EmbeddingFeature</code> objects</p> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#json","title":"json","text":"<pre><code>EmbeddingIndex.json()\n</code></pre> <p>Serialize the EmbeddingIndex object to a JSON string.</p> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#to_dict","title":"to_dict","text":"<pre><code>EmbeddingIndex.to_dict()\n</code></pre> <p>Convert the EmbeddingIndex object to a dictionary.</p> <p>Returns:     dict: A dictionary representation of the EmbeddingIndex object.</p>"},{"location":"generated/api/environment/","title":"Environment API","text":""},{"location":"generated/api/environment/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/api/environment/#get_environment_api","title":"get_environment_api","text":"<pre><code>Project.get_environment_api()\n</code></pre> <p>Get the Python environment AP</p> <p>Returns</p> <p><code>EnvironmentApi</code>: The Python Environment Api handle</p>"},{"location":"generated/api/environment/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/environment/#create_environment","title":"create_environment","text":"<pre><code>EnvironmentApi.create_environment(\n    name, description=None, base_environment_name=\"python-feature-pipeline\", await_creation=True\n)\n</code></pre> <p>Create Python environment for the project</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nenv_api = project.get_environment_api()\n\nnew_env = env_api.create_environment(\"my_custom_environment\", base_environment_name=\"python-feature-pipeline\")\n</code></pre> Arguments</p> <ul> <li>name <code>str</code>: name of the environment</li> <li>base_environment_name <code>str | None</code>: the name of the environment to clone from</li> <li>await_creation <code>bool | None</code>: bool. If True the method returns only when the creation is finished. Default True</li> </ul> <p>Returns</p> <p><code>Environment</code>: The Environment object</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to create the environment</li> </ul>"},{"location":"generated/api/environment/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/environment/#get_environment","title":"get_environment","text":"<pre><code>EnvironmentApi.get_environment(name)\n</code></pre> <p>Get handle for the Python environment for the project</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nenv_api = project.get_environment_api()\n\nenv = env_api.get_environment(\"my_custom_environment\")\n</code></pre> Arguments</p> <ul> <li>name <code>str</code>: name of the environment</li> </ul> <p>Returns</p> <p><code>Environment</code>: The Environment object</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the environment</li> </ul>"},{"location":"generated/api/environment/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/environment/#delete","title":"delete","text":"<pre><code>Environment.delete()\n</code></pre> <p>Delete the environment</p> <p>Potentially dangerous operation</p> <p>This operation deletes the python environment.</p> <p>Raises</p> <p><code>RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/environment/#install_requirements","title":"install_requirements","text":"<pre><code>Environment.install_requirements(path, await_installation=True)\n</code></pre> <p>Install libraries specified in a requirements.txt file</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# Upload to Hopsworks\nds_api = project.get_dataset_api()\nrequirements_path = ds_api.upload(\"requirements.txt\", \"Resources\")\n\n# Install\nenv_api = project.get_environment_api()\nenv = env_api.get_environment(\"my_custom_environment\")\n\n\nenv.install_requirements(requirements_path)\n</code></pre> <p>Arguments</p> <ul> <li>path <code>str</code>: str. The path on Hopsworks where the requirements.txt file is located</li> <li>await_installation <code>bool | None</code>: bool. If True the method returns only when the installation is finished. Default True</li> </ul> <p>Returns</p> <p><code>Library</code>: The library object</p> <p>[source]</p>"},{"location":"generated/api/environment/#install_wheel","title":"install_wheel","text":"<pre><code>Environment.install_wheel(path, await_installation=True)\n</code></pre> <p>Install a python library packaged in a wheel file</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# Upload to Hopsworks\nds_api = project.get_dataset_api()\nwhl_path = ds_api.upload(\"matplotlib-3.1.3-cp38-cp38-manylinux1_x86_64.whl\", \"Resources\")\n\n# Install\nenv_api = project.get_environment_api()\nenv = env_api.get_environment(\"my_custom_environment\")\n\nenv.install_wheel(whl_path)\n</code></pre> <p>Arguments</p> <ul> <li>path <code>str</code>: str. The path on Hopsworks where the wheel file is located</li> <li>await_installation <code>bool | None</code>: bool. If True the method returns only when the installation finishes. Default True</li> </ul> <p>Returns</p> <p><code>Library</code>: The library object</p>"},{"location":"generated/api/executions/","title":"Executions API","text":""},{"location":"generated/api/executions/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/executions/#run","title":"run","text":"<pre><code>Job.run(args=None, await_termination=True)\n</code></pre> <p>Run the job.</p> <p>Run the job, by default awaiting its completion, with the option of passing runtime arguments.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instances\nfg = fs.get_or_create_feature_group(...)\n\n# insert in to feature group\njob, _ = fg.insert(df, write_options={\"start_offline_materialization\": False})\n\n# run job\nexecution = job.run()\n\n# True if job executed successfully\nprint(execution.success)\n\n# Download logs\nout_log_path, err_log_path = execution.download_logs()\n</code></pre> <p>Arguments</p> <ul> <li>args <code>str</code>: Optional runtime arguments for the job.</li> <li>await_termination <code>bool</code>: Identifies if the client should wait for the job to complete, defaults to True.</li> </ul> <p>Returns</p> <p><code>Execution</code>. The execution object for the submitted run.</p>"},{"location":"generated/api/executions/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/executions/#get_executions","title":"get_executions","text":"<pre><code>Job.get_executions()\n</code></pre> <p>Retrieves all executions for the job ordered by submission time.</p> <p>Returns</p> <p><code>List[Execution]</code></p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve executions.</p>"},{"location":"generated/api/executions/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/executions/#app_id","title":"app_id","text":"<p>Application id for the execution</p> <p>[source]</p>"},{"location":"generated/api/executions/#args","title":"args","text":"<p>Arguments set for the execution.</p> <p>[source]</p>"},{"location":"generated/api/executions/#duration","title":"duration","text":"<p>Duration in milliseconds the execution ran.</p> <p>[source]</p>"},{"location":"generated/api/executions/#final_status","title":"final_status","text":"<p>Final status of the execution. Can be UNDEFINED, SUCCEEDED, FAILED or KILLED.</p> <p>[source]</p>"},{"location":"generated/api/executions/#hdfs_user","title":"hdfs_user","text":"<p>Filesystem user for the execution.</p> <p>[source]</p>"},{"location":"generated/api/executions/#id","title":"id","text":"<p>Id of the execution</p> <p>[source]</p>"},{"location":"generated/api/executions/#job_name","title":"job_name","text":"<p>Name of the job the execution belongs to</p> <p>[source]</p>"},{"location":"generated/api/executions/#job_type","title":"job_type","text":"<p>Type of the job the execution belongs to</p> <p>[source]</p>"},{"location":"generated/api/executions/#progress","title":"progress","text":"<p>Progress of the execution.</p> <p>[source]</p>"},{"location":"generated/api/executions/#state","title":"state","text":"<p>Current state of the execution.</p> <p>Can be: <code>INITIALIZING</code>, <code>INITIALIZATION_FAILED</code>, <code>FINISHED</code>, <code>RUNNING</code>, <code>ACCEPTED</code>, <code>FAILED</code>, <code>KILLED</code>, <code>NEW</code>, <code>NEW_SAVING</code>, <code>SUBMITTED</code>, <code>AGGREGATING_LOGS</code>, <code>FRAMEWORK_FAILURE</code>, <code>STARTING_APP_MASTER</code>, <code>APP_MASTER_START_FAILED</code>, <code>GENERATING_SECURITY_MATERIAL</code>, or <code>CONVERTING_NOTEBOOK</code>.</p> <p>[source]</p>"},{"location":"generated/api/executions/#stderr_path","title":"stderr_path","text":"<p>Path in Hopsworks Filesystem to stderr log file</p> <p>[source]</p>"},{"location":"generated/api/executions/#stdout_path","title":"stdout_path","text":"<p>Path in Hopsworks Filesystem to stdout log file</p> <p>[source]</p>"},{"location":"generated/api/executions/#submission_time","title":"submission_time","text":"<p>Timestamp when the execution was submitted</p> <p>[source]</p>"},{"location":"generated/api/executions/#success","title":"success","text":"<p>Boolean to indicate if execution ran successfully or failed</p> <p>Returns</p> <p><code>bool</code>. True if execution ran successfully. False if execution failed or was killed.</p> <p>[source]</p>"},{"location":"generated/api/executions/#user","title":"user","text":"<p>User that submitted the execution.</p>"},{"location":"generated/api/executions/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/executions/#await_termination","title":"await_termination","text":"<pre><code>Execution.await_termination(timeout=None)\n</code></pre> <p>Wait until execution terminates.</p> <p>Arguments</p> <ul> <li>timeout <code>float | None</code>: the maximum waiting time in seconds, if <code>None</code> the waiting time is unbounded; defaults to <code>None</code>. Note: the actual waiting time may be bigger by approximately 3 seconds.</li> </ul> <p>Raises</p> <p><code>RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/executions/#delete","title":"delete","text":"<pre><code>Execution.delete()\n</code></pre> <p>Delete the execution</p> <p>Potentially dangerous operation</p> <p>This operation deletes the execution.</p> <p>Raises</p> <p><code>RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/executions/#download_logs","title":"download_logs","text":"<pre><code>Execution.download_logs(path=None)\n</code></pre> <p>Download stdout and stderr logs for the execution Example for downloading and printing the logs</p> <pre><code># Download logs\nout_log_path, err_log_path = execution.download_logs()\n\nout_fd = open(out_log_path, \"r\")\nprint(out_fd.read())\n\nerr_fd = open(err_log_path, \"r\")\nprint(err_fd.read())\n</code></pre> <p>Arguments</p> <ul> <li>path: path to download the logs. must be <code>str</code></li> </ul> <p>Returns</p> <p><code>str</code>. Path to downloaded log for stdout. <code>str</code>. Path to downloaded log for stderr.</p> <p>[source]</p>"},{"location":"generated/api/executions/#get_url","title":"get_url","text":"<pre><code>Execution.get_url()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/executions/#stop","title":"stop","text":"<pre><code>Execution.stop()\n</code></pre> <p>Stop the execution</p> <p>Potentially dangerous operation</p> <p>This operation stops the execution.</p> <p>Raises</p> <p><code>RestAPIError</code>.</p>"},{"location":"generated/api/expectation_api/","title":"Expectation","text":"<p>{{expectation}}</p>"},{"location":"generated/api/expectation_api/#properties","title":"Properties","text":"<p>{{expectation_properties}}</p>"},{"location":"generated/api/expectation_api/#methods","title":"Methods","text":"<p>{{expectation_methods}}</p>"},{"location":"generated/api/expectation_api/#creation","title":"Creation","text":"<p>{{expectation_create}}</p>"},{"location":"generated/api/expectation_api/#retrieval","title":"Retrieval","text":"<p>{{expectation_getall}}</p> <p>{{expectation_get}}</p>"},{"location":"generated/api/expectation_suite_api/","title":"Expectation Suite","text":"<p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#expectationsuite","title":"ExpectationSuite","text":"<pre><code>hsfs.expectation_suite.ExpectationSuite(\n    expectation_suite_name,\n    expectations,\n    meta,\n    id=None,\n    run_validation=True,\n    validation_ingestion_policy=\"always\",\n    feature_store_id=None,\n    feature_group_id=None,\n    href=None,\n    **kwargs\n)\n</code></pre> <p>Metadata object representing an feature validation expectation in the Feature Store.</p>"},{"location":"generated/api/expectation_suite_api/#creation-with-great-expectations","title":"Creation with Great Expectations","text":"<pre><code>import great_expectations as ge\n\nexpectation_suite = ge.core.ExpectationSuite(\n    \"new_expectation_suite\",\n    expectations=[\n        ge.core.ExpectationConfiguration(\n            expectation_type=\"expect_column_max_to_be_between\",\n            kwargs={\n                \"column\": \"feature\",\n                \"min_value\": -1,\n                \"max_value\": 1\n            }\n        )\n    ]\n)\n</code></pre>"},{"location":"generated/api/expectation_suite_api/#attach-to-feature-group","title":"Attach to Feature Group","text":"<p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#save_expectation_suite","title":"save_expectation_suite","text":"<pre><code>FeatureGroup.save_expectation_suite(\n    expectation_suite, run_validation=True, validation_ingestion_policy=\"always\", overwrite=False\n)\n</code></pre> <p>Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.save_expectation_suite(expectation_suite, run_validation=True)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | great_expectations.core.expectation_suite.ExpectationSuite</code>: The expectation suite to attach to the Feature Group.</li> <li>overwrite <code>bool</code>: If an Expectation Suite is already attached, overwrite it.     The new suite will have its own validation history, but former reports are preserved.</li> <li>run_validation <code>bool</code>: Set whether the expectation_suite will run on ingestion</li> <li>validation_ingestion_policy <code>Literal['always', 'strict']</code>: Set the policy for ingestion to the Feature Group.<ul> <li>\"STRICT\" only allows DataFrame passing validation to be inserted into Feature Group.</li> <li>\"ALWAYS\" always insert the DataFrame to the Feature Group, irrespective of overall validation result.</li> </ul> </li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p>"},{"location":"generated/api/expectation_suite_api/#single-expectation-api","title":"Single Expectation API","text":"<p>An API to edit the expectation list based on Great Expectations API.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#add_expectation","title":"add_expectation","text":"<pre><code>ExpectationSuite.add_expectation(expectation, ge_type=True)\n</code></pre> <p>Append an expectation to the local suite or in the backend if attached to a Feature Group.</p> <p>Example</p> <pre><code># check if the minimum value of specific column is within a range of 0 and 1\nexpectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_min_to_be_between\",\n        kwargs={\n            \"column\": \"foo_id\",\n            \"min_value\": 0,\n            \"max_value\": 1\n        }\n    )\n)\n\n# check if the length of specific column value is within a range of 3 and 10\nexpectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_value_lengths_to_be_between\",\n        kwargs={\n            \"column\": \"bar_name\",\n            \"min_value\": 3,\n            \"max_value\": 10\n        }\n    )\n)\n</code></pre> <p>Arguments</p> <ul> <li>expectation <code>hsfs.ge_expectation.GeExpectation | great_expectations.core.expectation_configuration.ExpectationConfiguration</code>: The new expectation object.</li> <li>ge_type <code>bool</code>: Whether to return native Great Expectations object or Hopsworks abstraction,     defaults to True if great_expectations is installed else false.</li> </ul> <p>Returns</p> <p>The new expectation attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#replace_expectation","title":"replace_expectation","text":"<pre><code>ExpectationSuite.replace_expectation(expectation, ge_type=True)\n</code></pre> <p>Update an expectation from the suite locally or from the backend if attached to a Feature Group.</p> <p>Example</p> <pre><code>updated_expectation = expectation_suite.replace_expectation(new_expectation_object)\n</code></pre> <p>Arguments</p> <ul> <li>expectation <code>hsfs.ge_expectation.GeExpectation | great_expectations.core.expectation_configuration.ExpectationConfiguration</code>: The updated expectation object. The meta field should contain an expectationId field.</li> <li>ge_type <code>bool</code>: Whether to return native Great Expectations object or Hopsworks abstraction,     defaults to True if great_expectations is installed else false.</li> </ul> <p>Returns</p> <p>The updated expectation attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#remove_expectation","title":"remove_expectation","text":"<pre><code>ExpectationSuite.remove_expectation(expectation_id=None)\n</code></pre> <p>Remove an expectation from the suite locally and from the backend if attached to a Feature Group.</p> <p>Example</p> <pre><code>expectation_suite.remove_expectation(expectation_id=123)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_id <code>int | None</code>: Id of the expectation to remove. The expectation will be deleted both locally and from the backend.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p>"},{"location":"generated/api/expectation_suite_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#data_asset_type","title":"data_asset_type","text":"<p>Data asset type of the expectation suite, not used by backend.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#expectation_suite_name","title":"expectation_suite_name","text":"<p>Name of the expectation suite.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#expectations","title":"expectations","text":"<p>List of expectations to run at validation.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#ge_cloud_id","title":"ge_cloud_id","text":"<p>ge_cloud_id of the expectation suite, not used by backend.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#ge_cloud_id_1","title":"ge_cloud_id","text":"<p>ge_cloud_id of the expectation suite, not used by backend.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#id","title":"id","text":"<p>Id of the expectation suite, set by backend.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#meta","title":"meta","text":"<p>Meta field of the expectation suite to store additional informations.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#run_validation","title":"run_validation","text":"<p>Boolean to determine whether or not the expectation suite shoudl run on ingestion.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#validation_ingestion_policy","title":"validation_ingestion_policy","text":"<p>Whether to ingest a df based on the validation result.</p> <p>\"strict\" : ingest df only if all expectations succeed, \"always\" : always ingest df, even if one or more expectations fail</p>"},{"location":"generated/api/expectation_suite_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#add_expectation_1","title":"add_expectation","text":"<pre><code>ExpectationSuite.add_expectation(expectation, ge_type=True)\n</code></pre> <p>Append an expectation to the local suite or in the backend if attached to a Feature Group.</p> <p>Example</p> <pre><code># check if the minimum value of specific column is within a range of 0 and 1\nexpectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_min_to_be_between\",\n        kwargs={\n            \"column\": \"foo_id\",\n            \"min_value\": 0,\n            \"max_value\": 1\n        }\n    )\n)\n\n# check if the length of specific column value is within a range of 3 and 10\nexpectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_value_lengths_to_be_between\",\n        kwargs={\n            \"column\": \"bar_name\",\n            \"min_value\": 3,\n            \"max_value\": 10\n        }\n    )\n)\n</code></pre> <p>Arguments</p> <ul> <li>expectation <code>hsfs.ge_expectation.GeExpectation | great_expectations.core.expectation_configuration.ExpectationConfiguration</code>: The new expectation object.</li> <li>ge_type <code>bool</code>: Whether to return native Great Expectations object or Hopsworks abstraction,     defaults to True if great_expectations is installed else false.</li> </ul> <p>Returns</p> <p>The new expectation attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#from_ge_type","title":"from_ge_type","text":"<pre><code>ExpectationSuite.from_ge_type(\n    ge_expectation_suite,\n    run_validation=True,\n    validation_ingestion_policy=\"ALWAYS\",\n    id=None,\n    feature_store_id=None,\n    feature_group_id=None,\n)\n</code></pre> <p>Used to create a Hopsworks Expectation Suite instance from a great_expectations instance.</p> <p>Arguments</p> <ul> <li>ge_expectation_suite <code>great_expectations.core.expectation_suite.ExpectationSuite</code>: great_expectations.core.ExpectationSuite     The great_expectations ExpectationSuite instance to convert to a Hopsworks ExpectationSuite.</li> <li>run_validation <code>bool</code>: bool     Whether to run validation on inserts when the expectation suite is attached.</li> <li>validation_ingestion_policy <code>Literal['ALWAYS', 'STRICT']</code>: str     The validation ingestion policy to use when the expectation suite is attached. Defaults to \"ALWAYS\".     Options are \"STRICT\" or \"ALWAYS\".</li> <li>id <code>int | None</code>: int     The id of the expectation suite in Hopsworks. If not provided, a new expectation suite will be created.</li> <li>feature_store_id <code>int | None</code>: int     The id of the feature store of the feature group to which the expectation suite belongs.</li> <li>feature_group_id <code>int | None</code>: int     The id of the feature group to which the expectation suite belongs.</li> </ul> <p>Returns</p> <p>Hopsworks Expectation Suite instance.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#from_response_json","title":"from_response_json","text":"<pre><code>ExpectationSuite.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#get_expectation","title":"get_expectation","text":"<pre><code>ExpectationSuite.get_expectation(expectation_id, ge_type=True)\n</code></pre> <p>Fetch expectation with expectation_id from the backend.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nexpectation_suite = fg.get_expectation_suite()\nselected_expectation = expectation_suite.get_expectation(expectation_id=123)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_id <code>int</code>: Id of the expectation to fetch from the backend.</li> <li>ge_type <code>bool</code>: Whether to return native Great Expectations object or Hopsworks abstraction,     defaults to True if great_expectations is installed else false.</li> </ul> <p>Returns</p> <p>The expectation with expectation_id registered in the backend.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#json","title":"json","text":"<pre><code>ExpectationSuite.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#remove_expectation_1","title":"remove_expectation","text":"<pre><code>ExpectationSuite.remove_expectation(expectation_id=None)\n</code></pre> <p>Remove an expectation from the suite locally and from the backend if attached to a Feature Group.</p> <p>Example</p> <pre><code>expectation_suite.remove_expectation(expectation_id=123)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_id <code>int | None</code>: Id of the expectation to remove. The expectation will be deleted both locally and from the backend.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#replace_expectation_1","title":"replace_expectation","text":"<pre><code>ExpectationSuite.replace_expectation(expectation, ge_type=True)\n</code></pre> <p>Update an expectation from the suite locally or from the backend if attached to a Feature Group.</p> <p>Example</p> <pre><code>updated_expectation = expectation_suite.replace_expectation(new_expectation_object)\n</code></pre> <p>Arguments</p> <ul> <li>expectation <code>hsfs.ge_expectation.GeExpectation | great_expectations.core.expectation_configuration.ExpectationConfiguration</code>: The updated expectation object. The meta field should contain an expectationId field.</li> <li>ge_type <code>bool</code>: Whether to return native Great Expectations object or Hopsworks abstraction,     defaults to True if great_expectations is installed else false.</li> </ul> <p>Returns</p> <p>The updated expectation attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#to_dict","title":"to_dict","text":"<pre><code>ExpectationSuite.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#to_ge_type","title":"to_ge_type","text":"<pre><code>ExpectationSuite.to_ge_type()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#to_json_dict","title":"to_json_dict","text":"<pre><code>ExpectationSuite.to_json_dict(decamelize=False)\n</code></pre>"},{"location":"generated/api/external_feature_group_api/","title":"ExternalFeatureGroup","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#externalfeaturegroup_1","title":"ExternalFeatureGroup","text":"<pre><code>hsfs.feature_group.ExternalFeatureGroup(\n    storage_connector,\n    query=None,\n    data_format=None,\n    path=None,\n    options=None,\n    name=None,\n    version=None,\n    description=None,\n    primary_key=None,\n    featurestore_id=None,\n    featurestore_name=None,\n    created=None,\n    creator=None,\n    id=None,\n    features=None,\n    location=None,\n    statistics_config=None,\n    event_time=None,\n    expectation_suite=None,\n    online_enabled=False,\n    href=None,\n    online_topic_name=None,\n    topic_name=None,\n    notification_topic_name=None,\n    spine=False,\n    deprecated=False,\n    embedding_index=None,\n    online_config=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/external_feature_group_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#create_external_feature_group","title":"create_external_feature_group","text":"<pre><code>FeatureStore.create_external_feature_group(\n    name,\n    storage_connector,\n    query=None,\n    data_format=None,\n    path=\"\",\n    options=None,\n    version=None,\n    description=\"\",\n    primary_key=None,\n    embedding_index=None,\n    features=None,\n    statistics_config=None,\n    event_time=None,\n    expectation_suite=None,\n    online_enabled=False,\n    topic_name=None,\n    notification_topic_name=None,\n    online_config=None,\n)\n</code></pre> <p>Create a external feature group metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fg = fs.create_external_feature_group(\n                    name=\"sales\",\n                    version=1,\n                    description=\"Physical shop sales features\",\n                    query=query,\n                    storage_connector=connector,\n                    primary_key=['ss_store_sk'],\n                    event_time='sale_date'\n                    )\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the <code>save()</code> method.</p> <p>You can enable online storage for external feature groups, however, the sync from the external storage to Hopsworks online storage needs to be done manually:</p> <pre><code>external_fg = fs.create_external_feature_group(\n            name=\"sales\",\n            version=1,\n            description=\"Physical shop sales features\",\n            query=query,\n            storage_connector=connector,\n            primary_key=['ss_store_sk'],\n            event_time='sale_date',\n            online_enabled=True,\n            online_config={'online_comments': ['NDB_TABLE=READ_BACKUP=1']}\n            )\nexternal_fg.save()\n\n# read from external storage and filter data to sync to online\ndf = external_fg.read().filter(external_fg.customer_status == \"active\")\n\n# insert to online storage\nexternal_fg.insert(df)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to create.</li> <li>storage_connector <code>hsfs.StorageConnector</code>: the storage connector used to establish connectivity     with the data source.</li> <li>query <code>str | None</code>: A string containing a SQL query valid for the target data source.     the query will be used to pull data from the data sources when the     feature group is used.</li> <li>data_format <code>str | None</code>: If the external feature groups refers to a directory with data,     the data format to use when reading it</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> <li>options <code>Dict[str, str] | None</code>: Additional options to be used by the engine when reading data from the     specified storage connector. For example, <code>{\"header\": True}</code> when reading     CSV files with column names in the first row.</li> <li>version <code>int | None</code>: Version of the external feature group to retrieve, defaults to <code>None</code> and     will create the feature group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the external feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the external feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame resulting by executing the provided query     against the data source.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this external feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li> <p>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> <ul> <li>__ online_enabled__: Define whether it should be possible to sync the feature group to the online feature store for low latency access, defaults to <code>False</code>.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | hsfs.feature_store.great_expectations.core.ExpectationSuite | None</code>: Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to <code>None</code>.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> <li>online_config <code>hsfs.online_config.OnlineConfig | Dict[str, Any] | None</code>: Optionally, define configuration which is used to configure online table.</li> </ul> </li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>. The external feature group metadata object.</p>"},{"location":"generated/api/external_feature_group_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_external_feature_group","title":"get_external_feature_group","text":"<pre><code>FeatureStore.get_external_feature_group(name, version=None)\n</code></pre> <p>Get a external feature group entity from the feature store.</p> <p>Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fg = fs.get_external_feature_group(\"external_fg_test\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to get.</li> <li>version <code>int</code>: Version of the external feature group to retrieve,     defaults to <code>None</code> and will return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>: The external feature group metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul>"},{"location":"generated/api/external_feature_group_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#avro_schema","title":"avro_schema","text":"<p>Avro schema representation of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#created","title":"created","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#creator","title":"creator","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#data_format","title":"data_format","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#deprecated","title":"deprecated","text":"<p>Setting if the feature group is deprecated.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#description","title":"description","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#embedding_index","title":"embedding_index","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#event_time","title":"event_time","text":"<p>Event time feature in the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#expectation_suite","title":"expectation_suite","text":"<p>Expectation Suite configuration object defining the settings for data validation of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#feature_store","title":"feature_store","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#feature_store_id","title":"feature_store_id","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#feature_store_name","title":"feature_store_name","text":"<p>Name of the feature store in which the feature group is located.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#features","title":"features","text":"<p>Feature Group schema (alias)</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#id","title":"id","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#location","title":"location","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#name","title":"name","text":"<p>Name of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#notification_topic_name","title":"notification_topic_name","text":"<p>The topic used for feature group notifications.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#online_enabled","title":"online_enabled","text":"<p>Setting if the feature group is available in online storage.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#options","title":"options","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#path","title":"path","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#primary_key","title":"primary_key","text":"<p>List of features building the primary key.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#query","title":"query","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#schema","title":"schema","text":"<p>Feature Group schema</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#statistics","title":"statistics","text":"<p>Get the latest computed statistics for the whole feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#statistics_config","title":"statistics_config","text":"<p>Statistics configuration object defining the settings for statistics computation of the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#storage_connector","title":"storage_connector","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#subject","title":"subject","text":"<p>Subject of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#topic_name","title":"topic_name","text":"<p>The topic used for feature group data ingestion.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#version","title":"version","text":"<p>Version number of the feature group.</p>"},{"location":"generated/api/external_feature_group_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#add_tag","title":"add_tag","text":"<pre><code>ExternalFeatureGroup.add_tag(name, value)\n</code></pre> <p>Attach a tag to a feature group.</p> <p>A tag consists of a  pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.add_tag(name=\"example_tag\", value=\"42\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value <code>Any</code>: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to add the tag.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#append_features","title":"append_features","text":"<pre><code>ExternalFeatureGroup.append_features(features)\n</code></pre> <p>Append features to the schema of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# define features to be inserted in the feature group\nfeatures = [\n    Feature(name=\"id\",type=\"int\",online_type=\"int\"),\n    Feature(name=\"name\",type=\"string\",online_type=\"varchar(20)\")\n]\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.append_features(features)\n</code></pre> <p>Safe append</p> <p>This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema.</p> <p>It is only possible to append features to a feature group. Removing features is considered a breaking change. Note that feature views built on top of this feature group will not read appended feature data. Create a new feature view based on an updated query via <code>fg.select</code> to include the new features.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.feature.Feature | List[hsfs.feature.Feature]</code>: Feature or list. A feature object or list thereof to append to     the schema of the feature group.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#check_deprecated","title":"check_deprecated","text":"<pre><code>ExternalFeatureGroup.check_deprecated()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#compute_statistics","title":"compute_statistics","text":"<pre><code>ExternalFeatureGroup.compute_statistics()\n</code></pre> <p>Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nstatistics_metadata = fg.compute_statistics()\n</code></pre> <p>Returns</p> <p><code>Statistics</code>. The statistics metadata object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. Unable to persist the statistics. <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#create_feature_monitoring","title":"create_feature_monitoring","text":"<pre><code>ExternalFeatureGroup.create_feature_monitoring(\n    name,\n    feature_name,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # Data inserted last week on the same day\n    time_offset=\"1w1d\",\n    window_length=\"1d\",\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str</code>: Name of the feature to monitor.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#create_statistics_monitoring","title":"create_statistics_monitoring","text":"<pre><code>ExternalFeatureGroup.create_statistics_monitoring(\n    name,\n    feature_name=None,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable statistics monitoring\nmy_config = fg.create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str | None</code>: Name of the feature to monitor. If not specified, statistics     will be computed for all features.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#delete","title":"delete","text":"<pre><code>ExternalFeatureGroup.delete()\n</code></pre> <p>Drop the entire feature group along with its feature data.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(\n        name='bitcoin_price',\n        version=1\n        )\n\n# delete the feature group\nfg.delete()\n</code></pre> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#delete_expectation_suite","title":"delete_expectation_suite","text":"<pre><code>ExternalFeatureGroup.delete_expectation_suite()\n</code></pre> <p>Delete the expectation suite attached to the Feature Group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.delete_expectation_suite()\n</code></pre> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#delete_tag","title":"delete_tag","text":"<pre><code>ExternalFeatureGroup.delete_tag(name)\n</code></pre> <p>Delete a tag attached to a feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.delete_tag(\"example_tag\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#filter","title":"filter","text":"<pre><code>ExternalFeatureGroup.filter(f)\n</code></pre> <p>Apply filter to the feature group.</p> <p>Selects all features and returns the resulting <code>Query</code> with the applied filter.</p> <p>Example</p> <pre><code>from hsfs.feature import Feature\n\n# connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.filter(Feature(\"weekly_sales\") &gt; 1000)\n</code></pre> <p>If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group:</p> <p>Example</p> <pre><code>fg.filter(fg.feature1 == 1).show(10)\n</code></pre> <p>Composite filters require parenthesis and symbols for logical operands (e.g. <code>&amp;</code>, <code>|</code>, ...):</p> <p>Example</p> <pre><code>fg.filter((fg.feature1 == 1) | (fg.feature2 &gt;= 2))\n</code></pre> <p>Arguments</p> <ul> <li>f <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic</code>: Filter object.</li> </ul> <p>Returns</p> <p><code>Query</code>. The query object with the applied filter.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#find_neighbors","title":"find_neighbors","text":"<pre><code>ExternalFeatureGroup.find_neighbors(embedding, col=None, k=10, filter=None, options=None)\n</code></pre> <p>Finds the nearest neighbors for a given embedding in the vector database.</p> <p>If <code>filter</code> is specified, or if embedding feature is stored in default project index, the number of results returned may be less than k. Try using a large value of k and extract the top k items from the results if needed.</p> <p>Arguments</p> <ul> <li>embedding <code>List[int | float]</code>: The target embedding for which neighbors are to be found.</li> <li>col <code>str | None</code>: The column name used to compute similarity score. Required only if there are multiple embeddings (optional).</li> <li>k <code>int | None</code>: The number of nearest neighbors to retrieve (default is 10).</li> <li>filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: A filter expression to restrict the search space (optional).</li> <li>options <code>dict | None</code>: The options used for the request to the vector database.     The keys are attribute values of the <code>hsfs.core.opensearch.OpensearchRequestOption</code> class.</li> </ul> <p>Returns</p> <p>A list of tuples representing the nearest neighbors. Each tuple contains: <code>(The similarity score, A list of feature values)</code></p> <p>Example</p> <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=3)\nfg = fs.create_feature_group(\n            name='air_quality',\n            embedding_index = embedding_index,\n            version=1,\n            primary_key=['id1'],\n            online_enabled=True,\n        )\nfg.insert(data)\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n)\n\n# apply filter\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n    filter=(fg.id1 &gt; 10) &amp; (fg.id1 &lt; 30)\n)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#from_response_json","title":"from_response_json","text":"<pre><code>ExternalFeatureGroup.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_all_statistics","title":"get_all_statistics","text":"<pre><code>ExternalFeatureGroup.get_all_statistics(computation_time=None, feature_names=None)\n</code></pre> <p>Returns all the statistics metadata computed before a specific time for the current feature group.</p> <p>If <code>computation_time</code> is <code>None</code>, all the statistics metadata are returned.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_statistics = fg.get_statistics(computation_time=None)\n</code></pre> <p>Arguments</p> <ul> <li>computation_time <code>str | int | float | datetime.datetime | datetime.date | None</code>: Date and time when statistics were computed. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. Statistics object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_all_validation_reports","title":"get_all_validation_reports","text":"<pre><code>ExternalFeatureGroup.get_all_validation_reports(ge_type=True)\n</code></pre> <p>Return the latest validation report attached to the feature group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nval_reports = fg.get_all_validation_reports()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code> if Great Expectations is installed,     else <code>False</code>.</li> </ul> <p>Returns</p> <p>Union[List[<code>ValidationReport</code>], <code>ValidationReport</code>]. All validation reports attached to the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_complex_features","title":"get_complex_features","text":"<pre><code>ExternalFeatureGroup.get_complex_features()\n</code></pre> <p>Returns the names of all features with a complex data type in this feature group.</p> <p>Example</p> <pre><code>complex_dtype_features = fg.get_complex_features()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_expectation_suite","title":"get_expectation_suite","text":"<pre><code>ExternalFeatureGroup.get_expectation_suite(ge_type=True)\n</code></pre> <p>Return the expectation suite attached to the feature group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nexp_suite = fg.get_expectation_suite()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code> if Great Expectations is installed,     else <code>False</code>.</li> </ul> <p>Returns</p> <p><code>ExpectationSuite</code>. The expectation suite attached to the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_feature","title":"get_feature","text":"<pre><code>ExternalFeatureGroup.get_feature(name)\n</code></pre> <p>Retrieve a <code>Feature</code> object from the schema of the feature group.</p> <p>There are several ways to access features of a feature group:</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# get Feature instanse\nfg.feature1\nfg[\"feature1\"]\nfg.get_feature(\"feature1\")\n</code></pre> <p>Note</p> <p>Attribute access to features works only for non-reserved names. For example features named <code>id</code> or <code>name</code> will not be accessible via <code>fg.name</code>, instead this will return the name of the feature group itself. Fall back on using the <code>get_feature</code> method.</p> <p>Arguments:</p> <p>name: The name of the feature to retrieve</p> <p>Returns:</p> <p>Feature: The feature object</p> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_feature_monitoring_configs","title":"get_feature_monitoring_configs","text":"<pre><code>ExternalFeatureGroup.get_feature_monitoring_configs(\n    name=None, feature_name=None, config_id=None\n)\n</code></pre> <p>Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch all feature monitoring configs attached to the feature group\nfm_configs = fg.get_feature_monitoring_configs()\n\n# fetch a single feature monitoring config by name\nfm_config = fg.get_feature_monitoring_configs(name=\"my_config\")\n\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fg.get_feature_monitoring_configs(feature_name=\"my_feature\")\n\n# fetch a single feature monitoring config with a given id\nfm_config = fg.get_feature_monitoring_configs(config_id=1)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: If provided fetch only the feature monitoring config with the given name.     Defaults to None.</li> <li>feature_name <code>str | None</code>: If provided, fetch only configs attached to a particular feature.     Defaults to None.</li> <li>config_id <code>int | None</code>: If provided, fetch only the feature monitoring config with the given id.     Defaults to None.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both name and feature_name are provided. - TypeError: if name or feature_name are not string or None.</p> <p>Return</p> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None]     A list of feature monitoring configs. If name provided,     returns either a single config or None if not found.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_feature_monitoring_history","title":"get_feature_monitoring_history","text":"<pre><code>ExternalFeatureGroup.get_feature_monitoring_history(\n    config_name=None, config_id=None, start_time=None, end_time=None, with_statistics=True\n)\n</code></pre> <p>Fetch feature monitoring history for a given feature monitoring config.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch feature monitoring history for a given feature monitoring config\nfm_history = fg.get_feature_monitoring_history(\n    config_name=\"my_config\",\n    start_time=\"2020-01-01\",\n)\n\n# fetch feature monitoring history for a given feature monitoring config id\nfm_history = fg.get_feature_monitoring_history(\n    config_id=1,\n    start_time=datetime.now() - timedelta(weeks=2),\n    end_time=datetime.now() - timedelta(weeks=1),\n    with_statistics=False,\n)\n</code></pre> <p>Arguments</p> <ul> <li>config_name <code>str | None</code>: The name of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>config_id <code>int | None</code>: The id of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>start_time <code>str | int | datetime.datetime | datetime.date | None</code>: The start date of the feature monitoring history to fetch.     Defaults to None.</li> <li>end_time <code>str | int | datetime.datetime | datetime.date | None</code>: The end date of the feature monitoring history to fetch.     Defaults to None.</li> <li>with_statistics <code>bool | None</code>: Whether to include statistics in the feature monitoring history.     Defaults to True. If False, only metadata about the monitoring will be fetched.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both config_name and config_id are provided. - TypeError: if config_name or config_id are not respectively string, int or None.</p> <p>Return</p> <p>List[<code>FeatureMonitoringResult</code>]     A list of feature monitoring results containing the monitoring metadata     as well as the computed statistics for the detection and reference window     if requested.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_fg_name","title":"get_fg_name","text":"<pre><code>ExternalFeatureGroup.get_fg_name()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_generated_feature_groups","title":"get_generated_feature_groups","text":"<pre><code>ExternalFeatureGroup.get_generated_feature_groups()\n</code></pre> <p>Get the generated feature groups using this feature group, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_generated_feature_views","title":"get_generated_feature_views","text":"<pre><code>ExternalFeatureGroup.get_generated_feature_views()\n</code></pre> <p>Get the generated feature view using this feature group, based on explicit provenance. These feature views can be accessible or inaccessible. Explicit provenance does not track deleted generated feature view links, so deleted will always be empty. For inaccessible feature views, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_latest_validation_report","title":"get_latest_validation_report","text":"<pre><code>ExternalFeatureGroup.get_latest_validation_report(ge_type=True)\n</code></pre> <p>Return the latest validation report attached to the Feature Group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nlatest_val_report = fg.get_latest_validation_report()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code> if Great Expectations is installed,     else <code>False</code>.</li> </ul> <p>Returns</p> <p><code>ValidationReport</code>. The latest validation report attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_parent_feature_groups","title":"get_parent_feature_groups","text":"<pre><code>ExternalFeatureGroup.get_parent_feature_groups()\n</code></pre> <p>Get the parents of this feature group, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_statistics","title":"get_statistics","text":"<pre><code>ExternalFeatureGroup.get_statistics(computation_time=None, feature_names=None)\n</code></pre> <p>Returns the statistics computed at a specific time for the current feature group.</p> <p>If <code>computation_time</code> is <code>None</code>, the most recent statistics are returned.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_statistics = fg.get_statistics(computation_time=None)\n</code></pre> <p>Arguments</p> <ul> <li>computation_time <code>str | int | float | datetime.datetime | datetime.date | None</code>: Date and time when statistics were computed. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. Statistics object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_storage_connector","title":"get_storage_connector","text":"<pre><code>ExternalFeatureGroup.get_storage_connector()\n</code></pre> <p>Get the storage connector using this feature group, based on explicit provenance. Only the accessible storage connector is returned. For more items use the base method - get_storage_connector_provenance</p> <p>Returns</p> <p>`StorageConnector: Storage connector.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_storage_connector_provenance","title":"get_storage_connector_provenance","text":"<pre><code>ExternalFeatureGroup.get_storage_connector_provenance()\n</code></pre> <p>Get the parents of this feature group, based on explicit provenance. Parents are storage connectors. These storage connector can be accessible, deleted or inaccessible. For deleted and inaccessible storage connector, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the storage connector used to generated this feature group</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_tag","title":"get_tag","text":"<pre><code>ExternalFeatureGroup.get_tag(name)\n</code></pre> <p>Get the tags of a feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_tag_value = fg.get_tag(\"example_tag\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_tags","title":"get_tags","text":"<pre><code>ExternalFeatureGroup.get_tags()\n</code></pre> <p>Retrieves all tags attached to a feature group.</p> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_validation_history","title":"get_validation_history","text":"<pre><code>ExternalFeatureGroup.get_validation_history(\n    expectation_id,\n    start_validation_time=None,\n    end_validation_time=None,\n    filter_by=None,\n    ge_type=True,\n)\n</code></pre> <p>Fetch validation history of an Expectation specified by its id.</p> <p>Example</p> <pre><code>validation_history = fg.get_validation_history(\n    expectation_id=1,\n    filter_by=[\"REJECTED\", \"UNKNOWN\"],\n    start_validation_time=\"2022-01-01 00:00:00\",\n    end_validation_time=datetime.datetime.now(),\n    ge_type=False\n)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_id <code>int</code>: id of the Expectation for which to fetch the validation history</li> <li>filter_by <code>List[Literal['ingested', 'rejected', 'unknown', 'fg_data', 'experiment']] | None</code>: list of ingestion_result category to keep. Ooptions are \"INGESTED\", \"REJECTED\", \"FG_DATA\", \"EXPERIMENT\", \"UNKNOWN\".</li> <li>start_validation_time <code>str | int | datetime.datetime | datetime.date | None</code>: fetch only validation result posterior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above.</li> <li>end_validation_time <code>str | int | datetime.datetime | datetime.date | None</code>: fetch only validation result prior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above.</li> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code> if Great Expectations is installed,     else <code>False</code>.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>Return</p> <p>Union[List[<code>ValidationResult</code>], List[<code>ExpectationValidationResult</code>]] A list of validation result connected to the expectation_id</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#insert","title":"insert","text":"<pre><code>ExternalFeatureGroup.insert(features, write_options=None, validation_options=None, wait=False)\n</code></pre> <p>Insert the dataframe feature values ONLY in the online feature store.</p> <p>External Feature Groups contains metadata about feature data in an external storage system. External storage system are usually offline, meaning feature values cannot be retrieved in real-time. In order to use the feature values for real-time use-cases, you can insert them in Hopsoworks Online Feature Store via this method.</p> <p>The Online Feature Store has a single-entry per primary key value, meaining that providing a new value with for a given primary key will overwrite the existing value. No record of the previous value is kept.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the External Feature Group instance\nfg = fs.get_feature_group(name=\"external_sales_records\", version=1)\n\n# get the feature values, e.g reading from csv files in a S3 bucket\nfeature_values = ...\n\n# insert the feature values in the online feature store\nfg.insert(feature_values)\n</code></pre> <p>Note</p> <p>Data Validation via Great Expectation is supported if you have attached an expectation suite to your External Feature Group. However, as opposed to regular Feature Groups, this can lead to discrepancies between the data in the external storage system and the online feature store.</p> <p>Arguments</p> <ul> <li>features <code>pandas.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | hsfs.feature_group.pyspark.RDD | numpy.ndarray | List[list]</code>: DataFrame, RDD, Ndarray, list. Features to be saved.</li> <li>write_options <code>Dict[str, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>kafka_producer_config</code> and value an object of type properties   used to configure the Kafka client. To optimize for throughput in high latency connection consider   changing producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established   connectivity from you Python environment to the internal advertised   listeners of the Hopsworks Kafka Cluster. Defaults to <code>False</code> and   will use external listeners when connecting from outside of Hopsworks.</li> </ul> </li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> <li>key <code>fetch_expectation_suite</code> a boolean value, by default <code>True</code>, to control whether the expectation    suite of the feature group should be fetched before every insert.</li> </ul> </li> </ul> <p>Returns</p> <p>Tuple(None, <code>ge.core.ExpectationSuiteValidationResult</code>) The validation report if validation is enabled.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. e.g fail to create feature group, dataframe schema does not match     existing feature group schema, etc. <code>hsfs.client.exceptions.DataValidationException</code>. If data validation fails and the expectation     suite <code>validation_ingestion_policy</code> is set to <code>STRICT</code>. Data is NOT ingested.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#json","title":"json","text":"<pre><code>ExternalFeatureGroup.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#prepare_spark_location","title":"prepare_spark_location","text":"<pre><code>ExternalFeatureGroup.prepare_spark_location()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#read","title":"read","text":"<pre><code>ExternalFeatureGroup.read(dataframe_type=\"default\", online=False, read_options=None)\n</code></pre> <p>Get the feature group as a DataFrame.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\ndf = fg.read()\n</code></pre> <p>Engine Support</p> <p>Spark only</p> <p>Reading an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups.</p> <p>Arguments</p> <ul> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> <li>online <code>bool</code>: bool, optional. If <code>True</code> read from online feature store, defaults     to <code>False</code>.</li> <li>read_options <code>Dict[str, Any] | None</code>: Additional options as key/value pairs to pass to the spark engine.     Defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>: The spark dataframe containing the feature data. <code>pyspark.DataFrame</code>. A Spark DataFrame. <code>pandas.DataFrame</code>. A Pandas DataFrame. <code>numpy.ndarray</code>. A two-dimensional Numpy array. <code>list</code>. A two-dimensional Python list.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#save","title":"save","text":"<pre><code>ExternalFeatureGroup.save()\n</code></pre> <p>Persist the metadata for this external feature group.</p> <p>Without calling this method, your feature group will only exist in your Python Kernel, but not in Hopsworks.</p> <pre><code>query = \"SELECT * FROM sales\"\n\nfg = feature_store.create_external_feature_group(name=\"sales\",\n    version=1,\n    description=\"Physical shop sales features\",\n    query=query,\n    storage_connector=connector,\n    primary_key=['ss_store_sk'],\n    event_time='sale_date'\n)\n\nfg.save()\n\n\n----\n\n&lt;span style=\"float:right;\"&gt;[[source]](https://github.com/logicalclocks/hopsworks-api/tree/2515818837a004c9768474781c93562443bdb44b/python/hsfs/feature_group.py#L1081)&lt;/span&gt;\n\n### save_expectation_suite\n\n\n```python\nExternalFeatureGroup.save_expectation_suite(\n    expectation_suite, run_validation=True, validation_ingestion_policy=\"always\", overwrite=False\n)\n</code></pre> <p>Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.save_expectation_suite(expectation_suite, run_validation=True)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | great_expectations.core.expectation_suite.ExpectationSuite</code>: The expectation suite to attach to the Feature Group.</li> <li>overwrite <code>bool</code>: If an Expectation Suite is already attached, overwrite it.     The new suite will have its own validation history, but former reports are preserved.</li> <li>run_validation <code>bool</code>: Set whether the expectation_suite will run on ingestion</li> <li>validation_ingestion_policy <code>Literal['always', 'strict']</code>: Set the policy for ingestion to the Feature Group.<ul> <li>\"STRICT\" only allows DataFrame passing validation to be inserted into Feature Group.</li> <li>\"ALWAYS\" always insert the DataFrame to the Feature Group, irrespective of overall validation result.</li> </ul> </li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#save_validation_report","title":"save_validation_report","text":"<pre><code>ExternalFeatureGroup.save_validation_report(\n    validation_report, ingestion_result=\"UNKNOWN\", ge_type=True\n)\n</code></pre> <p>Save validation report to hopsworks platform along previous reports of the same Feature Group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(..., expectation_suite=expectation_suite)\n\nvalidation_report = great_expectations.from_pandas(\n    my_experimental_features_df,\n    fg.get_expectation_suite()).validate()\n\nfg.save_validation_report(validation_report, ingestion_result=\"EXPERIMENT\")\n</code></pre> <p>Arguments</p> <ul> <li>validation_report <code>Dict[str, Any] | hsfs.validation_report.ValidationReport | great_expectations.core.expectation_validation_result.ExpectationSuiteValidationResult</code>: The validation report to attach to the Feature Group.</li> <li>ingestion_result <code>Literal['unknown', 'experiment', 'fg_data']</code>: Specify the fate of the associated data, defaults     to \"UNKNOWN\". Supported options are  \"UNKNOWN\", \"INGESTED\", \"REJECTED\",     \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation     of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\"     for testing and development and \"FG_DATA\" when validating data     already in the Feature Group.</li> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code> if Great Expectations is installed,     else <code>False</code>.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#select","title":"select","text":"<pre><code>ExternalFeatureGroup.select(features)\n</code></pre> <p>Select a subset of features of the feature group and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\n# construct query\nquery = fg.select([\"id\", \"f1\"])\nquery.features\n# [Feature('id', ...), Feature('f1', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>features <code>List[str | hsfs.feature.Feature]</code>: A list of <code>Feature</code> objects or feature names as     strings to be selected.</li> </ul> <p>Returns</p> <p><code>Query</code>: A query object with the selected features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#select_all","title":"select_all","text":"<pre><code>ExternalFeatureGroup.select_all(include_primary_key=True, include_event_time=True)\n</code></pre> <p>Select all features along with primary key and event time from the feature group and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instances\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\n# construct the query\nquery = fg1.select_all().join(fg2.select_all())\n\n# show first 5 rows\nquery.show(5)\n\n\n# select all features exclude primary key and event time\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\nquery = fg.select_all()\nquery.features\n# [Feature('id', ...), Feature('ts', ...), Feature('f1', ...), Feature('f2', ...)]\n\nquery = fg.select_all(include_primary_key=False, include_event_time=False)\nquery.features\n# [Feature('f1', ...), Feature('f2', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>include_primary_key <code>bool | None</code>: If True, include primary key of the feature group     to the feature list. Defaults to True.</li> <li>include_event_time <code>bool | None</code>: If True, include event time of the feature group     to the feature list. Defaults to True.</li> </ul> <p>Returns</p> <p><code>Query</code>. A query object with all features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#select_except","title":"select_except","text":"<pre><code>ExternalFeatureGroup.select_except(features=None)\n</code></pre> <p>Select all features including primary key and event time feature of the feature group except provided <code>features</code> and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\n# construct query\nquery = fg.select_except([\"ts\", \"f1\"])\nquery.features\n# [Feature('id', ...), Feature('f1', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>features <code>List[str | hsfs.feature.Feature] | None</code>: A list of <code>Feature</code> objects or feature names as     strings to be excluded from the selection. Defaults to [],     selecting all features.</li> </ul> <p>Returns</p> <p><code>Query</code>: A query object with the selected features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#select_features","title":"select_features","text":"<pre><code>ExternalFeatureGroup.select_features()\n</code></pre> <p>Select all the features in the feature group and return a query object.</p> <p>Queries define the schema of Feature View objects which can be used to create Training Datasets, read from the Online Feature Store, and more. They can also be composed to create more complex queries using the <code>join</code> method.</p> <p>Info</p> <p>This method does not select the primary key and event time of the feature group. Use <code>select_all</code> to include them. Note that primary keys do not need to be included in the query to allow joining on them.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = hopsworks.login().get_feature_store()\n\n# Some dataframe to create the feature group with\n# both an event time and a primary key column\nmy_df.head()\n+------------+------------+------------+------------+\n|    id      | feature_1  |    ...     |    ts      |\n+------------+------------+------------+------------+\n|     8      |     8      |            |    15      |\n|     3      |     3      |    ...     |    6       |\n|     1      |     1      |            |    18      |\n+------------+------------+------------+------------+\n\n# Create the Feature Group instances\nfg1 = fs.create_feature_group(\n        name = \"fg1\",\n        version=1,\n        primary_key=[\"id\"],\n        event_time=\"ts\",\n    )\n\n# Insert data to the feature group.\nfg1.insert(my_df)\n\n# select all features from `fg1` excluding primary key and event time\nquery = fg1.select_features()\n\n# show first 3 rows\nquery.show(3)\n\n# Output, no id or ts columns\n+------------+------------+------------+\n| feature_1  | feature_2  | feature_3  |\n+------------+------------+------------+\n|     8      |     7      |    15      |\n|     3      |     1      |     6      |\n|     1      |     2      |    18      |\n+------------+------------+------------+\n</code></pre> <p>Example</p> <pre><code># connect to the Feature Store\nfs = hopsworks.login().get_feature_store()\n\n# Get the Feature Group from the previous example\nfg1 = fs.get_feature_group(\"fg1\", 1)\n\n# Some dataframe to create another feature group\n# with a primary key column\n+------------+------------+------------+\n|    id_2    | feature_6  | feature_7  |\n+------------+------------+------------+\n|     8      |     11     |            |\n|     3      |     4      |    ...     |\n|     1      |     9      |            |\n+------------+------------+------------+\n\n# join the two feature groups on their indexes, `id` and `id_2`\n# but does not include them in the query\nquery = fg1.select_features().join(fg2.select_features(), left_on=\"id\", right_on=\"id_2\")\n\n# show first 5 rows\nquery.show(3)\n\n# Output\n+------------+------------+------------+------------+------------+\n| feature_1  | feature_2  | feature_3  | feature_6  | feature_7  |\n+------------+------------+------------+------------+------------+\n|     8      |     7      |    15      |    11      |    15      |\n|     3      |     1      |     6      |     4      |     3      |\n|     1      |     2      |    18      |     9      |    20      |\n+------------+------------+------------+------------+------------+\n</code></pre> <p>Returns</p> <p><code>Query</code>. A query object with all features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#show","title":"show","text":"<pre><code>ExternalFeatureGroup.show(n, online=False)\n</code></pre> <p>Show the first <code>n</code> rows of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# make a query and show top 5 rows\nfg.select(['date','weekly_sales','is_holiday']).show(5)\n</code></pre> <p>Arguments</p> <ul> <li>n <code>int</code>: int. Number of rows to show.</li> <li>online <code>bool</code>: bool, optional. If <code>True</code> read from online feature store, defaults     to <code>False</code>.</li> </ul> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#to_dict","title":"to_dict","text":"<pre><code>ExternalFeatureGroup.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_deprecated","title":"update_deprecated","text":"<pre><code>ExternalFeatureGroup.update_deprecated(deprecate=True)\n</code></pre> <p>Deprecate the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_deprecated(deprecate=True)\n</code></pre> <p>Safe update</p> <p>This method updates the feature group safely. In case of failure your local metadata object will be kept unchanged.</p> <p>Arguments</p> <ul> <li>deprecate <code>bool</code>: Boolean value identifying if the feature group should be deprecated. Defaults to True.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_description","title":"update_description","text":"<pre><code>ExternalFeatureGroup.update_description(description)\n</code></pre> <p>Update the description of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_description(description=\"Much better description.\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature group description safely. In case of failure your local metadata object will keep the old description.</p> <p>Arguments</p> <ul> <li>description <code>str</code>: New description string.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_feature_description","title":"update_feature_description","text":"<pre><code>ExternalFeatureGroup.update_feature_description(feature_name, description)\n</code></pre> <p>Update the description of a single feature in this feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_feature_description(feature_name=\"min_temp\",\n                              description=\"Much better feature description.\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature description safely. In case of failure your local metadata object will keep the old description.</p> <p>Arguments</p> <ul> <li>feature_name <code>str</code>: Name of the feature to be updated.</li> <li>description <code>str</code>: New description string.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_features","title":"update_features","text":"<pre><code>ExternalFeatureGroup.update_features(features)\n</code></pre> <p>Update metadata of features in this feature group.</p> <p>Currently it's only supported to update the description of a feature.</p> <p>Unsafe update</p> <p>Note that if you use an existing <code>Feature</code> object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.feature.Feature | List[hsfs.feature.Feature]</code>: <code>Feature</code> or list of features. A feature object or list thereof to     be updated.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_from_response_json","title":"update_from_response_json","text":"<pre><code>ExternalFeatureGroup.update_from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_notification_topic_name","title":"update_notification_topic_name","text":"<pre><code>ExternalFeatureGroup.update_notification_topic_name(notification_topic_name)\n</code></pre> <p>Update the notification topic name of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_notification_topic_name(notification_topic_name=\"notification_topic_name\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature group notification topic name safely. In case of failure your local metadata object will keep the old notification topic name.</p> <p>Arguments</p> <ul> <li>notification_topic_name <code>str</code>: Name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If set to None no notifications are sent.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_statistics_config","title":"update_statistics_config","text":"<pre><code>ExternalFeatureGroup.update_statistics_config()\n</code></pre> <p>Update the statistics configuration of the feature group.</p> <p>Change the <code>statistics_config</code> object and persist the changes by calling this method.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_statistics_config()\n</code></pre> <p>Returns</p> <p><code>FeatureGroup</code>. The updated metadata object of the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#validate","title":"validate","text":"<pre><code>ExternalFeatureGroup.validate(\n    dataframe=None,\n    expectation_suite=None,\n    save_report=False,\n    validation_options=None,\n    ingestion_result=\"unknown\",\n    ge_type=True,\n)\n</code></pre> <p>Run validation based on the attached expectations.</p> <p>Runs the expectation suite attached to the feature group against the provided dataframe. Raise an error if the great_expectations package is not installed.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get feature group instance\nfg = fs.get_or_create_feature_group(...)\n\nge_report = fg.validate(df, save_report=False)\n</code></pre> <p>Arguments</p> <ul> <li>dataframe <code>pandas.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | None</code>: The dataframe to run the data validation expectations against.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | None</code>: Optionally provide an Expectation Suite to override the     one that is possibly attached to the feature group. This is useful for     testing new Expectation suites. When an extra suite is provided, the results     will never be persisted. Defaults to <code>None</code>.</li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> </ul> </li> <li>ingestion_result <code>Literal['unknown', 'ingested', 'rejected', 'fg_data', 'experiement']</code>: Specify the fate of the associated data, defaults     to \"UNKNOWN\". Supported options are  \"UNKNOWN\", \"INGESTED\", \"REJECTED\",     \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation     of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\"     for testing and development and \"FG_DATA\" when validating data     already in the Feature Group.</li> <li>save_report <code>bool | None</code>: Whether to save the report to the backend. This is only possible if the Expectation suite     is initialised and attached to the Feature Group. Defaults to False.</li> <li>ge_type <code>bool</code>: Whether to return a Great Expectations object or Hopsworks own abstraction.     Defaults to <code>True</code> if Great Expectations is installed, else <code>False</code>.</li> </ul> <p>Returns</p> <p>A Validation Report produced by Great Expectations.</p>"},{"location":"generated/api/feature_api/","title":"Feature","text":"<p>[source]</p>"},{"location":"generated/api/feature_api/#feature_1","title":"Feature","text":"<pre><code>hsfs.feature.Feature(\n    name,\n    type=None,\n    description=None,\n    primary=False,\n    partition=False,\n    hudi_precombine_key=False,\n    online_type=None,\n    default_value=None,\n    feature_group_id=None,\n    feature_group=None,\n    on_demand=False,\n    **kwargs\n)\n</code></pre> <p>Metadata object representing a feature in a feature group in the Feature Store.</p> <p>See Training Dataset Feature for the feature representation of training dataset schemas.</p>"},{"location":"generated/api/feature_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_api/#default_value","title":"default_value","text":"<p>Default value of the feature as string, if the feature was appended to the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#description","title":"description","text":"<p>Description of the feature.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#feature_group_id","title":"feature_group_id","text":"<p>[source]</p>"},{"location":"generated/api/feature_api/#hudi_precombine_key","title":"hudi_precombine_key","text":"<p>Whether the feature is part of the hudi precombine key of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#name","title":"name","text":"<p>Name of the feature.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#on_demand","title":"on_demand","text":"<p>Whether the feature is a on-demand feature computed using on-demand transformation functions</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#online_type","title":"online_type","text":"<p>Data type of the feature in the online feature store.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#partition","title":"partition","text":"<p>Whether the feature is part of the partition key of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#primary","title":"primary","text":"<p>Whether the feature is part of the primary key of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#type","title":"type","text":"<p>Data type of the feature in the offline feature store.</p> <p>Not a Python type</p> <p>This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store.</p>"},{"location":"generated/api/feature_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/feature_api/#contains","title":"contains","text":"<pre><code>Feature.contains(other)\n</code></pre> <p>Deprecated</p> <p><code>contains</code> method is deprecated. Use <code>isin</code> instead.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#from_response_json","title":"from_response_json","text":"<pre><code>Feature.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_api/#is_complex","title":"is_complex","text":"<pre><code>Feature.is_complex()\n</code></pre> <p>Returns true if the feature has a complex type.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nselected_feature = fg.get_feature(\"min_temp\")\nselected_feature.is_complex()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_api/#isin","title":"isin","text":"<pre><code>Feature.isin(other)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_api/#json","title":"json","text":"<pre><code>Feature.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_api/#like","title":"like","text":"<pre><code>Feature.like(other)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_api/#to_dict","title":"to_dict","text":"<pre><code>Feature.to_dict()\n</code></pre> <p>Get structured info about specific Feature in python dictionary format.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nselected_feature = fg.get_feature(\"min_temp\")\nselected_feature.to_dict()\n</code></pre>"},{"location":"generated/api/feature_descriptive_statistics_api/","title":"Feature Descriptive Statistics","text":"<p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#featuredescriptivestatistics","title":"FeatureDescriptiveStatistics","text":"<pre><code>hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics(\n    feature_name,\n    feature_type=None,\n    count=None,\n    completeness=None,\n    num_non_null_values=None,\n    num_null_values=None,\n    approx_num_distinct_values=None,\n    min=None,\n    max=None,\n    sum=None,\n    mean=None,\n    stddev=None,\n    percentiles=None,\n    distinctness=None,\n    entropy=None,\n    uniqueness=None,\n    exact_num_distinct_values=None,\n    extended_statistics=None,\n    id=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_descriptive_statistics_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#approx_num_distinct_values","title":"approx_num_distinct_values","text":"<p>Approximate number of distinct values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#completeness","title":"completeness","text":"<p>Fraction of non-null values in a column.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#count","title":"count","text":"<p>Number of values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#distinctness","title":"distinctness","text":"<p>Fraction of distinct values of a feature over the number of all its values. Distinct values occur at least once.</p> <p>Example</p> <p>[a, a, b] contains two distinct values a and b, so distinctness is 2/3.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#entropy","title":"entropy","text":"<p>Entropy is a measure of the level of information contained in an event (feature value) when considering all possible events (all feature values). Entropy is estimated using observed value counts as the negative sum of (value_count/total_count) * log(value_count/total_count).</p> <p>Example</p> <p>[a, b, b, c, c] has three distinct values with counts [1, 2, 2].</p> <p>Entropy is then (-1/5*log(1/5)-2/5*log(2/5)-2/5*log(2/5)) = 1.055.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#exact_num_distinct_values","title":"exact_num_distinct_values","text":"<p>Exact number of distinct values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#extended_statistics","title":"extended_statistics","text":"<p>Additional statistics computed on the feature values such as histograms and correlations.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#feature_name","title":"feature_name","text":"<p>Name of the feature.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#feature_type","title":"feature_type","text":"<p>Data type of the feature. It can be one of Boolean, Fractional, Integral, or String.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#id","title":"id","text":"<p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#max","title":"max","text":"<p>Maximum value.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#mean","title":"mean","text":"<p>Mean value.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#min","title":"min","text":"<p>Minimum value.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#num_non_null_values","title":"num_non_null_values","text":"<p>Number of non-null values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#num_null_values","title":"num_null_values","text":"<p>Number of null values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#percentiles","title":"percentiles","text":"<p>Percentiles.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#stddev","title":"stddev","text":"<p>Standard deviation of the feature values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#sum","title":"sum","text":"<p>Sum of all feature values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#uniqueness","title":"uniqueness","text":"<p>Fraction of unique values over the number of all values of a column. Unique values occur exactly once.</p> <p>Example</p> <p>[a, a, b] contains one unique value b, so uniqueness is 1/3.</p>"},{"location":"generated/api/feature_group_api/","title":"FeatureGroup","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#featuregroup_1","title":"FeatureGroup","text":"<pre><code>hsfs.feature_group.FeatureGroup(\n    name,\n    version,\n    featurestore_id,\n    description=\"\",\n    partition_key=None,\n    primary_key=None,\n    hudi_precombine_key=None,\n    featurestore_name=None,\n    embedding_index=None,\n    created=None,\n    creator=None,\n    id=None,\n    features=None,\n    location=None,\n    online_enabled=False,\n    time_travel_format=None,\n    statistics_config=None,\n    online_topic_name=None,\n    topic_name=None,\n    notification_topic_name=None,\n    event_time=None,\n    stream=False,\n    expectation_suite=None,\n    parents=None,\n    href=None,\n    delta_streamer_job_conf=None,\n    deprecated=False,\n    transformation_functions=None,\n    online_config=None,\n    offline_backfill_every_hr=None,\n    storage_connector=None,\n    path=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_group_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#create_feature_group","title":"create_feature_group","text":"<pre><code>FeatureStore.create_feature_group(\n    name,\n    version=None,\n    description=\"\",\n    online_enabled=False,\n    time_travel_format=\"HUDI\",\n    partition_key=None,\n    primary_key=None,\n    embedding_index=None,\n    hudi_precombine_key=None,\n    features=None,\n    statistics_config=None,\n    event_time=None,\n    stream=False,\n    expectation_suite=None,\n    parents=None,\n    topic_name=None,\n    notification_topic_name=None,\n    transformation_functions=None,\n    online_config=None,\n    offline_backfill_every_hr=None,\n    storage_connector=None,\n    path=None,\n)\n</code></pre> <p>Create a feature group metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# define the on-demand transformation functions\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n@udf(int)\ndef plus_two(value):\n    return value + 2\n\n# construct list of \"transformation functions\" on features\ntransformation_functions = [plus_one(\"feature1\"), plus_two(\"feature2\"))]\n\nfg = fs.create_feature_group(\n        name='air_quality',\n        description='Air Quality characteristics of each day',\n        version=1,\n        primary_key=['city','date'],\n        online_enabled=True,\n        event_time='date',\n        transformation_functions=transformation_functions,\n        online_config={'online_comments': ['NDB_TABLE=READ_BACKUP=1']}\n    )\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the <code>save()</code> method with a DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to create.</li> <li>version <code>int | None</code>: Version of the feature group to retrieve, defaults to <code>None</code> and     will create the feature group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>online_enabled <code>bool | None</code>: Define whether the feature group should be made available     also in the online feature store for low latency access, defaults to     <code>False</code>.</li> <li>time_travel_format <code>str | None</code>: Format used for time travel, defaults to <code>\"HUDI\"</code>.</li> <li>partition_key <code>List[str] | None</code>: A list of feature names to be used as partition key when     writing the feature data to the offline storage, defaults to empty list     <code>[]</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>embedding_index <code>hsfs.embedding.EmbeddingIndex | None</code>: <code>EmbeddingIndex</code>. If an embedding index is provided,     vector database is used as online feature store. This enables similarity search by     using <code>find_neighbors</code>.     default to <code>None</code></li> <li>hudi_precombine_key <code>str | None</code>: A feature name to be used as a precombine key for the <code>\"HUDI\"</code>     feature group. Defaults to <code>None</code>. If feature group has time travel format     <code>\"HUDI\"</code> and hudi precombine key was not specified then the first primary key of     the feature group will be used as hudi precombine key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame provided in the <code>save</code> method.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li> <p>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ stream__: Optionally, Define whether the feature group should support real time stream writing capabilities.     Stream enabled Feature Groups have unified single API for writing streaming features transparently     to both online and offline store.</p> </li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | hsfs.feature_store.great_expectations.core.ExpectationSuite | None</code>: Optionally, attach an expectation suite to the feature     group which dataframes should be validated against upon insertion.     Defaults to <code>None</code>.</li> <li>parents <code>List[hsfs.feature_group.FeatureGroup] | None</code>: Optionally, Define the parents of this feature group as the     origin where the data is coming from.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it     defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> <li>transformation_functions <code>List[hsfs.transformation_function.TransformationFunction | hsfs.hopsworks_udf.HopsworksUdf] | None</code>: On-Demand Transformation functions attached to the feature group.     It can be a list of list of user defined functions defined using the hopsworks <code>@udf</code> decorator.     Defaults to <code>None</code>, no transformations.</li> <li>online_config <code>hsfs.online_config.OnlineConfig | Dict[str, Any] | None</code>: Optionally, define configuration which is used to configure online table.</li> <li>offline_backfill_every_hr <code>int | str | None</code>: Optional. If specified, the materialization job will be scheduled to run     periodically. The value can be either an integer representing the number of hours between each run     or a string representing a cron expression. Set the value to None to avoid scheduling the materialization     job. Defaults to None (i.e no scheduling).</li> <li>storage_connector <code>hsfs.StorageConnector | Dict[str, Any]</code>: the storage connector used to establish connectivity     with the data source.</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The feature group metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_or_create_feature_group","title":"get_or_create_feature_group","text":"<pre><code>FeatureStore.get_or_create_feature_group(\n    name,\n    version,\n    description=\"\",\n    online_enabled=False,\n    time_travel_format=\"HUDI\",\n    partition_key=None,\n    primary_key=None,\n    embedding_index=None,\n    hudi_precombine_key=None,\n    features=None,\n    statistics_config=None,\n    expectation_suite=None,\n    event_time=None,\n    stream=False,\n    parents=None,\n    topic_name=None,\n    notification_topic_name=None,\n    transformation_functions=None,\n    online_config=None,\n    offline_backfill_every_hr=None,\n    storage_connector=None,\n    path=None,\n)\n</code></pre> <p>Get feature group metadata object or create a new one if it doesn't exist. This method doesn't update existing feature group metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_or_create_feature_group(\n        name=\"electricity_prices\",\n        version=1,\n        description=\"Electricity prices from NORD POOL\",\n        primary_key=[\"day\", \"area\"],\n        online_enabled=True,\n        event_time=\"timestamp\",\n        transformation_functions=transformation_functions,\n        online_config={'online_comments': ['NDB_TABLE=READ_BACKUP=1']}\n        )\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the <code>insert()</code> method with a DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to create.</li> <li>version <code>int</code>: Version of the feature group to retrieve or create.</li> <li>description <code>str | None</code>: A string describing the contents of the feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>online_enabled <code>bool | None</code>: Define whether the feature group should be made available     also in the online feature store for low latency access, defaults to     <code>False</code>.</li> <li>time_travel_format <code>str | None</code>: Format used for time travel, defaults to <code>\"HUDI\"</code>.</li> <li>partition_key <code>List[str] | None</code>: A list of feature names to be used as partition key when     writing the feature data to the offline storage, defaults to empty list     <code>[]</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>embedding_index <code>hsfs.embedding.EmbeddingIndex | None</code>: <code>EmbeddingIndex</code>. If an embedding index is provided,     the vector database is used as online feature store. This enables similarity search by     using <code>find_neighbors</code>.     default is <code>None</code></li> <li>hudi_precombine_key <code>str | None</code>: A feature name to be used as a precombine key for the <code>\"HUDI\"</code>     feature group. Defaults to <code>None</code>. If feature group has time travel format     <code>\"HUDI\"</code> and hudi precombine key was not specified then the first primary key of     the feature group will be used as hudi precombine key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame provided in the <code>save</code> method.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | hsfs.feature_store.great_expectations.core.ExpectationSuite | None</code>: Optionally, attach an expectation suite to the feature     group which dataframes should be validated against upon insertion.     Defaults to <code>None</code>.</li> <li> <p>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ stream__: Optionally, Define whether the feature group should support real time stream writing capabilities.     Stream enabled Feature Groups have unified single API for writing streaming features transparently     to both online and offline store.</p> </li> <li>parents <code>List[hsfs.feature_group.FeatureGroup] | None</code>: Optionally, Define the parents of this feature group as the     origin where the data is coming from.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it     defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> <li>transformation_functions <code>List[hsfs.transformation_function.TransformationFunction | hsfs.hopsworks_udf.HopsworksUdf] | None</code>: On-Demand Transformation functions attached to the feature group.     It can be a list of list of user defined functions defined using the hopsworks <code>@udf</code> decorator.     Defaults to <code>None</code>, no transformations.</li> <li>online_config <code>hsfs.online_config.OnlineConfig | Dict[str, Any] | None</code>: Optionally, define configuration which is used to configure online table.</li> <li>offline_backfill_every_hr <code>int | str | None</code>: Optional. If specified, the materialization job will be scheduled to run     periodically. The value can be either an integer representing the number of hours between each run     or a string representing a cron expression. Set the value to None to avoid scheduling the materialization     job. Defaults to None (i.e no automatic scheduling). Applies only on Feature Group creation.</li> <li>storage_connector <code>hsfs.StorageConnector | Dict[str, Any]</code>: the storage connector used to establish connectivity     with the data source.</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The feature group metadata object.</p>"},{"location":"generated/api/feature_group_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_feature_group","title":"get_feature_group","text":"<pre><code>FeatureStore.get_feature_group(name, version=None)\n</code></pre> <p>Get a feature group entity from the feature store.</p> <p>Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_feature_group(\n        name=\"electricity_prices\",\n        version=1,\n    )\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to get.</li> <li>version <code>int | None</code>: Version of the feature group to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>: The feature group metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul>"},{"location":"generated/api/feature_group_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#avro_schema","title":"avro_schema","text":"<p>Avro schema representation of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#created","title":"created","text":"<p>Timestamp when the feature group was created.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#creator","title":"creator","text":"<p>Username of the creator.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#deprecated","title":"deprecated","text":"<p>Setting if the feature group is deprecated.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#description","title":"description","text":"<p>Description of the feature group contents.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#embedding_index","title":"embedding_index","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#event_time","title":"event_time","text":"<p>Event time feature in the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#expectation_suite","title":"expectation_suite","text":"<p>Expectation Suite configuration object defining the settings for data validation of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#feature_store","title":"feature_store","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#feature_store_id","title":"feature_store_id","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#feature_store_name","title":"feature_store_name","text":"<p>Name of the feature store in which the feature group is located.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#features","title":"features","text":"<p>Feature Group schema (alias)</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#hudi_precombine_key","title":"hudi_precombine_key","text":"<p>Feature name that is the hudi precombine key.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#id","title":"id","text":"<p>Feature group id.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#location","title":"location","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#materialization_job","title":"materialization_job","text":"<p>Get the Job object reference for the materialization job for this Feature Group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#name","title":"name","text":"<p>Name of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#notification_topic_name","title":"notification_topic_name","text":"<p>The topic used for feature group notifications.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#offline_backfill_every_hr","title":"offline_backfill_every_hr","text":"<p>On Feature Group creation, used to set scheduled run of the materialisation job.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#online_enabled","title":"online_enabled","text":"<p>Setting if the feature group is available in online storage.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#parents","title":"parents","text":"<p>Parent feature groups as origin of the data in the current feature group. This is part of explicit provenance</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#partition_key","title":"partition_key","text":"<p>List of features building the partition key.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#path","title":"path","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#primary_key","title":"primary_key","text":"<p>List of features building the primary key.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#schema","title":"schema","text":"<p>Feature Group schema</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#statistics","title":"statistics","text":"<p>Get the latest computed statistics for the whole feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#statistics_config","title":"statistics_config","text":"<p>Statistics configuration object defining the settings for statistics computation of the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#storage_connector","title":"storage_connector","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#stream","title":"stream","text":"<p>Whether to enable real time stream writing capabilities.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#subject","title":"subject","text":"<p>Subject of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#time_travel_format","title":"time_travel_format","text":"<p>Setting of the feature group time travel format.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#topic_name","title":"topic_name","text":"<p>The topic used for feature group data ingestion.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#transformation_functions","title":"transformation_functions","text":"<p>Get transformation functions.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#version","title":"version","text":"<p>Version number of the feature group.</p>"},{"location":"generated/api/feature_group_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#add_tag","title":"add_tag","text":"<pre><code>FeatureGroup.add_tag(name, value)\n</code></pre> <p>Attach a tag to a feature group.</p> <p>A tag consists of a  pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.add_tag(name=\"example_tag\", value=\"42\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value <code>Any</code>: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to add the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#append_features","title":"append_features","text":"<pre><code>FeatureGroup.append_features(features)\n</code></pre> <p>Append features to the schema of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# define features to be inserted in the feature group\nfeatures = [\n    Feature(name=\"id\",type=\"int\",online_type=\"int\"),\n    Feature(name=\"name\",type=\"string\",online_type=\"varchar(20)\")\n]\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.append_features(features)\n</code></pre> <p>Safe append</p> <p>This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema.</p> <p>It is only possible to append features to a feature group. Removing features is considered a breaking change. Note that feature views built on top of this feature group will not read appended feature data. Create a new feature view based on an updated query via <code>fg.select</code> to include the new features.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.feature.Feature | List[hsfs.feature.Feature]</code>: Feature or list. A feature object or list thereof to append to     the schema of the feature group.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#as_of","title":"as_of","text":"<pre><code>FeatureGroup.as_of(wallclock_time=None, exclude_until=None)\n</code></pre> <p>Get Query object to retrieve all features of the group at a point in the past.</p> <p>Pyspark/Spark Only</p> <p>Apache HUDI exclusively supports Time Travel and Incremental Query via Spark Context</p> <p>This method selects all features in the feature group and returns a Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset.</p> <p>Reading features at a specific point in time:</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# get data at a specific point in time and show it\nfg.as_of(\"2020-10-20 07:34:11\").read().show()\n</code></pre> <p>Reading commits incrementally between specified points in time:</p> <pre><code>fg.as_of(\"2020-10-20 07:34:11\", exclude_until=\"2020-10-19 07:34:11\").read().show()\n</code></pre> <p>The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit.</p> <p>Reading only the changes from a single commit</p> <pre><code>fg.as_of(\"2020-10-20 07:31:38\", exclude_until=\"2020-10-20 07:31:37\").read().show()\n</code></pre> <p>When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded.</p> <p>Reading the latest state of features, excluding commits before a specified point in time:</p> <pre><code>fg.as_of(None, exclude_until=\"2020-10-20 07:31:38\").read().show()\n</code></pre> <p>Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion:</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\nfg1.select_all().as_of(\"2020-10-20\", exclude_until=\"2020-10-19\")\n    .join(fg2.select_all().as_of(\"2020-10-20\", exclude_until=\"2020-10-19\"))\n</code></pre> <p>If instead you apply another <code>as_of</code> selection after the join, all joined feature groups will be queried with this interval:</p> <p>Example</p> <pre><code>fg1.select_all().as_of(\"2020-10-20\", exclude_until=\"2020-10-19\")  # as_of is not applied\n    .join(fg2.select_all().as_of(\"2020-10-20\", exclude_until=\"2020-10-15\"))  # as_of is not applied\n    .as_of(\"2020-10-20\", exclude_until=\"2020-10-19\")\n</code></pre> <p>Warning</p> <p>This function only works for feature groups with time_travel_format='HUDI'.</p> <p>Warning</p> <p>Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: <code>hoodie.keep.min.commits</code> and <code>hoodie.keep.max.commits</code> when calling the <code>insert()</code> method.</p> <p>Arguments</p> <ul> <li>wallclock_time <code>str | int | datetime.datetime | datetime.date | None</code>: Read data as of this point in time. Strings should be formatted in one of the     following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, or <code>%Y-%m-%d %H:%M:%S</code>.</li> <li>exclude_until <code>str | int | datetime.datetime | datetime.date | None</code>: Exclude commits until this point in time. String should be formatted in one of the     following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, or <code>%Y-%m-%d %H:%M:%S</code>.</li> </ul> <p>Returns</p> <p><code>Query</code>. The query object with the applied time travel condition.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#check_deprecated","title":"check_deprecated","text":"<pre><code>FeatureGroup.check_deprecated()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#commit_delete_record","title":"commit_delete_record","text":"<pre><code>FeatureGroup.commit_delete_record(delete_df, write_options=None)\n</code></pre> <p>Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on feature groups stored as HUDI or DELTA.</p> <p>Arguments</p> <ul> <li>delete_df <code>hsfs.feature_group.pyspark.sql.DataFrame</code>: dataFrame containing records to be deleted.</li> <li>write_options <code>Dict[Any, Any] | None</code>: User provided write options. Defaults to <code>{}</code>.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#commit_details","title":"commit_details","text":"<pre><code>FeatureGroup.commit_details(wallclock_time=None, limit=None)\n</code></pre> <p>Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\ncommit_details = fg.commit_details()\n</code></pre> <p>Arguments</p> <ul> <li>wallclock_time <code>str | int | datetime.datetime | datetime.date | None</code>: Commit details as of specific point in time. Defaults to <code>None</code>.      Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>,     <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>limit <code>int | None</code>: Number of commits to retrieve. Defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>Dict[str, Dict[str, str]]</code>. Dictionary object of commit metadata timeline, where Key is commit id and value is <code>Dict[str, str]</code> with key value pairs of date committed on, number of rows updated, inserted and deleted.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. If the feature group does not have <code>HUDI</code> time travel format</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#compute_statistics","title":"compute_statistics","text":"<pre><code>FeatureGroup.compute_statistics(wallclock_time=None)\n</code></pre> <p>Recompute the statistics for the feature group and save them to the feature store.</p> <p>Statistics are only computed for data in the offline storage of the feature group.</p> <p>Arguments</p> <ul> <li>wallclock_time <code>str | int | datetime.datetime | datetime.date | None</code>: If specified will recompute statistics on     feature group as of specific point in time. If not specified then will compute statistics     as of most recent time of this feature group. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. The statistics metadata object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. Unable to persist the statistics.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#create_feature_monitoring","title":"create_feature_monitoring","text":"<pre><code>FeatureGroup.create_feature_monitoring(\n    name,\n    feature_name,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # Data inserted last week on the same day\n    time_offset=\"1w1d\",\n    window_length=\"1d\",\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str</code>: Name of the feature to monitor.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#create_statistics_monitoring","title":"create_statistics_monitoring","text":"<pre><code>FeatureGroup.create_statistics_monitoring(\n    name,\n    feature_name=None,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable statistics monitoring\nmy_config = fg.create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str | None</code>: Name of the feature to monitor. If not specified, statistics     will be computed for all features.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#delete","title":"delete","text":"<pre><code>FeatureGroup.delete()\n</code></pre> <p>Drop the entire feature group along with its feature data.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(\n        name='bitcoin_price',\n        version=1\n        )\n\n# delete the feature group\nfg.delete()\n</code></pre> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#delete_expectation_suite","title":"delete_expectation_suite","text":"<pre><code>FeatureGroup.delete_expectation_suite()\n</code></pre> <p>Delete the expectation suite attached to the Feature Group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.delete_expectation_suite()\n</code></pre> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#delete_tag","title":"delete_tag","text":"<pre><code>FeatureGroup.delete_tag(name)\n</code></pre> <p>Delete a tag attached to a feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.delete_tag(\"example_tag\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#delta_vacuum","title":"delta_vacuum","text":"<pre><code>FeatureGroup.delta_vacuum(retention_hours=None)\n</code></pre> <p>Vacuum files that are no longer referenced by a Delta table and are older than the retention threshold. This method can only be used on feature groups stored as DELTA.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\ncommit_details = fg.delta_vacuum(retention_hours = 168)\n</code></pre> <p>Arguments</p> <ul> <li>retention_hours <code>int | None</code>: User provided retention period. The default retention threshold for the files is 7 days.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#filter","title":"filter","text":"<pre><code>FeatureGroup.filter(f)\n</code></pre> <p>Apply filter to the feature group.</p> <p>Selects all features and returns the resulting <code>Query</code> with the applied filter.</p> <p>Example</p> <pre><code>from hsfs.feature import Feature\n\n# connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.filter(Feature(\"weekly_sales\") &gt; 1000)\n</code></pre> <p>If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group:</p> <p>Example</p> <pre><code>fg.filter(fg.feature1 == 1).show(10)\n</code></pre> <p>Composite filters require parenthesis and symbols for logical operands (e.g. <code>&amp;</code>, <code>|</code>, ...):</p> <p>Example</p> <pre><code>fg.filter((fg.feature1 == 1) | (fg.feature2 &gt;= 2))\n</code></pre> <p>Arguments</p> <ul> <li>f <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic</code>: Filter object.</li> </ul> <p>Returns</p> <p><code>Query</code>. The query object with the applied filter.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#finalize_multi_part_insert","title":"finalize_multi_part_insert","text":"<pre><code>FeatureGroup.finalize_multi_part_insert()\n</code></pre> <p>Finalizes and exits the multi part insert context opened by <code>multi_part_insert</code> in a blocking fashion once all rows have been transmitted.</p> <p>Multi part insert with manual context management</p> <p>Instead of letting Python handle the entering and exiting of the multi part insert context, you can start and finalize the context manually. <pre><code>feature_group = fs.get_or_create_feature_group(\"fg_name\", version=1)\n\nwhile loop:\n    small_batch_df = ...\n    feature_group.multi_part_insert(small_batch_df)\n\n# IMPORTANT: finalize the multi part insert to make sure all rows\n# have been transmitted\nfeature_group.finalize_multi_part_insert()\n</code></pre> Note that the first call to <code>multi_part_insert</code> initiates the context and be sure to finalize it. The <code>finalize_multi_part_insert</code> is a blocking call that returns once all rows have been transmitted.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#find_neighbors","title":"find_neighbors","text":"<pre><code>FeatureGroup.find_neighbors(embedding, col=None, k=10, filter=None, options=None)\n</code></pre> <p>Finds the nearest neighbors for a given embedding in the vector database.</p> <p>If <code>filter</code> is specified, or if embedding feature is stored in default project index, the number of results returned may be less than k. Try using a large value of k and extract the top k items from the results if needed.</p> <p>Arguments</p> <ul> <li>embedding <code>List[int | float]</code>: The target embedding for which neighbors are to be found.</li> <li>col <code>str | None</code>: The column name used to compute similarity score. Required only if there are multiple embeddings (optional).</li> <li>k <code>int | None</code>: The number of nearest neighbors to retrieve (default is 10).</li> <li>filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: A filter expression to restrict the search space (optional).</li> <li>options <code>dict | None</code>: The options used for the request to the vector database.     The keys are attribute values of the <code>hsfs.core.opensearch.OpensearchRequestOption</code> class.</li> </ul> <p>Returns</p> <p>A list of tuples representing the nearest neighbors. Each tuple contains: <code>(The similarity score, A list of feature values)</code></p> <p>Example</p> <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=3)\nfg = fs.create_feature_group(\n            name='air_quality',\n            embedding_index = embedding_index,\n            version=1,\n            primary_key=['id1'],\n            online_enabled=True,\n        )\nfg.insert(data)\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n)\n\n# apply filter\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n    filter=(fg.id1 &gt; 10) &amp; (fg.id1 &lt; 30)\n)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#from_response_json","title":"from_response_json","text":"<pre><code>FeatureGroup.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_all_statistics","title":"get_all_statistics","text":"<pre><code>FeatureGroup.get_all_statistics(computation_time=None, feature_names=None)\n</code></pre> <p>Returns all the statistics metadata computed before a specific time for the current feature group.</p> <p>If <code>computation_time</code> is <code>None</code>, all the statistics metadata are returned.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_statistics = fg.get_statistics(computation_time=None)\n</code></pre> <p>Arguments</p> <ul> <li>computation_time <code>str | int | float | datetime.datetime | datetime.date | None</code>: Date and time when statistics were computed. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. Statistics object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_all_validation_reports","title":"get_all_validation_reports","text":"<pre><code>FeatureGroup.get_all_validation_reports(ge_type=True)\n</code></pre> <p>Return the latest validation report attached to the feature group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nval_reports = fg.get_all_validation_reports()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code> if Great Expectations is installed,     else <code>False</code>.</li> </ul> <p>Returns</p> <p>Union[List[<code>ValidationReport</code>], <code>ValidationReport</code>]. All validation reports attached to the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_complex_features","title":"get_complex_features","text":"<pre><code>FeatureGroup.get_complex_features()\n</code></pre> <p>Returns the names of all features with a complex data type in this feature group.</p> <p>Example</p> <pre><code>complex_dtype_features = fg.get_complex_features()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_expectation_suite","title":"get_expectation_suite","text":"<pre><code>FeatureGroup.get_expectation_suite(ge_type=True)\n</code></pre> <p>Return the expectation suite attached to the feature group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nexp_suite = fg.get_expectation_suite()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code> if Great Expectations is installed,     else <code>False</code>.</li> </ul> <p>Returns</p> <p><code>ExpectationSuite</code>. The expectation suite attached to the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_feature","title":"get_feature","text":"<pre><code>FeatureGroup.get_feature(name)\n</code></pre> <p>Retrieve a <code>Feature</code> object from the schema of the feature group.</p> <p>There are several ways to access features of a feature group:</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# get Feature instanse\nfg.feature1\nfg[\"feature1\"]\nfg.get_feature(\"feature1\")\n</code></pre> <p>Note</p> <p>Attribute access to features works only for non-reserved names. For example features named <code>id</code> or <code>name</code> will not be accessible via <code>fg.name</code>, instead this will return the name of the feature group itself. Fall back on using the <code>get_feature</code> method.</p> <p>Arguments:</p> <p>name: The name of the feature to retrieve</p> <p>Returns:</p> <p>Feature: The feature object</p> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_feature_monitoring_configs","title":"get_feature_monitoring_configs","text":"<pre><code>FeatureGroup.get_feature_monitoring_configs(name=None, feature_name=None, config_id=None)\n</code></pre> <p>Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch all feature monitoring configs attached to the feature group\nfm_configs = fg.get_feature_monitoring_configs()\n\n# fetch a single feature monitoring config by name\nfm_config = fg.get_feature_monitoring_configs(name=\"my_config\")\n\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fg.get_feature_monitoring_configs(feature_name=\"my_feature\")\n\n# fetch a single feature monitoring config with a given id\nfm_config = fg.get_feature_monitoring_configs(config_id=1)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: If provided fetch only the feature monitoring config with the given name.     Defaults to None.</li> <li>feature_name <code>str | None</code>: If provided, fetch only configs attached to a particular feature.     Defaults to None.</li> <li>config_id <code>int | None</code>: If provided, fetch only the feature monitoring config with the given id.     Defaults to None.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both name and feature_name are provided. - TypeError: if name or feature_name are not string or None.</p> <p>Return</p> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None]     A list of feature monitoring configs. If name provided,     returns either a single config or None if not found.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_feature_monitoring_history","title":"get_feature_monitoring_history","text":"<pre><code>FeatureGroup.get_feature_monitoring_history(\n    config_name=None, config_id=None, start_time=None, end_time=None, with_statistics=True\n)\n</code></pre> <p>Fetch feature monitoring history for a given feature monitoring config.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch feature monitoring history for a given feature monitoring config\nfm_history = fg.get_feature_monitoring_history(\n    config_name=\"my_config\",\n    start_time=\"2020-01-01\",\n)\n\n# fetch feature monitoring history for a given feature monitoring config id\nfm_history = fg.get_feature_monitoring_history(\n    config_id=1,\n    start_time=datetime.now() - timedelta(weeks=2),\n    end_time=datetime.now() - timedelta(weeks=1),\n    with_statistics=False,\n)\n</code></pre> <p>Arguments</p> <ul> <li>config_name <code>str | None</code>: The name of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>config_id <code>int | None</code>: The id of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>start_time <code>str | int | datetime.datetime | datetime.date | None</code>: The start date of the feature monitoring history to fetch.     Defaults to None.</li> <li>end_time <code>str | int | datetime.datetime | datetime.date | None</code>: The end date of the feature monitoring history to fetch.     Defaults to None.</li> <li>with_statistics <code>bool | None</code>: Whether to include statistics in the feature monitoring history.     Defaults to True. If False, only metadata about the monitoring will be fetched.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both config_name and config_id are provided. - TypeError: if config_name or config_id are not respectively string, int or None.</p> <p>Return</p> <p>List[<code>FeatureMonitoringResult</code>]     A list of feature monitoring results containing the monitoring metadata     as well as the computed statistics for the detection and reference window     if requested.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_fg_name","title":"get_fg_name","text":"<pre><code>FeatureGroup.get_fg_name()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_generated_feature_groups","title":"get_generated_feature_groups","text":"<pre><code>FeatureGroup.get_generated_feature_groups()\n</code></pre> <p>Get the generated feature groups using this feature group, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_generated_feature_views","title":"get_generated_feature_views","text":"<pre><code>FeatureGroup.get_generated_feature_views()\n</code></pre> <p>Get the generated feature view using this feature group, based on explicit provenance. These feature views can be accessible or inaccessible. Explicit provenance does not track deleted generated feature view links, so deleted will always be empty. For inaccessible feature views, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_latest_validation_report","title":"get_latest_validation_report","text":"<pre><code>FeatureGroup.get_latest_validation_report(ge_type=True)\n</code></pre> <p>Return the latest validation report attached to the Feature Group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nlatest_val_report = fg.get_latest_validation_report()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code> if Great Expectations is installed,     else <code>False</code>.</li> </ul> <p>Returns</p> <p><code>ValidationReport</code>. The latest validation report attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_parent_feature_groups","title":"get_parent_feature_groups","text":"<pre><code>FeatureGroup.get_parent_feature_groups()\n</code></pre> <p>Get the parents of this feature group, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_statistics","title":"get_statistics","text":"<pre><code>FeatureGroup.get_statistics(computation_time=None, feature_names=None)\n</code></pre> <p>Returns the statistics computed at a specific time for the current feature group.</p> <p>If <code>computation_time</code> is <code>None</code>, the most recent statistics are returned.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_statistics = fg.get_statistics(computation_time=None)\n</code></pre> <p>Arguments</p> <ul> <li>computation_time <code>str | int | float | datetime.datetime | datetime.date | None</code>: Date and time when statistics were computed. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. Statistics object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_statistics_by_commit_window","title":"get_statistics_by_commit_window","text":"<pre><code>FeatureGroup.get_statistics_by_commit_window(\n    from_commit_time=None, to_commit_time=None, feature_names=None\n)\n</code></pre> <p>Returns the statistics computed on a specific commit window for this feature group. If time travel is not enabled, it raises an exception.</p> <p>If <code>from_commit_time</code> is <code>None</code>, the commit window starts from the first commit. If <code>to_commit_time</code> is <code>None</code>, the commit window ends at the last commit.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\nfg_statistics = fg.get_statistics_by_commit_window(from_commit_time=None, to_commit_time=None)\n</code></pre> <p>Arguments</p> <ul> <li>to_commit_time <code>str | int | datetime.datetime | datetime.date | None</code>: Date and time of the last commit of the window. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>from_commit_time <code>str | int | datetime.datetime | datetime.date | None</code>: Date and time of the first commit of the window. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. Statistics object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_storage_connector","title":"get_storage_connector","text":"<pre><code>FeatureGroup.get_storage_connector()\n</code></pre> <p>Get the storage connector using this feature group, based on explicit provenance. Only the accessible storage connector is returned. For more items use the base method - get_storage_connector_provenance</p> <p>Returns</p> <p>`StorageConnector: Storage connector.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_storage_connector_provenance","title":"get_storage_connector_provenance","text":"<pre><code>FeatureGroup.get_storage_connector_provenance()\n</code></pre> <p>Get the parents of this feature group, based on explicit provenance. Parents are storage connectors. These storage connector can be accessible, deleted or inaccessible. For deleted and inaccessible storage connector, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the storage connector used to generated this feature group</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_tag","title":"get_tag","text":"<pre><code>FeatureGroup.get_tag(name)\n</code></pre> <p>Get the tags of a feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_tag_value = fg.get_tag(\"example_tag\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_tags","title":"get_tags","text":"<pre><code>FeatureGroup.get_tags()\n</code></pre> <p>Retrieves all tags attached to a feature group.</p> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_validation_history","title":"get_validation_history","text":"<pre><code>FeatureGroup.get_validation_history(\n    expectation_id,\n    start_validation_time=None,\n    end_validation_time=None,\n    filter_by=None,\n    ge_type=True,\n)\n</code></pre> <p>Fetch validation history of an Expectation specified by its id.</p> <p>Example</p> <pre><code>validation_history = fg.get_validation_history(\n    expectation_id=1,\n    filter_by=[\"REJECTED\", \"UNKNOWN\"],\n    start_validation_time=\"2022-01-01 00:00:00\",\n    end_validation_time=datetime.datetime.now(),\n    ge_type=False\n)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_id <code>int</code>: id of the Expectation for which to fetch the validation history</li> <li>filter_by <code>List[Literal['ingested', 'rejected', 'unknown', 'fg_data', 'experiment']] | None</code>: list of ingestion_result category to keep. Ooptions are \"INGESTED\", \"REJECTED\", \"FG_DATA\", \"EXPERIMENT\", \"UNKNOWN\".</li> <li>start_validation_time <code>str | int | datetime.datetime | datetime.date | None</code>: fetch only validation result posterior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above.</li> <li>end_validation_time <code>str | int | datetime.datetime | datetime.date | None</code>: fetch only validation result prior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above.</li> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code> if Great Expectations is installed,     else <code>False</code>.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>Return</p> <p>Union[List[<code>ValidationResult</code>], List[<code>ExpectationValidationResult</code>]] A list of validation result connected to the expectation_id</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#insert","title":"insert","text":"<pre><code>FeatureGroup.insert(\n    features,\n    overwrite=False,\n    operation=\"upsert\",\n    storage=None,\n    write_options=None,\n    validation_options=None,\n    wait=False,\n)\n</code></pre> <p>Persist the metadata and materialize the feature group to the feature store or insert data from a dataframe into the existing feature group.</p> <p>Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is <code>online_enabled=True</code>.</p> <p>The <code>features</code> dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, a Polars DataFrame or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is <code>HUDI</code> then <code>operation</code> argument can be either <code>insert</code> or <code>upsert</code>.</p> <p>If feature group doesn't exist the insert method will create the necessary metadata the first time it is invoked and writes the specified <code>features</code> dataframe as feature group to the online/offline feature store.</p> <p>Changed in 3.3.0</p> <p><code>insert</code> and <code>save</code> methods are now async by default in non-spark clients. To achieve the old behaviour, set <code>wait</code> argument to <code>True</code>.</p> <p>Upsert new feature data with time travel format <code>HUDI</code></p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_or_create_feature_group(\n    name='bitcoin_price',\n    description='Bitcoin price aggregated for days',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\n\nfg.insert(df_bitcoin_processed)\n</code></pre> <p>Async insert</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg1 = fs.get_or_create_feature_group(\n    name='feature_group_name1',\n    description='Description of the first FG',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\n# async insertion in order not to wait till finish of the job\nfg.insert(df_for_fg1, write_options={\"wait_for_job\" : False})\n\nfg2 = fs.get_or_create_feature_group(\n    name='feature_group_name2',\n    description='Description of the second FG',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\nfg.insert(df_for_fg2)\n</code></pre> <p>Arguments</p> <ul> <li>features <code>pandas.DataFrame | polars.dataframe.frame.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | hsfs.feature_group.pyspark.RDD | numpy.ndarray | List[list]</code>: Pandas DataFrame, Polars DataFrame, RDD, Ndarray, list. Features to be saved.</li> <li>overwrite <code>bool</code>: Drop all data in the feature group before     inserting new data. This does not affect metadata, defaults to False.</li> <li>operation <code>str | None</code>: Apache Hudi operation type <code>\"insert\"</code> or <code>\"upsert\"</code>.     Defaults to <code>\"upsert\"</code>.</li> <li>storage <code>str | None</code>: Overwrite default behaviour, write to offline     storage only with <code>\"offline\"</code> or online only with <code>\"online\"</code>, defaults     to <code>None</code> (If the streaming APIs are enabled, specifying the storage option is not supported).</li> <li>write_options <code>Dict[str, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to write data into the   feature group.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the insert call should return only   after the Hopsworks Job has finished. By default it waits.</li> <li>key <code>start_offline_backfill</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. <code>start_offline_backfill</code> is deprecated. Use <code>start_offline_materialization</code> instead.</li> <li>key <code>start_offline_materialization</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. By default the materialization job gets started immediately.</li> <li>key <code>kafka_producer_config</code> and value an object of type properties   used to configure the Kafka client. To optimize for throughput in high latency connection consider   changing producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established   connectivity from you Python environment to the internal advertised   listeners of the Hopsworks Kafka Cluster. Defaults to <code>False</code> and   will use external listeners when connecting from outside of Hopsworks.</li> </ul> </li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> <li>key <code>fetch_expectation_suite</code> a boolean value, by default <code>True</code>, to control whether the expectation    suite of the feature group should be fetched before every insert.</li> </ul> </li> <li>wait <code>bool</code>: Wait for job to finish before returning, defaults to <code>False</code>.     Shortcut for read_options <code>{\"wait_for_job\": False}</code>.</li> </ul> <p>Returns</p> <p>(<code>Job</code>, <code>ValidationReport</code>) A tuple with job information if python engine is used and the validation report if validation is enabled.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. e.g fail to create feature group, dataframe schema does not match     existing feature group schema, etc. <code>hsfs.client.exceptions.DataValidationException</code>. If data validation fails and the expectation     suite <code>validation_ingestion_policy</code> is set to <code>STRICT</code>. Data is NOT ingested.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#insert_stream","title":"insert_stream","text":"<pre><code>FeatureGroup.insert_stream(\n    features,\n    query_name=None,\n    output_mode=\"append\",\n    await_termination=False,\n    timeout=None,\n    checkpoint_dir=None,\n    write_options=None,\n)\n</code></pre> <p>Ingest a Spark Structured Streaming Dataframe to the online feature store.</p> <p>This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments.</p> <p>It is possible to stop the returned query with the <code>.stop()</code> and check its status with <code>.isActive</code>.</p> <p>To get a list of all active queries, use:</p> <pre><code>sqm = spark.streams\n\n# get the list of active streaming queries\n[q.name for q in sqm.active]\n</code></pre> <p>Engine Support</p> <p>Spark only</p> <p>Stream ingestion using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming.</p> <p>Data Validation Support</p> <p><code>insert_stream</code> does not perform any data validation using Great Expectations even when a expectation suite is attached.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.feature_group.pyspark.sql.DataFrame</code>: Features in Streaming Dataframe to be saved.</li> <li>query_name <code>str | None</code>: It is possible to optionally specify a name for the query to     make it easier to recognise in the Spark UI. Defaults to <code>None</code>.</li> <li>output_mode <code>str | None</code>: Specifies how data of a streaming DataFrame/Dataset is     written to a streaming sink. (1) <code>\"append\"</code>: Only the new rows in the     streaming DataFrame/Dataset will be written to the sink. (2)     <code>\"complete\"</code>: All the rows in the streaming DataFrame/Dataset will be     written to the sink every time there is some update. (3) <code>\"update\"</code>:     only the rows that were updated in the streaming DataFrame/Dataset will     be written to the sink every time there are some updates.     If the query doesn\u2019t contain aggregations, it will be equivalent to     append mode. Defaults to <code>\"append\"</code>.</li> <li>await_termination <code>bool</code>: Waits for the termination of this query, either by     query.stop() or by an exception. If the query has terminated with an     exception, then the exception will be thrown. If timeout is set, it     returns whether the query has terminated or not within the timeout     seconds. Defaults to <code>False</code>.</li> <li>timeout <code>int | None</code>: Only relevant in combination with <code>await_termination=True</code>.     Defaults to <code>None</code>.</li> <li>checkpoint_dir <code>str | None</code>: Checkpoint directory location. This will be used to as a reference to     from where to resume the streaming job. If <code>None</code> then hsfs will construct as     \"insert_stream_\" + online_topic_name. Defaults to <code>None</code>.     write_options: Additional write options for Spark as key-value pairs.     Defaults to <code>{}</code>.</li> </ul> <p>Returns</p> <p><code>StreamingQuery</code>: Spark Structured Streaming Query object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#json","title":"json","text":"<pre><code>FeatureGroup.json()\n</code></pre> <p>Get specific Feature Group metadata in json format.</p> <p>Example</p> <pre><code>fg.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#multi_part_insert","title":"multi_part_insert","text":"<pre><code>FeatureGroup.multi_part_insert(\n    features=None,\n    overwrite=False,\n    operation=\"upsert\",\n    storage=None,\n    write_options=None,\n    validation_options=None,\n)\n</code></pre> <p>Get FeatureGroupWriter for optimized multi part inserts or call this method to start manual multi part optimized inserts.</p> <p>In use cases where very small batches (1 to 1000) rows per Dataframe need to be written to the feature store repeatedly, it might be inefficient to use the standard <code>feature_group.insert()</code> method as it performs some background actions to update the metadata of the feature group object first.</p> <p>For these cases, the feature group provides the <code>multi_part_insert</code> API, which is optimized for writing many small Dataframes after another.</p> <p>There are two ways to use this API:</p> <p>Python Context Manager</p> <p>Using the Python <code>with</code> syntax you can acquire a FeatureGroupWriter object that implements the same <code>multi_part_insert</code> API. <pre><code>feature_group = fs.get_or_create_feature_group(\"fg_name\", version=1)\n\nwith feature_group.multi_part_insert() as writer:\n    # run inserts in a loop:\n    while loop:\n        small_batch_df = ...\n        writer.insert(small_batch_df)\n</code></pre> The writer batches the small Dataframes and transmits them to Hopsworks efficiently. When exiting the context, the feature group writer is sure to exit only once all the rows have been transmitted.</p> <p>Multi part insert with manual context management</p> <p>Instead of letting Python handle the entering and exiting of the multi part insert context, you can start and finalize the context manually. <pre><code>feature_group = fs.get_or_create_feature_group(\"fg_name\", version=1)\n\nwhile loop:\n    small_batch_df = ...\n    feature_group.multi_part_insert(small_batch_df)\n\n# IMPORTANT: finalize the multi part insert to make sure all rows\n# have been transmitted\nfeature_group.finalize_multi_part_insert()\n</code></pre> Note that the first call to <code>multi_part_insert</code> initiates the context and be sure to finalize it. The <code>finalize_multi_part_insert</code> is a blocking call that returns once all rows have been transmitted.</p> <p>Once you are done with the multi part insert, it is good practice to start the materialization job in order to write the data to the offline storage: <pre><code>feature_group.materialization_job.run(await_termination=True)\n</code></pre></p> <p>Arguments</p> <ul> <li>features <code>pandas.DataFrame | polars.dataframe.frame.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | hsfs.feature_group.pyspark.RDD | numpy.ndarray | List[list] | None</code>: Pandas DataFrame, Polars DataFrame, RDD, Ndarray, list. Features to be saved.</li> <li>overwrite <code>bool</code>: Drop all data in the feature group before     inserting new data. This does not affect metadata, defaults to False.</li> <li>operation <code>str | None</code>: Apache Hudi operation type <code>\"insert\"</code> or <code>\"upsert\"</code>.     Defaults to <code>\"upsert\"</code>.</li> <li>storage <code>str | None</code>: Overwrite default behaviour, write to offline     storage only with <code>\"offline\"</code> or online only with <code>\"online\"</code>, defaults     to <code>None</code>.</li> <li>write_options <code>Dict[str, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to write data into the   feature group.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the insert call should return only   after the Hopsworks Job has finished. By default it waits.</li> <li>key <code>start_offline_backfill</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. <code>start_offline_backfill</code> is deprecated. Use <code>start_offline_materialization</code> instead.</li> <li>key <code>start_offline_materialization</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. By default the materialization job does not get started automatically   for multi part inserts.</li> <li>key <code>kafka_producer_config</code> and value an object of type properties   used to configure the Kafka client. To optimize for throughput in high latency connection consider   changing producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established   connectivity from you Python environment to the internal advertised   listeners of the Hopsworks Kafka Cluster. Defaults to <code>False</code> and   will use external listeners when connecting from outside of Hopsworks.</li> </ul> </li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> <li>key <code>fetch_expectation_suite</code> a boolean value, by default <code>False</code> for multi part inserts,    to control whether the expectation suite of the feature group should be fetched before every insert.</li> </ul> </li> </ul> <p>Returns</p> <p>(<code>Job</code>, <code>ValidationReport</code>) A tuple with job information if python engine is used and the validation report if validation is enabled. <code>FeatureGroupWriter</code> When used as a context manager with Python <code>with</code> statement.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#prepare_spark_location","title":"prepare_spark_location","text":"<pre><code>FeatureGroup.prepare_spark_location()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#read","title":"read","text":"<pre><code>FeatureGroup.read(\n    wallclock_time=None, online=False, dataframe_type=\"default\", read_options=None\n)\n</code></pre> <p>Read the feature group into a dataframe.</p> <p>Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments.</p> <p>Set <code>online</code> to <code>True</code> to read from the online storage, or change <code>dataframe_type</code> to read as a different format.</p> <p>Read feature group as of latest state:</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\nfg.read()\n</code></pre> <p>Read feature group as of specific point in time:</p> <pre><code>fg = fs.get_or_create_feature_group(...)\nfg.read(\"2020-10-20 07:34:11\")\n</code></pre> <p>Arguments</p> <ul> <li>wallclock_time <code>str | int | datetime.datetime | datetime.date | None</code>: If specified will retrieve feature group as of specific point in time. Defaults to <code>None</code>.     If not specified, will return as of most recent time.     Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>online <code>bool</code>: bool, optional. If <code>True</code> read from online feature store, defaults     to <code>False</code>.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.      Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> <li>read_options <code>dict | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     For python engine:<ul> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> <li>key <code>\"pandas_types\"</code> and value <code>True</code> to retrieve columns as   Pandas nullable types   rather than numpy/object(string) types (experimental). Defaults to <code>{}</code>.</li> </ul> </li> </ul> <p>Returns</p> <p><code>DataFrame</code>: The spark dataframe containing the feature data. <code>pyspark.DataFrame</code>. A Spark DataFrame. <code>pandas.DataFrame</code>. A Pandas DataFrame. <code>polars.DataFrame</code>. A Polars DataFrame. <code>numpy.ndarray</code>. A two-dimensional Numpy array. <code>list</code>. A two-dimensional Python list.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. No data is available for feature group with this commit date, If time travel enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#read_changes","title":"read_changes","text":"<pre><code>FeatureGroup.read_changes(start_wallclock_time, end_wallclock_time, read_options=None)\n</code></pre> <p>Reads updates of this feature that occurred between specified points in time.</p> <p>Deprecated</p> <pre><code>    `read_changes` method is deprecated. Use\n    `as_of(end_wallclock_time, exclude_until=start_wallclock_time).read(read_options=read_options)`\n    instead.\n</code></pre> <p>Pyspark/Spark Only</p> <p>Apache HUDI exclusively supports Time Travel and Incremental Query via Spark Context</p> <p>Warning</p> <p>This function only works for feature groups with time_travel_format='HUDI'.</p> <p>Arguments</p> <ul> <li>start_wallclock_time <code>str | int | datetime.datetime | datetime.date</code>: Start time of the time travel query. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>,     <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>end_wallclock_time <code>str | int | datetime.datetime | datetime.date</code>: End time of the time travel query. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>,     <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>read_options <code>dict | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     Defaults to <code>{}</code>.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>. The spark dataframe containing the incremental changes of feature data.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.  No data is available for feature group with this commit date. <code>hsfs.client.exceptions.FeatureStoreException</code>. If the feature group does not have <code>HUDI</code> time travel format</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#save","title":"save","text":"<pre><code>FeatureGroup.save(features=None, write_options=None, validation_options=None, wait=False)\n</code></pre> <p>Persist the metadata and materialize the feature group to the feature store.</p> <p>Changed in 3.3.0</p> <p><code>insert</code> and <code>save</code> methods are now async by default in non-spark clients. To achieve the old behaviour, set <code>wait</code> argument to <code>True</code>.</p> <p>Calling <code>save</code> creates the metadata for the feature group in the feature store. If a Pandas DataFrame, Polars DatFrame, RDD or Ndarray is provided, the data is written to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if <code>online_enabled</code> for the feature group, also to the online feature store. The <code>features</code> dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments</p> <ul> <li>features <code>pandas.DataFrame | polars.dataframe.frame.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | hsfs.feature_group.pyspark.RDD | numpy.ndarray | List[hsfs.feature.Feature] | None</code>: Pandas DataFrame, Polars DataFrame, RDD, Ndarray or a list of features. Features to be saved.     This argument is optional if the feature list is provided in the create_feature_group or     in the get_or_create_feature_group method invokation.</li> <li>write_options <code>Dict[str, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to write data into the   feature group.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it does not wait.</li> <li>key <code>start_offline_backfill</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. <code>start_offline_backfill</code> is deprecated. Use <code>start_offline_materialization</code> instead.</li> <li>key <code>start_offline_materialization</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. By default the materialization job gets started immediately.</li> <li>key <code>kafka_producer_config</code> and value an object of type properties   used to configure the Kafka client. To optimize for throughput in high latency connection, consider   changing the producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established   connectivity from you Python environment to the internal advertised   listeners of the Hopsworks Kafka Cluster. Defaults to <code>False</code> and   will use external listeners when connecting from outside of Hopsworks.</li> </ul> </li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> </ul> </li> <li>wait <code>bool</code>: Wait for job to finish before returning, defaults to <code>False</code>.     Shortcut for read_options <code>{\"wait_for_job\": False}</code>.</li> </ul> <p>Returns</p> <p><code>Job</code>: When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to ingest the feature group data.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. Unable to create feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#save_expectation_suite","title":"save_expectation_suite","text":"<pre><code>FeatureGroup.save_expectation_suite(\n    expectation_suite, run_validation=True, validation_ingestion_policy=\"always\", overwrite=False\n)\n</code></pre> <p>Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.save_expectation_suite(expectation_suite, run_validation=True)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | great_expectations.core.expectation_suite.ExpectationSuite</code>: The expectation suite to attach to the Feature Group.</li> <li>overwrite <code>bool</code>: If an Expectation Suite is already attached, overwrite it.     The new suite will have its own validation history, but former reports are preserved.</li> <li>run_validation <code>bool</code>: Set whether the expectation_suite will run on ingestion</li> <li>validation_ingestion_policy <code>Literal['always', 'strict']</code>: Set the policy for ingestion to the Feature Group.<ul> <li>\"STRICT\" only allows DataFrame passing validation to be inserted into Feature Group.</li> <li>\"ALWAYS\" always insert the DataFrame to the Feature Group, irrespective of overall validation result.</li> </ul> </li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#save_validation_report","title":"save_validation_report","text":"<pre><code>FeatureGroup.save_validation_report(\n    validation_report, ingestion_result=\"UNKNOWN\", ge_type=True\n)\n</code></pre> <p>Save validation report to hopsworks platform along previous reports of the same Feature Group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(..., expectation_suite=expectation_suite)\n\nvalidation_report = great_expectations.from_pandas(\n    my_experimental_features_df,\n    fg.get_expectation_suite()).validate()\n\nfg.save_validation_report(validation_report, ingestion_result=\"EXPERIMENT\")\n</code></pre> <p>Arguments</p> <ul> <li>validation_report <code>Dict[str, Any] | hsfs.validation_report.ValidationReport | great_expectations.core.expectation_validation_result.ExpectationSuiteValidationResult</code>: The validation report to attach to the Feature Group.</li> <li>ingestion_result <code>Literal['unknown', 'experiment', 'fg_data']</code>: Specify the fate of the associated data, defaults     to \"UNKNOWN\". Supported options are  \"UNKNOWN\", \"INGESTED\", \"REJECTED\",     \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation     of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\"     for testing and development and \"FG_DATA\" when validating data     already in the Feature Group.</li> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code> if Great Expectations is installed,     else <code>False</code>.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#select","title":"select","text":"<pre><code>FeatureGroup.select(features)\n</code></pre> <p>Select a subset of features of the feature group and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\n# construct query\nquery = fg.select([\"id\", \"f1\"])\nquery.features\n# [Feature('id', ...), Feature('f1', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>features <code>List[str | hsfs.feature.Feature]</code>: A list of <code>Feature</code> objects or feature names as     strings to be selected.</li> </ul> <p>Returns</p> <p><code>Query</code>: A query object with the selected features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#select_all","title":"select_all","text":"<pre><code>FeatureGroup.select_all(include_primary_key=True, include_event_time=True)\n</code></pre> <p>Select all features along with primary key and event time from the feature group and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instances\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\n# construct the query\nquery = fg1.select_all().join(fg2.select_all())\n\n# show first 5 rows\nquery.show(5)\n\n\n# select all features exclude primary key and event time\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\nquery = fg.select_all()\nquery.features\n# [Feature('id', ...), Feature('ts', ...), Feature('f1', ...), Feature('f2', ...)]\n\nquery = fg.select_all(include_primary_key=False, include_event_time=False)\nquery.features\n# [Feature('f1', ...), Feature('f2', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>include_primary_key <code>bool | None</code>: If True, include primary key of the feature group     to the feature list. Defaults to True.</li> <li>include_event_time <code>bool | None</code>: If True, include event time of the feature group     to the feature list. Defaults to True.</li> </ul> <p>Returns</p> <p><code>Query</code>. A query object with all features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#select_except","title":"select_except","text":"<pre><code>FeatureGroup.select_except(features=None)\n</code></pre> <p>Select all features including primary key and event time feature of the feature group except provided <code>features</code> and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\n# construct query\nquery = fg.select_except([\"ts\", \"f1\"])\nquery.features\n# [Feature('id', ...), Feature('f1', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>features <code>List[str | hsfs.feature.Feature] | None</code>: A list of <code>Feature</code> objects or feature names as     strings to be excluded from the selection. Defaults to [],     selecting all features.</li> </ul> <p>Returns</p> <p><code>Query</code>: A query object with the selected features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#select_features","title":"select_features","text":"<pre><code>FeatureGroup.select_features()\n</code></pre> <p>Select all the features in the feature group and return a query object.</p> <p>Queries define the schema of Feature View objects which can be used to create Training Datasets, read from the Online Feature Store, and more. They can also be composed to create more complex queries using the <code>join</code> method.</p> <p>Info</p> <p>This method does not select the primary key and event time of the feature group. Use <code>select_all</code> to include them. Note that primary keys do not need to be included in the query to allow joining on them.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = hopsworks.login().get_feature_store()\n\n# Some dataframe to create the feature group with\n# both an event time and a primary key column\nmy_df.head()\n+------------+------------+------------+------------+\n|    id      | feature_1  |    ...     |    ts      |\n+------------+------------+------------+------------+\n|     8      |     8      |            |    15      |\n|     3      |     3      |    ...     |    6       |\n|     1      |     1      |            |    18      |\n+------------+------------+------------+------------+\n\n# Create the Feature Group instances\nfg1 = fs.create_feature_group(\n        name = \"fg1\",\n        version=1,\n        primary_key=[\"id\"],\n        event_time=\"ts\",\n    )\n\n# Insert data to the feature group.\nfg1.insert(my_df)\n\n# select all features from `fg1` excluding primary key and event time\nquery = fg1.select_features()\n\n# show first 3 rows\nquery.show(3)\n\n# Output, no id or ts columns\n+------------+------------+------------+\n| feature_1  | feature_2  | feature_3  |\n+------------+------------+------------+\n|     8      |     7      |    15      |\n|     3      |     1      |     6      |\n|     1      |     2      |    18      |\n+------------+------------+------------+\n</code></pre> <p>Example</p> <pre><code># connect to the Feature Store\nfs = hopsworks.login().get_feature_store()\n\n# Get the Feature Group from the previous example\nfg1 = fs.get_feature_group(\"fg1\", 1)\n\n# Some dataframe to create another feature group\n# with a primary key column\n+------------+------------+------------+\n|    id_2    | feature_6  | feature_7  |\n+------------+------------+------------+\n|     8      |     11     |            |\n|     3      |     4      |    ...     |\n|     1      |     9      |            |\n+------------+------------+------------+\n\n# join the two feature groups on their indexes, `id` and `id_2`\n# but does not include them in the query\nquery = fg1.select_features().join(fg2.select_features(), left_on=\"id\", right_on=\"id_2\")\n\n# show first 5 rows\nquery.show(3)\n\n# Output\n+------------+------------+------------+------------+------------+\n| feature_1  | feature_2  | feature_3  | feature_6  | feature_7  |\n+------------+------------+------------+------------+------------+\n|     8      |     7      |    15      |    11      |    15      |\n|     3      |     1      |     6      |     4      |     3      |\n|     1      |     2      |    18      |     9      |    20      |\n+------------+------------+------------+------------+------------+\n</code></pre> <p>Returns</p> <p><code>Query</code>. A query object with all features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#show","title":"show","text":"<pre><code>FeatureGroup.show(n, online=False)\n</code></pre> <p>Show the first <code>n</code> rows of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# make a query and show top 5 rows\nfg.select(['date','weekly_sales','is_holiday']).show(5)\n</code></pre> <p>Arguments</p> <ul> <li>n <code>int</code>: int. Number of rows to show.</li> <li>online <code>bool | None</code>: bool, optional. If <code>True</code> read from online feature store, defaults     to <code>False</code>.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#to_dict","title":"to_dict","text":"<pre><code>FeatureGroup.to_dict()\n</code></pre> <p>Get structured info about specific Feature Group in python dictionary format.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_deprecated","title":"update_deprecated","text":"<pre><code>FeatureGroup.update_deprecated(deprecate=True)\n</code></pre> <p>Deprecate the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_deprecated(deprecate=True)\n</code></pre> <p>Safe update</p> <p>This method updates the feature group safely. In case of failure your local metadata object will be kept unchanged.</p> <p>Arguments</p> <ul> <li>deprecate <code>bool</code>: Boolean value identifying if the feature group should be deprecated. Defaults to True.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_description","title":"update_description","text":"<pre><code>FeatureGroup.update_description(description)\n</code></pre> <p>Update the description of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_description(description=\"Much better description.\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature group description safely. In case of failure your local metadata object will keep the old description.</p> <p>Arguments</p> <ul> <li>description <code>str</code>: New description string.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_feature_description","title":"update_feature_description","text":"<pre><code>FeatureGroup.update_feature_description(feature_name, description)\n</code></pre> <p>Update the description of a single feature in this feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_feature_description(feature_name=\"min_temp\",\n                              description=\"Much better feature description.\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature description safely. In case of failure your local metadata object will keep the old description.</p> <p>Arguments</p> <ul> <li>feature_name <code>str</code>: Name of the feature to be updated.</li> <li>description <code>str</code>: New description string.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_features","title":"update_features","text":"<pre><code>FeatureGroup.update_features(features)\n</code></pre> <p>Update metadata of features in this feature group.</p> <p>Currently it's only supported to update the description of a feature.</p> <p>Unsafe update</p> <p>Note that if you use an existing <code>Feature</code> object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.feature.Feature | List[hsfs.feature.Feature]</code>: <code>Feature</code> or list of features. A feature object or list thereof to     be updated.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_from_response_json","title":"update_from_response_json","text":"<pre><code>FeatureGroup.update_from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_notification_topic_name","title":"update_notification_topic_name","text":"<pre><code>FeatureGroup.update_notification_topic_name(notification_topic_name)\n</code></pre> <p>Update the notification topic name of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_notification_topic_name(notification_topic_name=\"notification_topic_name\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature group notification topic name safely. In case of failure your local metadata object will keep the old notification topic name.</p> <p>Arguments</p> <ul> <li>notification_topic_name <code>str</code>: Name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If set to None no notifications are sent.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_statistics_config","title":"update_statistics_config","text":"<pre><code>FeatureGroup.update_statistics_config()\n</code></pre> <p>Update the statistics configuration of the feature group.</p> <p>Change the <code>statistics_config</code> object and persist the changes by calling this method.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_statistics_config()\n</code></pre> <p>Returns</p> <p><code>FeatureGroup</code>. The updated metadata object of the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#validate","title":"validate","text":"<pre><code>FeatureGroup.validate(\n    dataframe=None,\n    expectation_suite=None,\n    save_report=False,\n    validation_options=None,\n    ingestion_result=\"unknown\",\n    ge_type=True,\n)\n</code></pre> <p>Run validation based on the attached expectations.</p> <p>Runs the expectation suite attached to the feature group against the provided dataframe. Raise an error if the great_expectations package is not installed.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get feature group instance\nfg = fs.get_or_create_feature_group(...)\n\nge_report = fg.validate(df, save_report=False)\n</code></pre> <p>Arguments</p> <ul> <li>dataframe <code>pandas.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | None</code>: The dataframe to run the data validation expectations against.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | None</code>: Optionally provide an Expectation Suite to override the     one that is possibly attached to the feature group. This is useful for     testing new Expectation suites. When an extra suite is provided, the results     will never be persisted. Defaults to <code>None</code>.</li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> </ul> </li> <li>ingestion_result <code>Literal['unknown', 'ingested', 'rejected', 'fg_data', 'experiement']</code>: Specify the fate of the associated data, defaults     to \"UNKNOWN\". Supported options are  \"UNKNOWN\", \"INGESTED\", \"REJECTED\",     \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation     of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\"     for testing and development and \"FG_DATA\" when validating data     already in the Feature Group.</li> <li>save_report <code>bool | None</code>: Whether to save the report to the backend. This is only possible if the Expectation suite     is initialised and attached to the Feature Group. Defaults to False.</li> <li>ge_type <code>bool</code>: Whether to return a Great Expectations object or Hopsworks own abstraction.     Defaults to <code>True</code> if Great Expectations is installed, else <code>False</code>.</li> </ul> <p>Returns</p> <p>A Validation Report produced by Great Expectations.</p>"},{"location":"generated/api/feature_monitoring_config_api/","title":"Feature Monitoring Configuration","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#featuremonitoringconfig","title":"FeatureMonitoringConfig","text":"<pre><code>hsfs.core.feature_monitoring_config.FeatureMonitoringConfig(\n    feature_store_id,\n    name,\n    feature_name=None,\n    feature_monitoring_type=STATISTICS_COMPUTATION,\n    job_name=None,\n    detection_window_config=None,\n    reference_window_config=None,\n    statistics_comparison_config=None,\n    job_schedule=None,\n    description=None,\n    id=None,\n    feature_group_id=None,\n    feature_view_name=None,\n    feature_view_version=None,\n    href=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_monitoring_config_api/#creation-from-feature-group","title":"Creation from Feature Group","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#create_statistics_monitoring","title":"create_statistics_monitoring","text":"<pre><code>FeatureGroup.create_statistics_monitoring(\n    name,\n    feature_name=None,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable statistics monitoring\nmy_config = fg.create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str | None</code>: Name of the feature to monitor. If not specified, statistics     will be computed for all features.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#create_feature_monitoring","title":"create_feature_monitoring","text":"<pre><code>FeatureGroup.create_feature_monitoring(\n    name,\n    feature_name,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # Data inserted last week on the same day\n    time_offset=\"1w1d\",\n    window_length=\"1d\",\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str</code>: Name of the feature to monitor.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p>"},{"location":"generated/api/feature_monitoring_config_api/#creation-from-feature-view","title":"Creation from Feature View","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#create_statistics_monitoring_1","title":"create_statistics_monitoring","text":"<pre><code>FeatureView.create_statistics_monitoring(\n    name,\n    feature_name=None,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature view\nfv = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# enable statistics monitoring\nmy_config = fv._create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature view.</li> <li>feature_name <code>str | None</code>: Name of the feature to monitor. If not specified, statistics     will be computed for all features.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#create_feature_monitoring_1","title":"create_feature_monitoring","text":"<pre><code>FeatureView.create_feature_monitoring(\n    name,\n    feature_name,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature view\nfg = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # compare to a given value\n    specific_value=0.5,\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str</code>: Name of the feature to monitor.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p>"},{"location":"generated/api/feature_monitoring_config_api/#retrieval-from-feature-group","title":"Retrieval from Feature Group","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#get_feature_monitoring_configs","title":"get_feature_monitoring_configs","text":"<pre><code>FeatureGroup.get_feature_monitoring_configs(name=None, feature_name=None, config_id=None)\n</code></pre> <p>Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch all feature monitoring configs attached to the feature group\nfm_configs = fg.get_feature_monitoring_configs()\n\n# fetch a single feature monitoring config by name\nfm_config = fg.get_feature_monitoring_configs(name=\"my_config\")\n\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fg.get_feature_monitoring_configs(feature_name=\"my_feature\")\n\n# fetch a single feature monitoring config with a given id\nfm_config = fg.get_feature_monitoring_configs(config_id=1)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: If provided fetch only the feature monitoring config with the given name.     Defaults to None.</li> <li>feature_name <code>str | None</code>: If provided, fetch only configs attached to a particular feature.     Defaults to None.</li> <li>config_id <code>int | None</code>: If provided, fetch only the feature monitoring config with the given id.     Defaults to None.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both name and feature_name are provided. - TypeError: if name or feature_name are not string or None.</p> <p>Return</p> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None]     A list of feature monitoring configs. If name provided,     returns either a single config or None if not found.</p>"},{"location":"generated/api/feature_monitoring_config_api/#retrieval-from-feature-view","title":"Retrieval from Feature View","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#get_feature_monitoring_configs_1","title":"get_feature_monitoring_configs","text":"<pre><code>FeatureView.get_feature_monitoring_configs(name=None, feature_name=None, config_id=None)\n</code></pre> <p>Fetch feature monitoring configs attached to the feature view. If no arguments is provided the method will return all feature monitoring configs attached to the feature view, meaning all feature monitoring configs that are attach to a feature in the feature view. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> <p>Example</p> <pre><code># fetch your feature view\nfv = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# fetch all feature monitoring configs attached to the feature view\nfm_configs = fv.get_feature_monitoring_configs()\n# fetch a single feature monitoring config by name\nfm_config = fv.get_feature_monitoring_configs(name=\"my_config\")\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fv.get_feature_monitoring_configs(feature_name=\"my_feature\")\n# fetch a single feature monitoring config with a particular id\nfm_config = fv.get_feature_monitoring_configs(config_id=1)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: If provided fetch only the feature monitoring config with the given name.     Defaults to None.</li> <li>feature_name <code>str | None</code>: If provided, fetch only configs attached to a particular feature.     Defaults to None.</li> <li>config_id <code>int | None</code>: If provided, fetch only the feature monitoring config with the given id.     Defaults to None.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both name and feature_name are provided. - TypeError: if name or feature_name are not string or None.</p> <p>Return</p> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None]     A list of feature monitoring configs. If name provided,     returns either a single config or None if not found.</p>"},{"location":"generated/api/feature_monitoring_config_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#description","title":"description","text":"<p>Description of the feature monitoring configuration.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#detection_window_config","title":"detection_window_config","text":"<p>Configuration for the detection window.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#enabled","title":"enabled","text":"<p>Controls whether or not this config is spawning new feature monitoring jobs. This field belongs to the scheduler configuration but is made transparent to the user for convenience.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#feature_group_id","title":"feature_group_id","text":"<p>Id of the Feature Group to which this feature monitoring configuration is attached.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#feature_monitoring_type","title":"feature_monitoring_type","text":"<p>The type of feature monitoring to perform. Used for internal validation. Options are:     - STATISTICS_COMPUTATION if no reference window (and, therefore, comparison config) is provided     - STATISTICS_COMPARISON if a reference window (and, therefore, comparison config) is provided.</p> <p>This property is read-only.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#feature_name","title":"feature_name","text":"<p>The name of the feature to monitor. If not set, all features of the Feature Group or Feature View are monitored, only available for scheduled statistics.</p> <p>This property is read-only</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#feature_store_id","title":"feature_store_id","text":"<p>Id of the Feature Store.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#feature_view_name","title":"feature_view_name","text":"<p>Name of the Feature View to which this feature monitoring configuration is attached.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#feature_view_version","title":"feature_view_version","text":"<p>Version of the Feature View to which this feature monitoring configuration is attached.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#id","title":"id","text":"<p>Id of the feature monitoring configuration.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#job_name","title":"job_name","text":"<p>Name of the feature monitoring job.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#job_schedule","title":"job_schedule","text":"<p>Schedule of the feature monitoring job. This field belongs to the job configuration but is made transparent to the user for convenience.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#name","title":"name","text":"<p>The name of the feature monitoring config. A Feature Group or Feature View cannot have multiple feature monitoring configurations with the same name. The name of a feature monitoring configuration is limited to 63 characters.</p> <p>This property is read-only once the feature monitoring configuration has been saved.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#reference_window_config","title":"reference_window_config","text":"<p>Configuration for the reference window.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#statistics_comparison_config","title":"statistics_comparison_config","text":"<p>Configuration for the comparison of detection and reference statistics.</p>"},{"location":"generated/api/feature_monitoring_config_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#compare_on","title":"compare_on","text":"<pre><code>FeatureMonitoringConfig.compare_on(metric, threshold, strict=False, relative=False)\n</code></pre> <p>Sets the statistics comparison criteria for feature monitoring with a reference window.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring, a detection window and a reference window\nmy_monitoring_config = fg.create_feature_monitoring(\n    ...\n).with_detection_window(...).with_reference_window(...)\n# Choose a metric and set a threshold for the difference\n# e.g compare the relative mean of detection and reference window\nmy_monitoring_config.compare_on(\n    metric=\"mean\",\n    threshold=1.0,\n    relative=True,\n).save()\n</code></pre> <p>Note</p> <p>Detection window and reference window/value/training_dataset must be set prior to comparison configuration.</p> <p>Arguments</p> <ul> <li>metric <code>str | None</code>: The metric to use for comparison. Different metric are available for different feature type.</li> <li>threshold <code>float | None</code>: The threshold to apply to the difference to potentially trigger an alert.</li> <li>strict <code>bool | None</code>: Whether to use a strict comparison (e.g. &gt; or &lt;) or a non-strict comparison (e.g. &gt;= or &lt;=).</li> <li>relative <code>bool | None</code>: Whether to use a relative comparison (e.g. relative mean) or an absolute comparison (e.g. absolute mean).</li> </ul> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#delete","title":"delete","text":"<pre><code>FeatureMonitoringConfig.delete()\n</code></pre> <p>Deletes the feature monitoring configuration.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Delete the feature monitoring config\nmy_monitoring_config.delete()\n</code></pre> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#disable","title":"disable","text":"<pre><code>FeatureMonitoringConfig.disable()\n</code></pre> <p>Disables the schedule of the feature monitoring job.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Disable the feature monitoring config\nmy_monitoring_config.disable()\n</code></pre> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#enable","title":"enable","text":"<pre><code>FeatureMonitoringConfig.enable()\n</code></pre> <p>Enables the schedule of the feature monitoring job. The scheduler can be configured via the <code>job_schedule</code> property.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Enable the feature monitoring config\nmy_monitoring_config.enable()\n</code></pre> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#get_history","title":"get_history","text":"<pre><code>FeatureMonitoringConfig.get_history(start_time=None, end_time=None, with_statistics=True)\n</code></pre> <p>Fetch the history of the computed statistics and comparison results for this configuration.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Fetch the history of the computed statistics for this configuration\nhistory = my_monitoring_config.get_history(\n    start_time=\"2021-01-01\",\n    end_time=\"2021-01-31\",\n)\n</code></pre> <p>Args:</p> <p>start_time: The start time of the time range to fetch the history for. end_time: The end time of the time range to fetch the history for. with_statistics: Whether to include the computed statistics in the results.</p> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#get_job","title":"get_job","text":"<pre><code>FeatureMonitoringConfig.get_job()\n</code></pre> <p>Get the feature monitoring job which computes and compares statistics on the detection and reference windows.</p> <p>Example</p> <pre><code># Fetch registered config by name via feature group or feature view\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Get the job which computes statistics on detection and reference window\njob = my_monitoring_config.get_job()\n# Print job history and ongoing executions\njob.executions\n</code></pre> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul> <p>Returns</p> <p><code>Job</code>. A handle for the job computing the statistics.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#run_job","title":"run_job","text":"<pre><code>FeatureMonitoringConfig.run_job()\n</code></pre> <p>Trigger the feature monitoring job which computes and compares statistics on the detection and reference windows.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Trigger the feature monitoring job once\nmy_monitoring_config.run_job()\n</code></pre> <p>Info</p> <p>The feature monitoring job will be triggered asynchronously and the method will return immediately. Calling this method does not affect the ongoing schedule.</p> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul> <p>Returns</p> <p><code>Job</code>. A handle for the job computing the statistics.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#save","title":"save","text":"<pre><code>FeatureMonitoringConfig.save()\n</code></pre> <p>Saves the feature monitoring configuration.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring and a detection window\nmy_monitoring_config = fg.create_statistics_monitoring(\n    name=\"my_monitoring_config\",\n).save()\n</code></pre> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The saved FeatureMonitoringConfig object.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#update","title":"update","text":"<pre><code>FeatureMonitoringConfig.update()\n</code></pre> <p>Updates allowed fields of the saved feature monitoring configuration.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Update the percentage of rows to use when computing the statistics\nmy_monitoring_config.detection_window.row_percentage = 10\nmy_monitoring_config.update()\n</code></pre> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#with_detection_window","title":"with_detection_window","text":"<pre><code>FeatureMonitoringConfig.with_detection_window(\n    time_offset=None, window_length=None, row_percentage=None\n)\n</code></pre> <p>Sets the detection window of data to compute statistics on.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Compute statistics on a regular basis\nfg.create_statistics_monitoring(\n    name=\"regular_stats\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    time_offset=\"1d\",\n    window_length=\"1d\",\n    row_percentage=0.1,\n).save()\n# Compute and compare statistics\nfg.create_feature_monitoring(\n    name=\"regular_stats\",\n    feature_name=\"my_feature\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    time_offset=\"1d\",\n    window_length=\"1d\",\n    row_percentage=0.1,\n).with_reference_window(...).compare_on(...).save()\n</code></pre> <p>Arguments</p> <ul> <li>time_offset <code>str | None</code>: The time offset from the current time to the start of the time window.</li> <li>window_length <code>str | None</code>: The length of the time window.</li> <li>row_percentage <code>float | None</code>: The fraction of rows to use when computing the statistics [0, 1.0].</li> </ul> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#with_reference_training_dataset","title":"with_reference_training_dataset","text":"<pre><code>FeatureMonitoringConfig.with_reference_training_dataset(training_dataset_version=None)\n</code></pre> <p>Sets the reference training dataset to compare statistics with. See also <code>with_reference_value(...)</code> and <code>with_reference_window(...)</code> for other reference options.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring and a detection window\nmy_monitoring_config = fg.create_feature_monitoring(...).with_detection_window(...)\n# Only for feature views: Compare to the statistics computed for one of your training datasets\n# particularly useful if it has been used to train a model currently in production\nmy_monitoring_config.with_reference_training_dataset(\n    training_dataset_version=3,\n).compare_on(...).save()\n</code></pre> <p>Provide a comparison configuration</p> <p>You must provide a comparison configuration via <code>compare_on()</code> before saving the feature monitoring config.</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: The version of the training dataset to use as reference.</li> </ul> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#with_reference_value","title":"with_reference_value","text":"<pre><code>FeatureMonitoringConfig.with_reference_value(value=None)\n</code></pre> <p>Sets the reference value to compare statistics with. See also <code>with_reference_window(...)</code> and <code>with_reference_training_dataset(...)</code> for other reference options.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring and a detection window\nmy_monitoring_config = fg.create_feature_monitoring(...).with_detection_window(...)\n# Simplest reference window is a specific value\nmy_monitoring_config.with_reference_value(\n    value=0.0,\n).compare_on(...).save()\n</code></pre> <p>Provide a comparison configuration</p> <p>You must provide a comparison configuration via <code>compare_on()</code> before saving the feature monitoring config.</p> <p>Arguments</p> <ul> <li>value <code>float | int | None</code>: A float value to use as reference.</li> </ul> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#with_reference_window","title":"with_reference_window","text":"<pre><code>FeatureMonitoringConfig.with_reference_window(\n    time_offset=None, window_length=None, row_percentage=None\n)\n</code></pre> <p>Sets the reference window of data to compute statistics on. See also <code>with_reference_value(...)</code> and <code>with_reference_training_dataset(...)</code> for other reference options.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring and a detection window\nmy_monitoring_config = fg.create_feature_monitoring(...).with_detection_window(...)\n# Statistics computed on a rolling time window, e.g. same day last week\nmy_monitoring_config.with_reference_window(\n    time_offset=\"1w\",\n    window_length=\"1d\",\n).compare_on(...).save()\n</code></pre> <p>Provide a comparison configuration</p> <p>You must provide a comparison configuration via <code>compare_on()</code> before saving the feature monitoring config.</p> <p>Arguments</p> <ul> <li>time_offset <code>str | None</code>: The time offset from the current time to the start of the time window.</li> <li>window_length <code>str | None</code>: The length of the time window.</li> <li>row_percentage <code>float | None</code>: The percentage of rows to use when computing the statistics. Defaults to 20%.</li> </ul> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p>"},{"location":"generated/api/feature_monitoring_result_api/","title":"Feature Monitoring Result","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#featuremonitoringresult","title":"FeatureMonitoringResult","text":"<pre><code>hsfs.core.feature_monitoring_result.FeatureMonitoringResult(\n    feature_store_id,\n    execution_id,\n    monitoring_time,\n    config_id,\n    feature_name,\n    difference=None,\n    shift_detected=False,\n    detection_statistics_id=None,\n    reference_statistics_id=None,\n    empty_detection_window=False,\n    empty_reference_window=False,\n    specific_value=None,\n    raised_exception=False,\n    detection_statistics=None,\n    reference_statistics=None,\n    id=None,\n    href=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_monitoring_result_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#get_history","title":"get_history","text":"<pre><code>FeatureMonitoringConfig.get_history(start_time=None, end_time=None, with_statistics=True)\n</code></pre> <p>Fetch the history of the computed statistics and comparison results for this configuration.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Fetch the history of the computed statistics for this configuration\nhistory = my_monitoring_config.get_history(\n    start_time=\"2021-01-01\",\n    end_time=\"2021-01-31\",\n)\n</code></pre> <p>Args:</p> <p>start_time: The start time of the time range to fetch the history for. end_time: The end time of the time range to fetch the history for. with_statistics: Whether to include the computed statistics in the results.</p> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul>"},{"location":"generated/api/feature_monitoring_result_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#config_id","title":"config_id","text":"<p>Id of the feature monitoring configuration containing this result.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#detection_statistics","title":"detection_statistics","text":"<p>Feature descriptive statistics computed on the detection window.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#detection_statistics_id","title":"detection_statistics_id","text":"<p>Id of the feature descriptive statistics computed on the detection window.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#difference","title":"difference","text":"<p>Difference between detection and reference values. It can be relative or absolute difference, depending on the statistics comparison configuration provided in <code>relative</code> parameter passed to <code>compare_on()</code> when enabling feature monitoring.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#empty_detection_window","title":"empty_detection_window","text":"<p>Whether or not the detection window was empty in this feature monitoring run.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#empty_reference_window","title":"empty_reference_window","text":"<p>Whether or not the reference window was empty in this feature monitoring run.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#execution_id","title":"execution_id","text":"<p>Execution id of the feature monitoring job.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#feature_name","title":"feature_name","text":"<p>Name of the feature being monitored.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#feature_store_id","title":"feature_store_id","text":"<p>Id of the Feature Store.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#id","title":"id","text":"<p>Id of the feature monitoring result.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#monitoring_time","title":"monitoring_time","text":"<p>Time at which this feature monitoring result was created.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#reference_statistics","title":"reference_statistics","text":"<p>Feature descriptive statistics computed on the reference window.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#reference_statistics_id","title":"reference_statistics_id","text":"<p>Id of the feature descriptive statistics computed on the reference window.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#shift_detected","title":"shift_detected","text":"<p>Whether or not shift was detected in the detection window based on the computed statistics and the threshold provided in <code>compare_on()</code> when enabling feature monitoring.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#specific_value","title":"specific_value","text":"<p>Specific value used as reference in the statistics comparison.</p>"},{"location":"generated/api/feature_monitoring_window_config_api/","title":"Feature Monitoring Window Configuration","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#monitoringwindowconfig","title":"MonitoringWindowConfig","text":"<pre><code>hsfs.core.monitoring_window_config.MonitoringWindowConfig(\n    id=None,\n    window_config_type=SPECIFIC_VALUE,\n    time_offset=None,\n    window_length=None,\n    training_dataset_version=None,\n    specific_value=None,\n    row_percentage=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_monitoring_window_config_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#id","title":"id","text":"<p>Id of the window configuration.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#row_percentage","title":"row_percentage","text":"<p>The percentage of rows to fetch and compute the statistics on. Only used for windows of type <code>ROLLING_TIME</code> and <code>ALL_TIME</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#specific_value","title":"specific_value","text":"<p>The specific value to use as reference. Only used for windows of type <code>SPECIFIC_VALUE</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#time_offset","title":"time_offset","text":"<p>The time offset from the current time to the start of the time window. Only used for windows of type <code>ROLLING_TIME</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#training_dataset_version","title":"training_dataset_version","text":"<p>The version of the training dataset to use as reference. Only used for windows of type <code>TRAINING_DATASET</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#window_config_type","title":"window_config_type","text":"<p>Type of the window. It can be one of <code>ALL_TIME</code>, <code>ROLLING_TIME</code>, <code>TRAINING_DATASET</code> or <code>SPECIFIC_VALUE</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#window_length","title":"window_length","text":"<p>The length of the time window. Only used for windows of type <code>ROLLING_TIME</code>.</p>"},{"location":"generated/api/feature_store_api/","title":"Feature Store","text":"<p>[source]</p>"},{"location":"generated/api/feature_store_api/#featurestore","title":"FeatureStore","text":"<pre><code>hsfs.feature_store.FeatureStore(\n    featurestore_id,\n    featurestore_name,\n    created,\n    project_name,\n    project_id,\n    offline_featurestore_name,\n    online_enabled,\n    num_feature_groups=None,\n    num_training_datasets=None,\n    num_storage_connectors=None,\n    num_feature_views=None,\n    online_featurestore_name=None,\n    online_featurestore_size=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_store_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_feature_store","title":"get_feature_store","text":"<pre><code>Connection.get_feature_store(name=None)\n</code></pre> <p>Get a reference to a feature store to perform operations on.</p> <p>Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required.</p> <p>Arguments</p> <ul> <li>name <code>str | None</code>: The name of the feature store, defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>FeatureStore</code>. A feature store handle object to perform operations on.</p>"},{"location":"generated/api/feature_store_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_store_api/#id","title":"id","text":"<p>Id of the feature store.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#name","title":"name","text":"<p>Name of the feature store.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#offline_featurestore_name","title":"offline_featurestore_name","text":"<p>Name of the offline feature store database.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#online_enabled","title":"online_enabled","text":"<p>Indicator whether online feature store is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#online_featurestore_name","title":"online_featurestore_name","text":"<p>Name of the online feature store database.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#project_id","title":"project_id","text":"<p>Id of the project in which the feature store is located.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#project_name","title":"project_name","text":"<p>Name of the project in which the feature store is located.</p>"},{"location":"generated/api/feature_store_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/feature_store_api/#create_external_feature_group","title":"create_external_feature_group","text":"<pre><code>FeatureStore.create_external_feature_group(\n    name,\n    storage_connector,\n    query=None,\n    data_format=None,\n    path=\"\",\n    options=None,\n    version=None,\n    description=\"\",\n    primary_key=None,\n    embedding_index=None,\n    features=None,\n    statistics_config=None,\n    event_time=None,\n    expectation_suite=None,\n    online_enabled=False,\n    topic_name=None,\n    notification_topic_name=None,\n    online_config=None,\n)\n</code></pre> <p>Create a external feature group metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fg = fs.create_external_feature_group(\n                    name=\"sales\",\n                    version=1,\n                    description=\"Physical shop sales features\",\n                    query=query,\n                    storage_connector=connector,\n                    primary_key=['ss_store_sk'],\n                    event_time='sale_date'\n                    )\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the <code>save()</code> method.</p> <p>You can enable online storage for external feature groups, however, the sync from the external storage to Hopsworks online storage needs to be done manually:</p> <pre><code>external_fg = fs.create_external_feature_group(\n            name=\"sales\",\n            version=1,\n            description=\"Physical shop sales features\",\n            query=query,\n            storage_connector=connector,\n            primary_key=['ss_store_sk'],\n            event_time='sale_date',\n            online_enabled=True,\n            online_config={'online_comments': ['NDB_TABLE=READ_BACKUP=1']}\n            )\nexternal_fg.save()\n\n# read from external storage and filter data to sync to online\ndf = external_fg.read().filter(external_fg.customer_status == \"active\")\n\n# insert to online storage\nexternal_fg.insert(df)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to create.</li> <li>storage_connector <code>hsfs.StorageConnector</code>: the storage connector used to establish connectivity     with the data source.</li> <li>query <code>str | None</code>: A string containing a SQL query valid for the target data source.     the query will be used to pull data from the data sources when the     feature group is used.</li> <li>data_format <code>str | None</code>: If the external feature groups refers to a directory with data,     the data format to use when reading it</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> <li>options <code>Dict[str, str] | None</code>: Additional options to be used by the engine when reading data from the     specified storage connector. For example, <code>{\"header\": True}</code> when reading     CSV files with column names in the first row.</li> <li>version <code>int | None</code>: Version of the external feature group to retrieve, defaults to <code>None</code> and     will create the feature group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the external feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the external feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame resulting by executing the provided query     against the data source.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this external feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li> <p>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> <ul> <li>__ online_enabled__: Define whether it should be possible to sync the feature group to the online feature store for low latency access, defaults to <code>False</code>.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | hsfs.feature_store.great_expectations.core.ExpectationSuite | None</code>: Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to <code>None</code>.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> <li>online_config <code>hsfs.online_config.OnlineConfig | Dict[str, Any] | None</code>: Optionally, define configuration which is used to configure online table.</li> </ul> </li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>. The external feature group metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#create_feature_group","title":"create_feature_group","text":"<pre><code>FeatureStore.create_feature_group(\n    name,\n    version=None,\n    description=\"\",\n    online_enabled=False,\n    time_travel_format=\"HUDI\",\n    partition_key=None,\n    primary_key=None,\n    embedding_index=None,\n    hudi_precombine_key=None,\n    features=None,\n    statistics_config=None,\n    event_time=None,\n    stream=False,\n    expectation_suite=None,\n    parents=None,\n    topic_name=None,\n    notification_topic_name=None,\n    transformation_functions=None,\n    online_config=None,\n    offline_backfill_every_hr=None,\n    storage_connector=None,\n    path=None,\n)\n</code></pre> <p>Create a feature group metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# define the on-demand transformation functions\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n@udf(int)\ndef plus_two(value):\n    return value + 2\n\n# construct list of \"transformation functions\" on features\ntransformation_functions = [plus_one(\"feature1\"), plus_two(\"feature2\"))]\n\nfg = fs.create_feature_group(\n        name='air_quality',\n        description='Air Quality characteristics of each day',\n        version=1,\n        primary_key=['city','date'],\n        online_enabled=True,\n        event_time='date',\n        transformation_functions=transformation_functions,\n        online_config={'online_comments': ['NDB_TABLE=READ_BACKUP=1']}\n    )\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the <code>save()</code> method with a DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to create.</li> <li>version <code>int | None</code>: Version of the feature group to retrieve, defaults to <code>None</code> and     will create the feature group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>online_enabled <code>bool | None</code>: Define whether the feature group should be made available     also in the online feature store for low latency access, defaults to     <code>False</code>.</li> <li>time_travel_format <code>str | None</code>: Format used for time travel, defaults to <code>\"HUDI\"</code>.</li> <li>partition_key <code>List[str] | None</code>: A list of feature names to be used as partition key when     writing the feature data to the offline storage, defaults to empty list     <code>[]</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>embedding_index <code>hsfs.embedding.EmbeddingIndex | None</code>: <code>EmbeddingIndex</code>. If an embedding index is provided,     vector database is used as online feature store. This enables similarity search by     using <code>find_neighbors</code>.     default to <code>None</code></li> <li>hudi_precombine_key <code>str | None</code>: A feature name to be used as a precombine key for the <code>\"HUDI\"</code>     feature group. Defaults to <code>None</code>. If feature group has time travel format     <code>\"HUDI\"</code> and hudi precombine key was not specified then the first primary key of     the feature group will be used as hudi precombine key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame provided in the <code>save</code> method.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li> <p>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ stream__: Optionally, Define whether the feature group should support real time stream writing capabilities.     Stream enabled Feature Groups have unified single API for writing streaming features transparently     to both online and offline store.</p> </li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | hsfs.feature_store.great_expectations.core.ExpectationSuite | None</code>: Optionally, attach an expectation suite to the feature     group which dataframes should be validated against upon insertion.     Defaults to <code>None</code>.</li> <li>parents <code>List[hsfs.feature_group.FeatureGroup] | None</code>: Optionally, Define the parents of this feature group as the     origin where the data is coming from.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it     defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> <li>transformation_functions <code>List[hsfs.transformation_function.TransformationFunction | hsfs.hopsworks_udf.HopsworksUdf] | None</code>: On-Demand Transformation functions attached to the feature group.     It can be a list of list of user defined functions defined using the hopsworks <code>@udf</code> decorator.     Defaults to <code>None</code>, no transformations.</li> <li>online_config <code>hsfs.online_config.OnlineConfig | Dict[str, Any] | None</code>: Optionally, define configuration which is used to configure online table.</li> <li>offline_backfill_every_hr <code>int | str | None</code>: Optional. If specified, the materialization job will be scheduled to run     periodically. The value can be either an integer representing the number of hours between each run     or a string representing a cron expression. Set the value to None to avoid scheduling the materialization     job. Defaults to None (i.e no scheduling).</li> <li>storage_connector <code>hsfs.StorageConnector | Dict[str, Any]</code>: the storage connector used to establish connectivity     with the data source.</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The feature group metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#create_feature_view","title":"create_feature_view","text":"<pre><code>FeatureStore.create_feature_view(\n    name,\n    query,\n    version=None,\n    description=\"\",\n    labels=None,\n    inference_helper_columns=None,\n    training_helper_columns=None,\n    transformation_functions=None,\n    logging_enabled=False,\n)\n</code></pre> <p>Create a feature view metadata object and saved it to hopsworks.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the feature group instances\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\n# construct the query\nquery = fg1.select_all().join(fg2.select_all())\n\n# define the transformation function as a Hopsworks's UDF\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n# construct list of \"transformation functions\" on features\ntransformation_functions = [plus_one(\"feature1\"), plus_one(\"feature1\"))]\n\nfeature_view = fs.create_feature_view(\n    name='air_quality_fv',\n    version=1,\n    transformation_functions=transformation_functions,\n    query=query\n)\n</code></pre> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# define list of transformation functions\nmapping_transformers = ...\n\n# create feature view\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    version=1,\n    transformation_functions=mapping_transformers,\n    query=query\n)\n</code></pre> <p>Warning</p> <p><code>as_of</code> argument in the <code>Query</code> will be ignored because feature view does not support time travel query.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to create.</li> <li>query <code>hsfs.constructor.query.Query</code>: Feature store <code>Query</code>.</li> <li>version <code>int | None</code>: Version of the feature view to create, defaults to <code>None</code> and     will create the feature view with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the feature view to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>labels <code>List[str] | None</code>: A list of feature names constituting the prediction label/feature of     the feature view. When replaying a <code>Query</code> during model inference,     the label features can be omitted from the feature vector retrieval.     Defaults to <code>[]</code>, no label.</li> <li>inference_helper_columns <code>List[str] | None</code>: A list of feature names that are not used in training the model itself but can be     used during batch or online inference for extra information. Inference helper column name(s) must be     part of the <code>Query</code> object. If inference helper column name(s) belong to feature group that is part     of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to be prepended to the original column name     when defining <code>inference_helper_columns</code> list. When replaying a <code>Query</code> during model inference,     the inference helper columns optionally can be omitted during batch (<code>get_batch_data</code>) and will be     omitted during online  inference (<code>get_feature_vector(s)</code>). To get inference helper column(s) during     online inference use <code>get_inference_helper(s)</code> method. Defaults to `[], no helper columns.</li> <li>training_helper_columns <code>List[str] | None</code>: A list of feature names that are not the part of the model schema itself but can be     used during training as a helper for extra information. Training helper column name(s) must be     part of the <code>Query</code> object. If training helper column name(s) belong to feature group that is part     of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to prepended to the original column name when     defining <code>training_helper_columns</code> list. When replaying a <code>Query</code> during model inference,     the training helper columns will be omitted during both batch and online inference.     Training helper columns can be optionally fetched with training data. For more details see     documentation for feature view's get training data methods.  Defaults to `[], no training helper     columns.</li> <li>transformation_functions <code>List[hsfs.transformation_function.TransformationFunction | hsfs.hopsworks_udf.HopsworksUdf] | None</code>: Model Dependent Transformation functions attached to the feature view.     It can be a list of list of user defined functions defined using the hopsworks <code>@udf</code> decorator.     Defaults to <code>None</code>, no transformations.</li> </ul> <p>Returns:</p> <p><code>FeatureView</code>: The feature view metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#create_on_demand_feature_group","title":"create_on_demand_feature_group","text":"<pre><code>FeatureStore.create_on_demand_feature_group(\n    name,\n    storage_connector,\n    query=None,\n    data_format=None,\n    path=\"\",\n    options=None,\n    version=None,\n    description=\"\",\n    primary_key=None,\n    features=None,\n    statistics_config=None,\n    event_time=None,\n    expectation_suite=None,\n    topic_name=None,\n    notification_topic_name=None,\n)\n</code></pre> <p>Create a external feature group metadata object.</p> <p>Deprecated</p> <p><code>create_on_demand_feature_group</code> method is deprecated. Use the <code>create_external_feature_group</code> method instead.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the <code>save()</code> method.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to create.</li> <li>storage_connector <code>hsfs.StorageConnector</code>: the storage connector used to establish connectivity     with the data source.</li> <li>query <code>str | None</code>: A string containing a SQL query valid for the target data source.     the query will be used to pull data from the data sources when the     feature group is used.</li> <li>data_format <code>str | None</code>: If the external feature groups refers to a directory with data,     the data format to use when reading it</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> <li>options <code>Dict[str, str] | None</code>: Additional options to be used by the engine when reading data from the     specified storage connector. For example, <code>{\"header\": True}</code> when reading     CSV files with column names in the first row.</li> <li>version <code>int | None</code>: Version of the external feature group to retrieve, defaults to <code>None</code> and     will create the feature group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the external feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the external feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame resulting by executing the provided query     against the data source.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this external feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.     !!! note \"Event time data type restriction\"         The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it     defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | hsfs.feature_store.great_expectations.core.ExpectationSuite | None</code>: Optionally, attach an expectation suite to the feature     group which dataframes should be validated against upon insertion.     Defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>. The external feature group metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#create_training_dataset","title":"create_training_dataset","text":"<pre><code>FeatureStore.create_training_dataset(\n    name,\n    version=None,\n    description=\"\",\n    data_format=\"tfrecords\",\n    coalesce=False,\n    storage_connector=None,\n    splits=None,\n    location=\"\",\n    seed=None,\n    statistics_config=None,\n    label=None,\n    transformation_functions=None,\n    train_split=None,\n)\n</code></pre> <p>Create a training dataset metadata object.</p> <p>Deprecated</p> <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead. From version 3.0 training datasets created with this API are not visibile in the API anymore.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the <code>save()</code> method with a <code>DataFrame</code> or <code>Query</code>.</p> <p>Data Formats</p> <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the training dataset to create.</li> <li>version <code>int | None</code>: Version of the training dataset to retrieve, defaults to <code>None</code> and     will create the training dataset with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>data_format <code>str | None</code>: The data format used to save the training dataset,     defaults to <code>\"tfrecords\"</code>-format.</li> <li>coalesce <code>bool | None</code>: If true the training dataset data will be coalesced into     a single partition before writing. The resulting training dataset     will be a single file per split. Default False.</li> <li>storage_connector <code>hsfs.StorageConnector | None</code>: Storage connector defining the sink location for the     training dataset, defaults to <code>None</code>, and materializes training dataset     on HopsFS.</li> <li>splits <code>Dict[str, float] | None</code>: A dictionary defining training dataset splits to be created. Keys in     the dictionary define the name of the split as <code>str</code>, values represent     percentage of samples in the split as <code>float</code>. Currently, only random     splits are supported. Defaults to empty dict<code>{}</code>, creating only a single     training dataset without splits.</li> <li>location <code>str | None</code>: Path to complement the sink storage connector with, e.g if the     storage connector points to an S3 bucket, this path can be used to     define a sub-directory inside the bucket to place the training dataset.     Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the     storage connector.</li> <li>seed <code>int | None</code>: Optionally, define a seed to create the random splits with, in order     to guarantee reproducability, defaults to <code>None</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>label <code>List[str] | None</code>: A list of feature names constituting the prediction label/feature of     the training dataset. When replaying a <code>Query</code> during model inference,     the label features can be omitted from the feature vector retrieval.     Defaults to <code>[]</code>, no label.</li> <li>transformation_functions <code>Dict[str, hsfs.transformation_function.TransformationFunction] | None</code>: A dictionary mapping tansformation functions to     to the features they should be applied to before writing out the     training data and at inference time. Defaults to <code>{}</code>, no     transformations.</li> <li>train_split <code>str | None</code>: If <code>splits</code> is set, provide the name of the split that is going     to be used for training. The statistics of this split will be used for     transformation functions if necessary. Defaults to <code>None</code>.</li> </ul> <p>Returns:</p> <p><code>TrainingDataset</code>: The training dataset metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#create_transformation_function","title":"create_transformation_function","text":"<pre><code>FeatureStore.create_transformation_function(transformation_function, version=None)\n</code></pre> <p>Create a transformation function metadata object.</p> <p>Example</p> <pre><code># define the transformation function as a Hopsworks's UDF\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n# create transformation function\nplus_one_meta = fs.create_transformation_function(\n        transformation_function=plus_one,\n        version=1\n    )\n\n# persist transformation function in backend\nplus_one_meta.save()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the <code>save()</code> method of the transformation function metadata object.</p> <p>Arguments</p> <ul> <li>transformation_function <code>hsfs.hopsworks_udf.HopsworksUdf</code>: Hopsworks UDF.</li> </ul> <p>Returns:</p> <p><code>TransformationFunction</code>: The TransformationFunction metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#from_response_json","title":"from_response_json","text":"<pre><code>FeatureStore.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_external_feature_group","title":"get_external_feature_group","text":"<pre><code>FeatureStore.get_external_feature_group(name, version=None)\n</code></pre> <p>Get a external feature group entity from the feature store.</p> <p>Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fg = fs.get_external_feature_group(\"external_fg_test\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to get.</li> <li>version <code>int</code>: Version of the external feature group to retrieve,     defaults to <code>None</code> and will return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>: The external feature group metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_external_feature_groups","title":"get_external_feature_groups","text":"<pre><code>FeatureStore.get_external_feature_groups(name)\n</code></pre> <p>Get a list of all versions of an external feature group entity from the feature store.</p> <p>Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fgs_list = fs.get_external_feature_groups(\"external_fg_test\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to get.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>: List of external feature group metadata objects.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_feature_group","title":"get_feature_group","text":"<pre><code>FeatureStore.get_feature_group(name, version=None)\n</code></pre> <p>Get a feature group entity from the feature store.</p> <p>Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_feature_group(\n        name=\"electricity_prices\",\n        version=1,\n    )\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to get.</li> <li>version <code>int | None</code>: Version of the feature group to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>: The feature group metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_feature_groups","title":"get_feature_groups","text":"<pre><code>FeatureStore.get_feature_groups(name)\n</code></pre> <p>Get a list of all versions of a feature group entity from the feature store.</p> <p>Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfgs_list = fs.get_feature_groups(\n        name=\"electricity_prices\"\n    )\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to get.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>: List of feature group metadata objects.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_feature_view","title":"get_feature_view","text":"<pre><code>FeatureStore.get_feature_view(name, version=None)\n</code></pre> <p>Get a feature view entity from the feature store.</p> <p>Getting a feature view from the Feature Store means getting its metadata.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(\n    name='feature_view_name',\n    version=1\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to get.</li> <li>version <code>int</code>: Version of the feature view to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>FeatureView</code>: The feature view metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature view from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_feature_views","title":"get_feature_views","text":"<pre><code>FeatureStore.get_feature_views(name)\n</code></pre> <p>Get a list of all versions of a feature view entity from the feature store.</p> <p>Getting a feature view from the Feature Store means getting its metadata.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get a list of all versions of a feature view\nfeature_view = fs.get_feature_views(\n    name='feature_view_name'\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to get.</li> </ul> <p>Returns</p> <p><code>FeatureView</code>: List of feature view metadata objects.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature view from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_group","title":"get_on_demand_feature_group","text":"<pre><code>FeatureStore.get_on_demand_feature_group(name, version=None)\n</code></pre> <p>Get a external feature group entity from the feature store.</p> <p>Deprecated</p> <p><code>get_on_demand_feature_group</code> method is deprecated. Use the <code>get_external_feature_group</code> method instead.</p> <p>Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to get.</li> <li>version <code>int</code>: Version of the external feature group to retrieve,     defaults to <code>None</code> and will return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>: The external feature group metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_groups","title":"get_on_demand_feature_groups","text":"<pre><code>FeatureStore.get_on_demand_feature_groups(name)\n</code></pre> <p>Get a list of all versions of an external feature group entity from the feature store.</p> <p>Deprecated</p> <p><code>get_on_demand_feature_groups</code> method is deprecated. Use the <code>get_external_feature_groups</code> method instead.</p> <p>Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to get.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>: List of external feature group metadata objects.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_online_storage_connector","title":"get_online_storage_connector","text":"<pre><code>FeatureStore.get_online_storage_connector()\n</code></pre> <p>Get the storage connector for the Online Feature Store of the respective project's feature store.</p> <p>The returned storage connector depends on the project that you are connected to.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nonline_storage_connector = fs.get_online_storage_connector()\n</code></pre> <p>Returns</p> <p><code>StorageConnector</code>. JDBC storage connector to the Online Feature Store.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_or_create_feature_group","title":"get_or_create_feature_group","text":"<pre><code>FeatureStore.get_or_create_feature_group(\n    name,\n    version,\n    description=\"\",\n    online_enabled=False,\n    time_travel_format=\"HUDI\",\n    partition_key=None,\n    primary_key=None,\n    embedding_index=None,\n    hudi_precombine_key=None,\n    features=None,\n    statistics_config=None,\n    expectation_suite=None,\n    event_time=None,\n    stream=False,\n    parents=None,\n    topic_name=None,\n    notification_topic_name=None,\n    transformation_functions=None,\n    online_config=None,\n    offline_backfill_every_hr=None,\n    storage_connector=None,\n    path=None,\n)\n</code></pre> <p>Get feature group metadata object or create a new one if it doesn't exist. This method doesn't update existing feature group metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_or_create_feature_group(\n        name=\"electricity_prices\",\n        version=1,\n        description=\"Electricity prices from NORD POOL\",\n        primary_key=[\"day\", \"area\"],\n        online_enabled=True,\n        event_time=\"timestamp\",\n        transformation_functions=transformation_functions,\n        online_config={'online_comments': ['NDB_TABLE=READ_BACKUP=1']}\n        )\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the <code>insert()</code> method with a DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to create.</li> <li>version <code>int</code>: Version of the feature group to retrieve or create.</li> <li>description <code>str | None</code>: A string describing the contents of the feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>online_enabled <code>bool | None</code>: Define whether the feature group should be made available     also in the online feature store for low latency access, defaults to     <code>False</code>.</li> <li>time_travel_format <code>str | None</code>: Format used for time travel, defaults to <code>\"HUDI\"</code>.</li> <li>partition_key <code>List[str] | None</code>: A list of feature names to be used as partition key when     writing the feature data to the offline storage, defaults to empty list     <code>[]</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>embedding_index <code>hsfs.embedding.EmbeddingIndex | None</code>: <code>EmbeddingIndex</code>. If an embedding index is provided,     the vector database is used as online feature store. This enables similarity search by     using <code>find_neighbors</code>.     default is <code>None</code></li> <li>hudi_precombine_key <code>str | None</code>: A feature name to be used as a precombine key for the <code>\"HUDI\"</code>     feature group. Defaults to <code>None</code>. If feature group has time travel format     <code>\"HUDI\"</code> and hudi precombine key was not specified then the first primary key of     the feature group will be used as hudi precombine key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame provided in the <code>save</code> method.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | hsfs.feature_store.great_expectations.core.ExpectationSuite | None</code>: Optionally, attach an expectation suite to the feature     group which dataframes should be validated against upon insertion.     Defaults to <code>None</code>.</li> <li> <p>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ stream__: Optionally, Define whether the feature group should support real time stream writing capabilities.     Stream enabled Feature Groups have unified single API for writing streaming features transparently     to both online and offline store.</p> </li> <li>parents <code>List[hsfs.feature_group.FeatureGroup] | None</code>: Optionally, Define the parents of this feature group as the     origin where the data is coming from.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it     defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> <li>transformation_functions <code>List[hsfs.transformation_function.TransformationFunction | hsfs.hopsworks_udf.HopsworksUdf] | None</code>: On-Demand Transformation functions attached to the feature group.     It can be a list of list of user defined functions defined using the hopsworks <code>@udf</code> decorator.     Defaults to <code>None</code>, no transformations.</li> <li>online_config <code>hsfs.online_config.OnlineConfig | Dict[str, Any] | None</code>: Optionally, define configuration which is used to configure online table.</li> <li>offline_backfill_every_hr <code>int | str | None</code>: Optional. If specified, the materialization job will be scheduled to run     periodically. The value can be either an integer representing the number of hours between each run     or a string representing a cron expression. Set the value to None to avoid scheduling the materialization     job. Defaults to None (i.e no automatic scheduling). Applies only on Feature Group creation.</li> <li>storage_connector <code>hsfs.StorageConnector | Dict[str, Any]</code>: the storage connector used to establish connectivity     with the data source.</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The feature group metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_or_create_feature_view","title":"get_or_create_feature_view","text":"<pre><code>FeatureStore.get_or_create_feature_view(\n    name,\n    query,\n    version,\n    description=\"\",\n    labels=None,\n    inference_helper_columns=None,\n    training_helper_columns=None,\n    transformation_functions=None,\n    logging_enabled=False,\n)\n</code></pre> <p>Get feature view metadata object or create a new one if it doesn't exist. This method doesn't update existing feature view metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfeature_view = fs.get_or_create_feature_view(\n    name='bitcoin_feature_view',\n    version=1,\n    transformation_functions=transformation_functions,\n    query=query\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to create.</li> <li>query <code>hsfs.constructor.query.Query</code>: Feature store <code>Query</code>.</li> <li>version <code>int</code>: Version of the feature view to create.</li> <li>description <code>str | None</code>: A string describing the contents of the feature view to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>labels <code>List[str] | None</code>: A list of feature names constituting the prediction label/feature of     the feature view. When replaying a <code>Query</code> during model inference,     the label features can be omitted from the feature vector retrieval.     Defaults to <code>[]</code>, no label.</li> <li>inference_helper_columns <code>List[str] | None</code>: A list of feature names that are not used in training the model itself but can be     used during batch or online inference for extra information. Inference helper column name(s) must be     part of the <code>Query</code> object. If inference helper column name(s) belong to feature group that is part     of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to be prepended to the original column name     when defining <code>inference_helper_columns</code> list. When replaying a <code>Query</code> during model inference,     the inference helper columns optionally can be omitted during batch (<code>get_batch_data</code>) and will be     omitted during online  inference (<code>get_feature_vector(s)</code>). To get inference helper column(s) during     online inference use <code>get_inference_helper(s)</code> method. Defaults to `[], no helper columns.</li> <li>training_helper_columns <code>List[str] | None</code>: A list of feature names that are not the part of the model schema itself but can be     used during training as a helper for extra information. Training helper column name(s) must be     part of the <code>Query</code> object. If training helper column name(s) belong to feature group that is part     of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to prepended to the original column name when     defining <code>training_helper_columns</code> list. When replaying a <code>Query</code> during model inference,     the training helper columns will be omitted during both batch and online inference.     Training helper columns can be optionally fetched with training data. For more details see     documentation for feature view's get training data methods.  Defaults to `[], no training helper     columns.</li> <li>transformation_functions <code>Dict[str, hsfs.transformation_function.TransformationFunction] | None</code>: Model Dependent Transformation functions attached to the feature view.     It can be a list of list of user defined functions defined using the hopsworks <code>@udf</code> decorator.     Defaults to <code>None</code>, no transformations.</li> <li>logging_enabled <code>bool | None</code>: If true, enable feature logging for the feature view.</li> </ul> <p>Returns:</p> <p><code>FeatureView</code>: The feature view metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_or_create_spine_group","title":"get_or_create_spine_group","text":"<pre><code>FeatureStore.get_or_create_spine_group(\n    name,\n    version=None,\n    description=\"\",\n    primary_key=None,\n    event_time=None,\n    features=None,\n    dataframe=None,\n)\n</code></pre> <p>Create a spine group metadata object.</p> <p>Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nspine_df = pd.Dataframe()\n\nspine_group = fs.get_or_create_spine_group(\n                    name=\"sales\",\n                    version=1,\n                    description=\"Physical shop sales features\",\n                    primary_key=['ss_store_sk'],\n                    event_time='sale_date',\n                    dataframe=spine_df\n                    )\n</code></pre> <p>Note that you can inspect the dataframe in the spine group, or replace the dataframe:</p> <pre><code>spine_group.dataframe.show()\n\nspine_group.dataframe = new_df\n</code></pre> <p>The spine can then be used to construct queries, with only one speciality:</p> <p>Note</p> <p>Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against.</p> <p>If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving.</p> <p>These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again.</p> <p>For example, to generate training data:</p> <pre><code>X_train, X_test, y_train, y_test = feature_view_spine.train_test_split(0.2, spine=training_data_entities)\n</code></pre> <p>Or to get batches of fresh data for batch scoring: <pre><code>feature_view_spine.get_batch_data(spine=scoring_entities_df).show()\n</code></pre></p> <p>Here you have the chance to pass a different set of entities to generate the training dataset.</p> <p>Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column.</p> <pre><code>feature_view.get_batch_data(spine=spine_group)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the spine group to create.</li> <li>version <code>int | None</code>: Version of the spine group to retrieve, defaults to <code>None</code> and     will create the spine group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the spine group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     spine group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the spine group won't have any primary key.</li> <li>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this spine group. If event_time is set     the spine group can be used for point-in-time joins. Defaults to <code>None</code>.</li> <li> <p>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the spine group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame resulting by executing the provided query     against the data source.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ dataframe__: DataFrame, RDD, Ndarray, list. Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features.</p> </li> </ul> <p>Returns</p> <p><code>SpineGroup</code>. The spine group metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_storage_connector","title":"get_storage_connector","text":"<pre><code>FeatureStore.get_storage_connector(name)\n</code></pre> <p>Get a previously created storage connector from the feature store.</p> <p>Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS.</p> <p>If you want to connect to the online feature store, see the <code>get_online_storage_connector</code> method to get the JDBC connector for the Online Feature Store.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nsc = fs.get_storage_connector(\"demo_fs_meb10000_Training_Datasets\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the storage connector to retrieve.</li> </ul> <p>Returns</p> <p><code>StorageConnector</code>. Storage connector object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_training_dataset","title":"get_training_dataset","text":"<pre><code>FeatureStore.get_training_dataset(name, version=None)\n</code></pre> <p>Get a training dataset entity from the feature store.</p> <p>Deprecated</p> <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version.</p> <p>It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects.</p> <p>Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the training dataset to get.</li> <li>version <code>int | None</code>: Version of the training dataset to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>TrainingDataset</code>: The training dataset metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve training dataset from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_training_datasets","title":"get_training_datasets","text":"<pre><code>FeatureStore.get_training_datasets(name)\n</code></pre> <p>Get a list of all versions of a training dataset entity from the feature store.</p> <p>Deprecated</p> <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead.</p> <p>Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the training dataset to get.</li> </ul> <p>Returns</p> <p><code>TrainingDataset</code>: List of training dataset metadata objects.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_transformation_function","title":"get_transformation_function","text":"<pre><code>FeatureStore.get_transformation_function(name, version=None)\n</code></pre> <p>Get  transformation function metadata object.</p> <p>Get transformation function by name. This will default to version 1</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nplus_one_fn = fs.get_transformation_function(name=\"plus_one\")\n</code></pre> <p>Get built-in transformation function min max scaler</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nmin_max_scaler_fn = fs.get_transformation_function(name=\"min_max_scaler\")\n</code></pre> <p>Get transformation function by name and version</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\", version=2)\n</code></pre> <p>You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function instance. Then the transformation functions are applied when you read training data, get batch data, or get feature vector(s).</p> <p>Attach transformation functions to the feature view</p> <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# get transformation function metadata object\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\", version=1)\n\n# attach transformation functions\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    query=query,\n    labels=[\"target_column\"],\n    transformation_functions=[min_max_scaler(\"feature1\")]\n)\n</code></pre> <p>Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for <code>min_max_scaler</code>; mean and standard deviation for <code>standard_scaler</code> etc.</p> <p>Attach built-in transformation functions to the feature view</p> <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# retrieve transformation functions\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\nstandard_scaler = fs.get_transformation_function(name=\"standard_scaler\")\nrobust_scaler = fs.get_transformation_function(name=\"robust_scaler\")\nlabel_encoder = fs.get_transformation_function(name=\"label_encoder\")\n\n# attach built-in transformation functions while creating feature view\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions = [\n        label_encoder(\"category_column\"),\n        robust_scaler(\"weight\"),\n        min_max_scaler(\"age\"),\n        standard_scaler(\"salary\")\n    ]\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: name of transformation function.</li> <li>version <code>int | None</code>: version of transformation function. Optional, if not provided all functions that match to provided     name will be retrieved.</li> </ul> <p>Returns:</p> <p><code>TransformationFunction</code>: The TransformationFunction metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_transformation_functions","title":"get_transformation_functions","text":"<pre><code>FeatureStore.get_transformation_functions()\n</code></pre> <p>Get  all transformation functions metadata objects.</p> <p>Get all transformation functions</p> <pre><code># get feature store instance\nfs = ...\n\n# get all transformation functions\nlist_transformation_fns = fs.get_transformation_functions()\n</code></pre> <p>Returns:</p> <p><code>List[TransformationFunction]</code>. List of transformation function instances.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#sql","title":"sql","text":"<pre><code>FeatureStore.sql(query, dataframe_type=\"default\", online=False, read_options=None)\n</code></pre> <p>Execute SQL command on the offline or online feature store database</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# construct the query and show head rows\nquery_res_head = fs.sql(\"SELECT * FROM `fg_1`\").head()\n</code></pre> <p>Arguments</p> <ul> <li>query <code>str</code>: The SQL query to execute.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> <li>online <code>bool | None</code>: Set to true to execute the query against the online feature store.     Defaults to False.</li> <li>read_options <code>dict | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     For python engine:     If running queries on the online feature store, users can provide an entry <code>{'external': True}</code>,     this instructs the library to use the <code>host</code> parameter in the <code>hopsworks.login()</code> to establish the connection to the online feature store.     If not set, or set to False, the online feature store storage connector is used which relies on     the private ip.     Defaults to <code>{}</code>.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>: DataFrame depending on the chosen type.</p>"},{"location":"generated/api/feature_transformation_statistics/","title":"Feature Transformation Statistics","text":"<p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#featuretransformationstatistics","title":"FeatureTransformationStatistics","text":"<pre><code>hsfs.transformation_statistics.FeatureTransformationStatistics(\n    feature_name,\n    count=None,\n    completeness=None,\n    num_non_null_values=None,\n    num_null_values=None,\n    approx_num_distinct_values=None,\n    min=None,\n    max=None,\n    sum=None,\n    mean=None,\n    stddev=None,\n    percentiles=None,\n    distinctness=None,\n    entropy=None,\n    uniqueness=None,\n    exact_num_distinct_values=None,\n    extended_statistics=None,\n    **kwargs\n)\n</code></pre> <p>Data class that contains all the statistics parameters that can be used for transformations inside a custom transformation function.</p>"},{"location":"generated/api/feature_transformation_statistics/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#approx_num_distinct_values","title":"approx_num_distinct_values","text":"<p>Approximate number of distinct values.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#completeness","title":"completeness","text":"<p>Fraction of non-null values in a column.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#correlations","title":"correlations","text":"<p>Correlations of feature values.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#count","title":"count","text":"<p>Number of values.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#distinctness","title":"distinctness","text":"<p>Fraction of distinct values of a feature over the number of all its values. Distinct values occur at least once.</p> <p>Example</p> <p>[a, a, b] contains two distinct values a and b, so distinctness is 2/3.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#entropy","title":"entropy","text":"<p>Entropy is a measure of the level of information contained in an event (feature value) when considering all possible events (all feature values). Entropy is estimated using observed value counts as the negative sum of (value_count/total_count) * log(value_count/total_count).</p> <p>Example</p> <p>[a, b, b, c, c] has three distinct values with counts [1, 2, 2].</p> <p>Entropy is then (-1/5*log(1/5)-2/5*log(2/5)-2/5*log(2/5)) = 1.055.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#exact_num_distinct_values","title":"exact_num_distinct_values","text":"<p>Exact number of distinct values.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#feature_name","title":"feature_name","text":"<p>Name of the feature.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#histogram","title":"histogram","text":"<p>Histogram of feature values.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#kll","title":"kll","text":"<p>KLL of feature values.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#max","title":"max","text":"<p>Maximum value.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#mean","title":"mean","text":"<p>Mean value.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#min","title":"min","text":"<p>Minimum value.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#num_non_null_values","title":"num_non_null_values","text":"<p>Number of non-null values.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#num_null_values","title":"num_null_values","text":"<p>Number of null values.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#percentiles","title":"percentiles","text":"<p>Percentiles.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#stddev","title":"stddev","text":"<p>Standard deviation of the feature values.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#sum","title":"sum","text":"<p>Sum of all feature values.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#unique_values","title":"unique_values","text":"<p>Number of Unique Values.</p> <p>[source]</p>"},{"location":"generated/api/feature_transformation_statistics/#uniqueness","title":"uniqueness","text":"<p>Fraction of unique values over the number of all values of a column. Unique values occur exactly once.</p> <p>Example</p> <p>[a, a, b] contains one unique value b, so uniqueness is 1/3.</p>"},{"location":"generated/api/feature_view_api/","title":"Feature View","text":"<p>[source]</p>"},{"location":"generated/api/feature_view_api/#featureview","title":"FeatureView","text":"<pre><code>hsfs.feature_view.FeatureView(\n    name,\n    query,\n    featurestore_id,\n    id=None,\n    version=None,\n    description=\"\",\n    labels=None,\n    inference_helper_columns=None,\n    training_helper_columns=None,\n    transformation_functions=None,\n    featurestore_name=None,\n    serving_keys=None,\n    logging_enabled=False,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_view_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/feature_view_api/#create_feature_view","title":"create_feature_view","text":"<pre><code>FeatureStore.create_feature_view(\n    name,\n    query,\n    version=None,\n    description=\"\",\n    labels=None,\n    inference_helper_columns=None,\n    training_helper_columns=None,\n    transformation_functions=None,\n    logging_enabled=False,\n)\n</code></pre> <p>Create a feature view metadata object and saved it to hopsworks.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the feature group instances\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\n# construct the query\nquery = fg1.select_all().join(fg2.select_all())\n\n# define the transformation function as a Hopsworks's UDF\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n# construct list of \"transformation functions\" on features\ntransformation_functions = [plus_one(\"feature1\"), plus_one(\"feature1\"))]\n\nfeature_view = fs.create_feature_view(\n    name='air_quality_fv',\n    version=1,\n    transformation_functions=transformation_functions,\n    query=query\n)\n</code></pre> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# define list of transformation functions\nmapping_transformers = ...\n\n# create feature view\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    version=1,\n    transformation_functions=mapping_transformers,\n    query=query\n)\n</code></pre> <p>Warning</p> <p><code>as_of</code> argument in the <code>Query</code> will be ignored because feature view does not support time travel query.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to create.</li> <li>query <code>hsfs.constructor.query.Query</code>: Feature store <code>Query</code>.</li> <li>version <code>int | None</code>: Version of the feature view to create, defaults to <code>None</code> and     will create the feature view with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the feature view to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>labels <code>List[str] | None</code>: A list of feature names constituting the prediction label/feature of     the feature view. When replaying a <code>Query</code> during model inference,     the label features can be omitted from the feature vector retrieval.     Defaults to <code>[]</code>, no label.</li> <li>inference_helper_columns <code>List[str] | None</code>: A list of feature names that are not used in training the model itself but can be     used during batch or online inference for extra information. Inference helper column name(s) must be     part of the <code>Query</code> object. If inference helper column name(s) belong to feature group that is part     of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to be prepended to the original column name     when defining <code>inference_helper_columns</code> list. When replaying a <code>Query</code> during model inference,     the inference helper columns optionally can be omitted during batch (<code>get_batch_data</code>) and will be     omitted during online  inference (<code>get_feature_vector(s)</code>). To get inference helper column(s) during     online inference use <code>get_inference_helper(s)</code> method. Defaults to `[], no helper columns.</li> <li>training_helper_columns <code>List[str] | None</code>: A list of feature names that are not the part of the model schema itself but can be     used during training as a helper for extra information. Training helper column name(s) must be     part of the <code>Query</code> object. If training helper column name(s) belong to feature group that is part     of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to prepended to the original column name when     defining <code>training_helper_columns</code> list. When replaying a <code>Query</code> during model inference,     the training helper columns will be omitted during both batch and online inference.     Training helper columns can be optionally fetched with training data. For more details see     documentation for feature view's get training data methods.  Defaults to `[], no training helper     columns.</li> <li>transformation_functions <code>List[hsfs.transformation_function.TransformationFunction | hsfs.hopsworks_udf.HopsworksUdf] | None</code>: Model Dependent Transformation functions attached to the feature view.     It can be a list of list of user defined functions defined using the hopsworks <code>@udf</code> decorator.     Defaults to <code>None</code>, no transformations.</li> </ul> <p>Returns:</p> <p><code>FeatureView</code>: The feature view metadata object.</p>"},{"location":"generated/api/feature_view_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_feature_view","title":"get_feature_view","text":"<pre><code>FeatureStore.get_feature_view(name, version=None)\n</code></pre> <p>Get a feature view entity from the feature store.</p> <p>Getting a feature view from the Feature Store means getting its metadata.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(\n    name='feature_view_name',\n    version=1\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to get.</li> <li>version <code>int</code>: Version of the feature view to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>FeatureView</code>: The feature view metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature view from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_feature_views","title":"get_feature_views","text":"<pre><code>FeatureStore.get_feature_views(name)\n</code></pre> <p>Get a list of all versions of a feature view entity from the feature store.</p> <p>Getting a feature view from the Feature Store means getting its metadata.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get a list of all versions of a feature view\nfeature_view = fs.get_feature_views(\n    name='feature_view_name'\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to get.</li> </ul> <p>Returns</p> <p><code>FeatureView</code>: List of feature view metadata objects.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature view from the feature store.</li> </ul>"},{"location":"generated/api/feature_view_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_view_api/#description","title":"description","text":"<p>Description of the feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#feature_logging","title":"feature_logging","text":"<p>[source]</p>"},{"location":"generated/api/feature_view_api/#feature_store_name","title":"feature_store_name","text":"<p>Name of the feature store in which the feature group is located.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#features","title":"features","text":"<p>Schema of untransformed features in the Feature view. (alias)</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#featurestore_id","title":"featurestore_id","text":"<p>Feature store id.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#id","title":"id","text":"<p>Feature view id.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#inference_helper_columns","title":"inference_helper_columns","text":"<p>The helper column sof the feature view.</p> <p>Can be a composite of multiple features.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#labels","title":"labels","text":"<p>The labels/prediction feature of the feature view.</p> <p>Can be a composite of multiple features.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#logging_enabled","title":"logging_enabled","text":"<p>[source]</p>"},{"location":"generated/api/feature_view_api/#model_dependent_transformations","title":"model_dependent_transformations","text":"<p>Get Model-Dependent transformations as a dictionary mapping transformed feature names to transformation function</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#name","title":"name","text":"<p>Name of the feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#on_demand_transformations","title":"on_demand_transformations","text":"<p>Get On-Demand transformations as a dictionary mapping on-demand feature names to transformation function</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#primary_keys","title":"primary_keys","text":"<p>Set of primary key names that is required as keys in input dict object for <code>get_feature_vector(s)</code> method. When there are duplicated primary key names and prefix is not defined in the query, prefix is generated and prepended to the primary key name in this format \"fgId_{feature_group_id}_{join_index}\" where <code>join_index</code> is the order of the join.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#query","title":"query","text":"<p>Query of the feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#schema","title":"schema","text":"<p>Schema of untransformed features in the Feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#serving_keys","title":"serving_keys","text":"<p>All primary keys of the feature groups included in the query.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#training_helper_columns","title":"training_helper_columns","text":"<p>The helper column sof the feature view.</p> <p>Can be a composite of multiple features.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#transformation_functions","title":"transformation_functions","text":"<p>Get transformation functions.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#version","title":"version","text":"<p>Version number of the feature view.</p>"},{"location":"generated/api/feature_view_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/feature_view_api/#add_tag","title":"add_tag","text":"<pre><code>FeatureView.add_tag(name, value)\n</code></pre> <p>Attach a tag to a feature view.</p> <p>A tag consists of a name and value pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# attach a tag to a feature view\nfeature_view.add_tag(name=\"tag_schema\", value={\"key\", \"value\"})\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value <code>Any</code>: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to add the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#add_training_dataset_tag","title":"add_training_dataset_tag","text":"<pre><code>FeatureView.add_training_dataset_tag(training_dataset_version, name, value)\n</code></pre> <p>Attach a tag to a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# attach a tag to a training dataset\nfeature_view.add_training_dataset_tag(\n    training_dataset_version=1,\n    name=\"tag_schema\",\n    value={\"key\", \"value\"}\n)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value <code>Dict[str, Any] | hopsworks_common.tag.Tag</code>: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to add the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#clean","title":"clean","text":"<pre><code>FeatureView.clean(feature_store_id, feature_view_name, feature_view_version)\n</code></pre> <p>Delete the feature view and all associated metadata and training data. This can delete corrupted feature view which cannot be retrieved due to a corrupted query for example.</p> <p>Example</p> <pre><code># delete a feature view and all associated metadata\nfrom hsfs.feature_view import FeatureView\n\nFeatureView.clean(\n    feature_store_id=1,\n    feature_view_name='feature_view_name',\n    feature_view_version=1\n)\n</code></pre> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS.</p> <p>Arguments</p> <ul> <li>feature_store_id <code>int</code>: int. Id of feature store.</li> <li>feature_view_name <code>str</code>: str. Name of feature view.</li> <li>feature_view_version <code>str</code>: str. Version of feature view.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#compute_on_demand_features","title":"compute_on_demand_features","text":"<pre><code>FeatureView.compute_on_demand_features(feature_vector, request_parameters=None, external=None)\n</code></pre> <p>Function computes on-demand features present in the feature view.</p> <p>Arguments</p> <ul> <li>feature_vector <code>List[Any] | List[List[Any]] | pandas.DataFrame | polars.dataframe.frame.DataFrame</code>: <code>Union[List[Any], List[List[Any]], pd.DataFrame, pl.DataFrame]</code>. The feature vector to be transformed.</li> <li>request_parameters <code>List[Dict[str, Any]] | Dict[str, Any] | None</code>: Request parameters required by on-demand transformation functions to compute on-demand features present in the feature view.</li> </ul> <p>Returns</p> <p><code>Union[List[Any], List[List[Any]], pd.DataFrame, pl.DataFrame]</code>: The feature vector that contains all on-demand features in the feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#create_feature_monitoring","title":"create_feature_monitoring","text":"<pre><code>FeatureView.create_feature_monitoring(\n    name,\n    feature_name,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature view\nfg = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # compare to a given value\n    specific_value=0.5,\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str</code>: Name of the feature to monitor.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#create_statistics_monitoring","title":"create_statistics_monitoring","text":"<pre><code>FeatureView.create_statistics_monitoring(\n    name,\n    feature_name=None,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature view\nfv = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# enable statistics monitoring\nmy_config = fv._create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature view.</li> <li>feature_name <code>str | None</code>: Name of the feature to monitor. If not specified, statistics     will be computed for all features.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#create_train_test_split","title":"create_train_test_split","text":"<pre><code>FeatureView.create_train_test_split(\n    test_size=None,\n    train_start=\"\",\n    train_end=\"\",\n    test_start=\"\",\n    test_end=\"\",\n    storage_connector=None,\n    location=\"\",\n    description=\"\",\n    extra_filter=None,\n    data_format=\"parquet\",\n    coalesce=False,\n    seed=None,\n    statistics_config=None,\n    write_options=None,\n    spine=None,\n    primary_key=False,\n    event_time=False,\n    training_helper_columns=False,\n    **kwargs\n)\n</code></pre> <p>Create the metadata for a training dataset and save the corresponding training data into <code>location</code>. The training data is split into train and test set at random or according to time ranges. The training data can be retrieved by calling <code>feature_view.get_train_test_split</code>.</p> <p>Create random splits</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# create a train-test split dataset\nversion, job = feature_view.create_train_test_split(\n    test_size=0.2,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> <p>Create time series splits by specifying date as string</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\ntrain_start = \"2022-01-01 00:00:00\"\ntrain_end = \"2022-06-06 23:59:59\"\ntest_start = \"2022-06-07 00:00:00\"\ntest_end = \"2022-12-25 23:59:59\"\n\n# create a train-test split dataset\nversion, job = feature_view.create_train_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> <p>Create time series splits by specifying date as datetime object</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nfrom datetime import datetime\ndate_format = \"%Y-%m-%d %H:%M:%S\"\n\ntrain_start = datetime.strptime(\"2022-01-01 00:00:00\", date_format)\ntrain_end = datetime.strptime(\"2022-06-06 23:59:59\", date_format)\ntest_start = datetime.strptime(\"2022-06-07 00:00:00\", date_format)\ntest_end = datetime.strptime(\"2022-12-25 23:59:59\" , date_format)\n\n# create a train-test split dataset\nversion, job = feature_view.create_train_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> <p>Write training dataset to external storage</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get storage connector instance\nexternal_storage_connector = fs.get_storage_connector(\"storage_connector_name\")\n\n# create a train-test split dataset\nversion, job = feature_view.create_train_test_split(\n    train_start=...,\n    train_end=...,\n    test_start=...,\n    test_end=...,\n    storage_connector = external_storage_connector,\n    description=...,\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format=...\n)\n</code></pre> <p>Data Formats</p> <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> <p>Warning, the following code will fail because category column contains sparse values and training dataset may not have all values available in test split.</p> <pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'category_col':['category_a','category_b','category_c','category_d'],\n    'numeric_col': [40,10,60,40]\n})\n\nfeature_group = fs.get_or_create_feature_group(\n    name='feature_group_name',\n    version=1,\n    primary_key=['category_col']\n)\n\nfeature_group.insert(df)\n\nlabel_encoder = fs.get_transformation_function(name='label_encoder')\n\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    query=feature_group.select_all(),\n    transformation_functions={'category_col':label_encoder}\n)\n\nfeature_view.create_train_test_split(\n    test_size=0.5\n)\n# Output: KeyError: 'category_c'\n</code></pre> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>test_size <code>float | None</code>: size of test set.</li> <li>train_start <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the train split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>train_end <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the train split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_start <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the test split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_end <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the test split query, exclusive. Strings should     be  formatted in one of the following ormats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>storage_connector <code>hsfs.StorageConnector | None</code>: Storage connector defining the sink location for the     training dataset, defaults to <code>None</code>, and materializes training dataset     on HopsFS.</li> <li>location <code>str | None</code>: Path to complement the sink storage connector with, e.g if the     storage connector points to an S3 bucket, this path can be used to     define a sub-directory inside the bucket to place the training dataset.     Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the     storage connector.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>extra_filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Additional filters to be attached to the training dataset.     The filters will be also applied in <code>get_batch_data</code>.</li> <li>data_format <code>str | None</code>: The data format used to save the training dataset,     defaults to <code>\"parquet\"</code>-format.</li> <li>coalesce <code>bool | None</code>: If true the training dataset data will be coalesced into     a single partition before writing. The resulting training dataset     will be a single file per split. Default False.</li> <li>seed <code>int | None</code>: Optionally, define a seed to create the random splits with, in order     to guarantee reproducability, defaults to <code>None</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>write_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>use_spark</code> and value <code>True</code> to materialize training dataset   with Spark instead of Hopsworks Feature Query Service.</li> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_key <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view     then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper     columns.</li> </ul> <p>Returns</p> <p>(td_version, <code>Job</code>): Tuple of training dataset version and job.     When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to create the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#create_train_validation_test_split","title":"create_train_validation_test_split","text":"<pre><code>FeatureView.create_train_validation_test_split(\n    validation_size=None,\n    test_size=None,\n    train_start=\"\",\n    train_end=\"\",\n    validation_start=\"\",\n    validation_end=\"\",\n    test_start=\"\",\n    test_end=\"\",\n    storage_connector=None,\n    location=\"\",\n    description=\"\",\n    extra_filter=None,\n    data_format=\"parquet\",\n    coalesce=False,\n    seed=None,\n    statistics_config=None,\n    write_options=None,\n    spine=None,\n    primary_key=False,\n    event_time=False,\n    training_helper_columns=False,\n    **kwargs\n)\n</code></pre> <p>Create the metadata for a training dataset and save the corresponding training data into <code>location</code>. The training data is split into train, validation, and test set at random or according to time range. The training data can be retrieved by calling <code>feature_view.get_train_validation_test_split</code>.</p> <p>Create random splits</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# create a train-validation-test split dataset\nversion, job = feature_view.create_train_validation_test_split(\n    validation_size=0.3,\n    test_size=0.2,\n    description='Description of a dataset',\n    data_format='csv'\n)\n</code></pre> <p>Create time series splits by specifying date as string</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\ntrain_start = \"2022-01-01 00:00:00\"\ntrain_end = \"2022-06-01 23:59:59\"\nvalidation_start = \"2022-06-02 00:00:00\"\nvalidation_end = \"2022-07-01 23:59:59\"\ntest_start = \"2022-07-02 00:00:00\"\ntest_end = \"2022-08-01 23:59:59\"\n\n# create a train-validation-test split dataset\nversion, job = feature_view.create_train_validation_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    validation_start=validation_start,\n    validation_end=validation_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> <p>Create time series splits by specifying date as datetime object</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nfrom datetime import datetime\ndate_format = \"%Y-%m-%d %H:%M:%S\"\n\ntrain_start = datetime.strptime(\"2022-01-01 00:00:00\", date_format)\ntrain_end = datetime.strptime(\"2022-06-06 23:59:59\", date_format)\nvalidation_start = datetime.strptime(\"2022-06-02 00:00:00\", date_format)\nvalidation_end = datetime.strptime(\"2022-07-01 23:59:59\", date_format)\ntest_start = datetime.strptime(\"2022-06-07 00:00:00\", date_format)\ntest_end = datetime.strptime(\"2022-12-25 23:59:59\", date_format)\n\n# create a train-validation-test split dataset\nversion, job = feature_view.create_train_validation_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    validation_start=validation_start,\n    validation_end=validation_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> <p>Write training dataset to external storage</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get storage connector instance\nexternal_storage_connector = fs.get_storage_connector(\"storage_connector_name\")\n\n# create a train-validation-test split dataset\nversion, job = feature_view.create_train_validation_test_split(\n    train_start=...,\n    train_end=...,\n    validation_start=...,\n    validation_end=...,\n    test_start=...,\n    test_end=...,\n    description=...,\n    storage_connector = external_storage_connector,\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format=...\n)\n</code></pre> <p>Data Formats</p> <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>validation_size <code>float | None</code>: size of validation set.</li> <li>test_size <code>float | None</code>: size of test set.</li> <li>train_start <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the train split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>train_end <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the train split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>validation_start <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the validation split query, inclusive. Strings     should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>validation_end <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the validation split query, exclusive. Strings     should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_start <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the test split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_end <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the test split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>storage_connector <code>hsfs.StorageConnector | None</code>: Storage connector defining the sink location for the     training dataset, defaults to <code>None</code>, and materializes training dataset     on HopsFS.</li> <li>location <code>str | None</code>: Path to complement the sink storage connector with, e.g if the     storage connector points to an S3 bucket, this path can be used to     define a sub-directory inside the bucket to place the training dataset.     Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the     storage connector.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>extra_filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Additional filters to be attached to the training dataset.     The filters will be also applied in <code>get_batch_data</code>.</li> <li>data_format <code>str | None</code>: The data format used to save the training dataset,     defaults to <code>\"parquet\"</code>-format.</li> <li>coalesce <code>bool | None</code>: If true the training dataset data will be coalesced into     a single partition before writing. The resulting training dataset     will be a single file per split. Default False.</li> <li>seed <code>int | None</code>: Optionally, define a seed to create the random splits with, in order     to guarantee reproducability, defaults to <code>None</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>write_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>use_spark</code> and value <code>True</code> to materialize training dataset   with Spark instead of Hopsworks Feature Query Service.</li> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_key <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view     then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper     columns.</li> </ul> <p>Returns</p> <p>(td_version, <code>Job</code>): Tuple of training dataset version and job.     When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to create the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#create_training_data","title":"create_training_data","text":"<pre><code>FeatureView.create_training_data(\n    start_time=\"\",\n    end_time=\"\",\n    storage_connector=None,\n    location=\"\",\n    description=\"\",\n    extra_filter=None,\n    data_format=\"parquet\",\n    coalesce=False,\n    seed=None,\n    statistics_config=None,\n    write_options=None,\n    spine=None,\n    primary_key=False,\n    event_time=False,\n    training_helper_columns=False,\n    **kwargs\n)\n</code></pre> <p>Create the metadata for a training dataset and save the corresponding training data into <code>location</code>. The training data can be retrieved by calling <code>feature_view.get_training_data</code>.</p> <p>Create training dataset</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# create a training dataset\nversion, job = feature_view.create_training_data(\n    description='Description of a dataset',\n    data_format='csv',\n    # async creation in order not to wait till finish of the job\n    write_options={\"wait_for_job\": False}\n)\n</code></pre> <p>Create training data specifying date range  with dates as strings</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nstart_time = \"2022-01-01 00:00:00\"\nend_time = \"2022-06-06 23:59:59\"\n\n# create a training dataset\nversion, job = feature_view.create_training_data(\n    start_time=start_time,\n    end_time=end_time,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n\n# When we want to read the training data, we need to supply the training data version returned by the create_training_data method:\nX_train, X_test, y_train, y_test = feature_view.get_training_data(version)\n</code></pre> <p>Create training data specifying date range  with dates as datetime objects</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nfrom datetime import datetime\ndate_format = \"%Y-%m-%d %H:%M:%S\"\n\nstart_time = datetime.strptime(\"2022-01-01 00:00:00\", date_format)\nend_time = datetime.strptime(\"2022-06-06 23:59:59\", date_format)\n\n# create a training dataset\nversion, job = feature_view.create_training_data(\n    start_time=start_time,\n    end_time=end_time,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> <p>Write training dataset to external storage</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get storage connector instance\nexternal_storage_connector = fs.get_storage_connector(\"storage_connector_name\")\n\n# create a train-test split dataset\nversion, job = feature_view.create_training_data(\n    start_time=...,\n    end_time=...,\n    storage_connector = external_storage_connector,\n    description=...,\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format=...\n)\n</code></pre> <p>Data Formats</p> <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>start_time <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the training dataset query, inclusive. Optional. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>end_time <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the training dataset query, exclusive. Optional. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>storage_connector <code>hsfs.StorageConnector | None</code>: Storage connector defining the sink location for the     training dataset, defaults to <code>None</code>, and materializes training dataset     on HopsFS.</li> <li>location <code>str | None</code>: Path to complement the sink storage connector with, e.g if the     storage connector points to an S3 bucket, this path can be used to     define a sub-directory inside the bucket to place the training dataset.     Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the     storage connector.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>extra_filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Additional filters to be attached to the training dataset.     The filters will be also applied in <code>get_batch_data</code>.</li> <li>data_format <code>str | None</code>: The data format used to save the training dataset,     defaults to <code>\"parquet\"</code>-format.</li> <li>coalesce <code>bool | None</code>: If true the training dataset data will be coalesced into     a single partition before writing. The resulting training dataset     will be a single file per split. Default False.</li> <li>seed <code>int | None</code>: Optionally, define a seed to create the random splits with, in order     to guarantee reproducability, defaults to <code>None</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>write_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>use_spark</code> and value <code>True</code> to materialize training dataset   with Spark instead of Hopsworks Feature Query Service.</li> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_key <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not. Training helper columns are a     list of feature names in the feature view, defined during its creation, that are not the part of the     model schema itself but can be used during training as a helper for extra information.     If training helper columns were not defined in the feature view then<code>training_helper_columns=True</code>     will not have any effect. Defaults to <code>False</code>, no training helper columns.</li> </ul> <p>Returns</p> <p>(td_version, <code>Job</code>): Tuple of training dataset version and job.     When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to create the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#delete","title":"delete","text":"<pre><code>FeatureView.delete()\n</code></pre> <p>Delete current feature view, all associated metadata and training data.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete a feature view\nfeature_view.delete()\n</code></pre> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#delete_all_training_datasets","title":"delete_all_training_datasets","text":"<pre><code>FeatureView.delete_all_training_datasets()\n</code></pre> <p>Delete all training datasets. This will delete both metadata and training data.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete all training datasets\nfeature_view.delete_all_training_datasets()\n</code></pre> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the training datasets.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#delete_log","title":"delete_log","text":"<pre><code>FeatureView.delete_log(transformed=None)\n</code></pre> <p>Delete the logged feature data for the current feature view.</p> <p>Arguments</p> <ul> <li>transformed <code>bool | None</code>: Whether to delete transformed logs. Defaults to None. Delete both transformed and untransformed logs.</li> </ul> <p>Example</p> <pre><code># delete log\nfeature_view.delete_log()\n</code></pre> <p># Raises  <code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the log.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#delete_tag","title":"delete_tag","text":"<pre><code>FeatureView.delete_tag(name)\n</code></pre> <p>Delete a tag attached to a feature view.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete a tag\nfeature_view.delete_tag('name_of_tag')\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#delete_training_dataset","title":"delete_training_dataset","text":"<pre><code>FeatureView.delete_training_dataset(training_dataset_version)\n</code></pre> <p>Delete a training dataset. This will delete both metadata and training data.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete a training dataset\nfeature_view.delete_training_dataset(\n    training_dataset_version=1\n)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: Version of the training dataset to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#delete_training_dataset_tag","title":"delete_training_dataset_tag","text":"<pre><code>FeatureView.delete_training_dataset_tag(training_dataset_version, name)\n</code></pre> <p>Delete a tag attached to a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete training dataset tag\nfeature_view.delete_training_dataset_tag(\n    training_dataset_version=1,\n    name='name_of_dataset'\n)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#enable_logging","title":"enable_logging","text":"<pre><code>FeatureView.enable_logging()\n</code></pre> <p>Enable feature logging for the current feature view.</p> <p>This method activates logging of features.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# enable logging\nfeature_view.enable_logging()\n</code></pre> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to enable feature logging.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#find_neighbors","title":"find_neighbors","text":"<pre><code>FeatureView.find_neighbors(\n    embedding, feature=None, k=10, filter=None, external=None, return_type=\"list\"\n)\n</code></pre> <p>Finds the nearest neighbors for a given embedding in the vector database.</p> <p>If <code>filter</code> is specified, or if embedding feature is stored in default project index, the number of results returned may be less than k. Try using a large value of k and extract the top k items from the results if needed.</p> <p>Duplicate column error in Polars</p> <p>If the feature view has duplicate column names, attempting to create a polars DataFrame will raise an error. To avoid this, set <code>return_type</code> to <code>\"list\"</code> or <code>\"pandas\"</code>.</p> <p>Arguments</p> <ul> <li>embedding <code>List[int | float]</code>: The target embedding for which neighbors are to be found.</li> <li>feature <code>hsfs.feature.Feature | None</code>: The feature used to compute similarity score. Required only if there are multiple embeddings (optional).</li> <li>k <code>int | None</code>: The number of nearest neighbors to retrieve (default is 10).</li> <li>filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: A filter expression to restrict the search space (optional).</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hopsworks.login()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> <li>return_type <code>Literal['list', 'polars', 'pandas']</code>: <code>\"list\"</code>, <code>\"pandas\"</code> or <code>\"polars\"</code>. Defaults to <code>\"list\"</code>.</li> </ul> <p>Returns</p> <p><code>list</code>, <code>pd.DataFrame</code> or <code>polars.DataFrame</code> if <code>return type</code> is set to <code>\"list\"</code>, <code>\"pandas\"</code> or <code>\"polars\"</code> respectively. Defaults to <code>list</code>.</p> <p>Example</p> <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=3)\nfg = fs.create_feature_group(\n            name='air_quality',\n            embedding_index=embedding_index,\n            version=1,\n            primary_key=['id1'],\n            online_enabled=True,\n        )\nfg.insert(data)\nfv = fs.create_feature_view(\"air_quality\", fg.select_all())\nfv.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n)\n\n# apply filter\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n    feature=fg.user_vector,  # optional\n    filter=(fg.id1 &gt; 10) &amp; (fg.id1 &lt; 30)\n)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#from_response_json","title":"from_response_json","text":"<pre><code>FeatureView.from_response_json(json_dict)\n</code></pre> <p>Function that constructs the class object from its json serialization.</p> <p>Arguments</p> <ul> <li>json_dict <code>Dict[str, Any]</code>: <code>Dict[str, Any]</code>. Json serialized dictionary for the class.</li> </ul> <p>Returns</p> <p><code>TransformationFunction</code>: Json deserialized class object.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_batch_data","title":"get_batch_data","text":"<pre><code>FeatureView.get_batch_data(\n    start_time=None,\n    end_time=None,\n    read_options=None,\n    spine=None,\n    primary_key=False,\n    event_time=False,\n    inference_helper_columns=False,\n    dataframe_type=\"default\",\n    transformed=True,\n    **kwargs\n)\n</code></pre> <p>Get a batch of data from an event time interval from the offline feature store.</p> <p>Batch data for the last 24 hours</p> <pre><code>    # get feature store instance\n    fs = ...\n\n    # get feature view instance\n    feature_view = fs.get_feature_view(...)\n\n    # set up dates\n    import datetime\n    start_date = (datetime.datetime.now() - datetime.timedelta(hours=24))\n    end_date = (datetime.datetime.now())\n\n    # get a batch of data\n    df = feature_view.get_batch_data(\n        start_time=start_date,\n        end_time=end_date\n    )\n</code></pre> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>start_time <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the batch query, inclusive. Optional. Strings should be     formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>end_time <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the batch query, exclusive. Optional. Strings should be     formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>read_options <code>Dict[str, Any] | None</code>: User provided read options for python engine, defaults to <code>{}</code>:<ul> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_key <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>inference_helper_columns <code>bool</code>: whether to include inference helper columns or not.     Inference helper columns are a list of feature names in the feature view, defined during its creation,     that may not be used in training the model itself but can be used during batch or online inference     for extra information. If inference helper columns were not defined in the feature view     <code>inference_helper_columns=True</code> will not any effect. Defaults to <code>False</code>, no helper columns.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> <li>transformed <code>bool | None</code>: Setting to <code>False</code> returns the untransformed feature vectors.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>: The spark dataframe containing the feature data. <code>pyspark.DataFrame</code>. A Spark DataFrame. <code>pandas.DataFrame</code>. A Pandas DataFrame. <code>polars.DataFrame</code>. A Polars DataFrame. <code>numpy.ndarray</code>. A two-dimensional Numpy array. <code>list</code>. A two-dimensional Python list.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_batch_query","title":"get_batch_query","text":"<pre><code>FeatureView.get_batch_query(start_time=None, end_time=None)\n</code></pre> <p>Get a query string of the batch query.</p> <p>Batch query for the last 24 hours</p> <pre><code>    # get feature store instance\n    fs = ...\n\n    # get feature view instance\n    feature_view = fs.get_feature_view(...)\n\n    # set up dates\n    import datetime\n    start_date = (datetime.datetime.now() - datetime.timedelta(hours=24))\n    end_date = (datetime.datetime.now())\n\n    # get a query string of batch query\n    query_str = feature_view.get_batch_query(\n        start_time=start_date,\n        end_time=end_date\n    )\n    # print query string\n    print(query_str)\n</code></pre> <p>Arguments</p> <ul> <li>start_time <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the batch query, inclusive. Optional. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>,     <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>end_time <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the batch query, exclusive. Optional. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>,     <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> </ul> <p>Returns</p> <p><code>str</code>: batch query</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_feature_monitoring_configs","title":"get_feature_monitoring_configs","text":"<pre><code>FeatureView.get_feature_monitoring_configs(name=None, feature_name=None, config_id=None)\n</code></pre> <p>Fetch feature monitoring configs attached to the feature view. If no arguments is provided the method will return all feature monitoring configs attached to the feature view, meaning all feature monitoring configs that are attach to a feature in the feature view. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> <p>Example</p> <pre><code># fetch your feature view\nfv = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# fetch all feature monitoring configs attached to the feature view\nfm_configs = fv.get_feature_monitoring_configs()\n# fetch a single feature monitoring config by name\nfm_config = fv.get_feature_monitoring_configs(name=\"my_config\")\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fv.get_feature_monitoring_configs(feature_name=\"my_feature\")\n# fetch a single feature monitoring config with a particular id\nfm_config = fv.get_feature_monitoring_configs(config_id=1)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: If provided fetch only the feature monitoring config with the given name.     Defaults to None.</li> <li>feature_name <code>str | None</code>: If provided, fetch only configs attached to a particular feature.     Defaults to None.</li> <li>config_id <code>int | None</code>: If provided, fetch only the feature monitoring config with the given id.     Defaults to None.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both name and feature_name are provided. - TypeError: if name or feature_name are not string or None.</p> <p>Return</p> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None]     A list of feature monitoring configs. If name provided,     returns either a single config or None if not found.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_feature_monitoring_history","title":"get_feature_monitoring_history","text":"<pre><code>FeatureView.get_feature_monitoring_history(\n    config_name=None, config_id=None, start_time=None, end_time=None, with_statistics=True\n)\n</code></pre> <p>Fetch feature monitoring history for a given feature monitoring config.</p> <p>Example</p> <pre><code># fetch your feature view\nfv = fs.get_feature_view(name=\"my_feature_group\", version=1)\n# fetch feature monitoring history for a given feature monitoring config\nfm_history = fv.get_feature_monitoring_history(\n    config_name=\"my_config\",\n    start_time=\"2020-01-01\",\n)\n# or use the config id\nfm_history = fv.get_feature_monitoring_history(\n    config_id=1,\n    start_time=datetime.now() - timedelta(weeks=2),\n    end_time=datetime.now() - timedelta(weeks=1),\n    with_statistics=False,\n)\n</code></pre> <p>Arguments</p> <ul> <li>config_name <code>str | None</code>: The name of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>config_id <code>int | None</code>: The id of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>start_date: The start date of the feature monitoring history to fetch.     Defaults to None.</li> <li>end_date: The end date of the feature monitoring history to fetch.     Defaults to None.</li> <li>with_statistics <code>bool | None</code>: Whether to include statistics in the feature monitoring history.     Defaults to True. If False, only metadata about the monitoring will be fetched.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both config_name and config_id are provided. - TypeError: if config_name or config_id are not respectively string, int or None.</p> <p>Return</p> <p>List[<code>FeatureMonitoringResult</code>]     A list of feature monitoring results containing the monitoring metadata     as well as the computed statistics for the detection and reference window     if requested.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_feature_vector","title":"get_feature_vector","text":"<pre><code>FeatureView.get_feature_vector(\n    entry,\n    passed_features=None,\n    external=None,\n    return_type=\"list\",\n    allow_missing=False,\n    force_rest_client=False,\n    force_sql_client=False,\n    transform=True,\n    request_parameters=None,\n)\n</code></pre> <p>Returns assembled feature vector from online feature store.     Call <code>feature_view.init_serving</code> before this method if the following configurations are needed.       1. The training dataset version of the transformation statistics       2. Additional configurations of online serving engine</p> <p>Missing primary key entries</p> <p>If the provided primary key <code>entry</code> can't be found in one or more of the feature groups used by this feature view the call to this method will raise an exception. Alternatively, setting <code>allow_missing</code> to <code>True</code> returns a feature vector with missing values.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get assembled serving vector as a python list\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2}\n)\n\n# get assembled serving vector as a pandas dataframe\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    return_type = \"pandas\"\n)\n\n# get assembled serving vector as a numpy array\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    return_type = \"numpy\"\n)\n</code></pre> <p>Get feature vector with user-supplied features</p> <pre><code># get feature store instance\nfs = ...\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# the application provides a feature value 'app_attr'\napp_attr = ...\n\n# get a feature vector\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    passed_features = { \"app_feature\" : app_attr }\n)\n</code></pre> <p>Arguments</p> <ul> <li>entry <code>Dict[str, Any]</code>: dictionary of feature group primary key and values provided by serving application.     Set of required primary keys is <code>feature_view.primary_keys</code>     If the required primary keys is not provided, it will look for name     of the primary key in feature group in the entry.</li> <li>passed_features <code>Dict[str, Any] | None</code>: dictionary of feature values provided by the application at runtime.     They can replace features values fetched from the feature store as well as     providing feature values which are not available in the feature store.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hopsworks.login()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> <li>return_type <code>Literal['list', 'polars', 'numpy', 'pandas']</code>: <code>\"list\"</code>, <code>\"pandas\"</code>, <code>\"polars\"</code> or <code>\"numpy\"</code>. Defaults to <code>\"list\"</code>.</li> <li>force_rest_client <code>bool</code>: boolean, defaults to False. If set to True, reads from online feature store     using the REST client if initialised.</li> <li>force_sql_client <code>bool</code>: boolean, defaults to False. If set to True, reads from online feature store     using the SQL client if initialised.</li> <li>allow_missing <code>bool</code>: Setting to <code>True</code> returns feature vectors with missing values.</li> <li>transformed: Setting to <code>False</code> returns the untransformed feature vectors.</li> <li>request_parameters <code>Dict[str, Any] | None</code>: Request parameters required by on-demand transformation functions to compute on-demand features present in the feature view.</li> </ul> <p>Returns</p> <p><code>list</code>, <code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>np.ndarray</code> if <code>return type</code> is set to <code>\"list\"</code>, <code>\"pandas\"</code>, <code>\"polars\"</code> or <code>\"numpy\"</code> respectively. Defaults to <code>list</code>. Returned <code>list</code>, <code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>np.ndarray</code> contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query.</p> <p>Raises</p> <p><code>Exception</code>. When primary key entry cannot be found in one or more of the feature groups used by this     feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_feature_vectors","title":"get_feature_vectors","text":"<pre><code>FeatureView.get_feature_vectors(\n    entry,\n    passed_features=None,\n    external=None,\n    return_type=\"list\",\n    allow_missing=False,\n    force_rest_client=False,\n    force_sql_client=False,\n    transform=True,\n    request_parameters=None,\n)\n</code></pre> <p>Returns assembled feature vectors in batches from online feature store.     Call <code>feature_view.init_serving</code> before this method if the following configurations are needed.       1. The training dataset version of the transformation statistics       2. Additional configurations of online serving engine</p> <p>Missing primary key entries</p> <p>If any of the provided primary key elements in <code>entry</code> can't be found in any of the feature groups, no feature vector for that primary key value will be returned. If it can be found in at least one but not all feature groups used by this feature view the call to this method will raise an exception. Alternatively, setting <code>allow_missing</code> to <code>True</code> returns feature vectors with missing values.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get assembled serving vectors as a python list of lists\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ]\n)\n\n# get assembled serving vectors as a pandas dataframe\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ],\n    return_type = \"pandas\"\n)\n\n# get assembled serving vectors as a numpy array\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ],\n    return_type = \"numpy\"\n)\n</code></pre> <p>Arguments</p> <ul> <li>entry <code>List[Dict[str, Any]]</code>: a list of dictionary of feature group primary key and values provided by serving application.     Set of required primary keys is <code>feature_view.primary_keys</code>     If the required primary keys is not provided, it will look for name     of the primary key in feature group in the entry.</li> <li>passed_features <code>List[Dict[str, Any]] | None</code>: a list of dictionary of feature values provided by the application at runtime.     They can replace features values fetched from the feature store as well as     providing feature values which are not available in the feature store.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hopsworks.login()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> <li>return_type <code>Literal['list', 'polars', 'numpy', 'pandas']</code>: <code>\"list\"</code>, <code>\"pandas\"</code>, <code>\"polars\"</code> or <code>\"numpy\"</code>. Defaults to <code>\"list\"</code>.</li> <li>force_sql_client <code>bool</code>: boolean, defaults to False. If set to True, reads from online feature store     using the SQL client if initialised.</li> <li>force_rest_client <code>bool</code>: boolean, defaults to False. If set to True, reads from online feature store     using the REST client if initialised.</li> <li>allow_missing <code>bool</code>: Setting to <code>True</code> returns feature vectors with missing values.</li> <li>transformed: Setting to <code>False</code> returns the untransformed feature vectors.</li> <li>request_parameters <code>List[Dict[str, Any]] | None</code>: Request parameters required by on-demand transformation functions to compute on-demand features present in the feature view.</li> </ul> <p>Returns</p> <p><code>List[list]</code>, <code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>np.ndarray</code> if <code>return type</code> is set to <code>\"list\",</code>\"pandas\"<code>,</code>\"polars\"<code>or</code>\"numpy\"<code>respectively. Defaults to</code>List[list]`.</p> <p>Returned <code>List[list]</code>, <code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>np.ndarray</code> contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query.</p> <p>Raises</p> <p><code>Exception</code>. When primary key entry cannot be found in one or more of the feature groups used by this     feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_inference_helper","title":"get_inference_helper","text":"<pre><code>FeatureView.get_inference_helper(\n    entry, external=None, return_type=\"pandas\", force_rest_client=False, force_sql_client=False\n)\n</code></pre> <p>Returns assembled inference helper column vectors from online feature store.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get assembled inference helper column vector\nfeature_view.get_inference_helper(\n    entry = {\"pk1\": 1, \"pk2\": 2}\n)\n</code></pre> <p>Arguments</p> <ul> <li>entry <code>Dict[str, Any]</code>: dictionary of feature group primary key and values provided by serving application.     Set of required primary keys is <code>feature_view.primary_keys</code></li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hopsworks.login()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> <li>return_type <code>Literal['pandas', 'dict', 'polars']</code>: <code>\"pandas\"</code>, <code>\"polars\"</code> or <code>\"dict\"</code>. Defaults to <code>\"pandas\"</code>.</li> </ul> <p>Returns</p> <p><code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>dict</code>. Defaults to <code>pd.DataFrame</code>.</p> <p>Raises</p> <p><code>Exception</code>. When primary key entry cannot be found in one or more of the feature groups used by this     feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_inference_helpers","title":"get_inference_helpers","text":"<pre><code>FeatureView.get_inference_helpers(\n    entry, external=None, return_type=\"pandas\", force_sql_client=False, force_rest_client=False\n)\n</code></pre> <p>Returns assembled inference helper column vectors in batches from online feature store.</p> <p>Missing primary key entries</p> <p>If any of the provided primary key elements in <code>entry</code> can't be found in any of the feature groups, no inference helper column vectors for that primary key value will be returned. If it can be found in at least one but not all feature groups used by this feature view the call to this method will raise an exception.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get assembled inference helper column vectors\nfeature_view.get_inference_helpers(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ]\n)\n</code></pre> <p>Arguments</p> <ul> <li>entry <code>List[Dict[str, Any]]</code>: a list of dictionary of feature group primary key and values provided by serving application.     Set of required primary keys is <code>feature_view.primary_keys</code></li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hopsworks.login()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> <li>return_type <code>Literal['pandas', 'dict', 'polars']</code>: <code>\"pandas\"</code>, <code>\"polars\"</code> or <code>\"dict\"</code>. Defaults to <code>\"pandas\"</code>.</li> </ul> <p>Returns</p> <p><code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>List[Dict[str, Any]]</code>.  Defaults to <code>pd.DataFrame</code>.</p> <p>Returned <code>pd.DataFrame</code> or <code>List[dict]</code>  contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query.</p> <p>Raises</p> <p><code>Exception</code>. When primary key entry cannot be found in one or more of the feature groups used by this     feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_last_accessed_training_dataset","title":"get_last_accessed_training_dataset","text":"<pre><code>FeatureView.get_last_accessed_training_dataset()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_log_timeline","title":"get_log_timeline","text":"<pre><code>FeatureView.get_log_timeline(wallclock_time=None, limit=None, transformed=False)\n</code></pre> <p>Retrieve the log timeline for the current feature view.</p> <p>Arguments</p> <ul> <li>wallclock_time <code>str | int | datetime.datetime | .datetime.date | None</code>: Specific time to get the log timeline for. Can be a string, integer, datetime, or date. Defaults to None.</li> <li>limit <code>int | None</code>: Maximum number of entries to retrieve. Defaults to None.</li> <li>transformed <code>bool | None</code>: Whether to include transformed logs. Defaults to False.</li> </ul> <p>Example</p> <pre><code># get log timeline\nlog_timeline = feature_view.get_log_timeline(limit=10)\n</code></pre> <p>Returns</p> <p><code>Dict[str, Dict[str, str]]</code>. Dictionary object of commit metadata timeline, where Key is commit id and value is <code>Dict[str, str]</code> with key value pairs of date committed on, number of rows updated, inserted and deleted.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the log timeline.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_models","title":"get_models","text":"<pre><code>FeatureView.get_models(training_dataset_version=None)\n</code></pre> <p>Get the generated models using this feature view, based on explicit provenance. Only the accessible models are returned. For more items use the base method - get_models_provenance</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: Filter generated models based on the used training dataset version.</li> </ul> <p>Returns</p> <p>`List[Model]: List of models.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_models_provenance","title":"get_models_provenance","text":"<pre><code>FeatureView.get_models_provenance(training_dataset_version=None)\n</code></pre> <p>Get the generated models using this feature view, based on explicit provenance. These models can be accessible or inaccessible. Explicit provenance does not track deleted generated model links, so deleted will always be empty. For inaccessible models, only a minimal information is returned.</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: Filter generated models based on the used training dataset version.</li> </ul> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_newest_model","title":"get_newest_model","text":"<pre><code>FeatureView.get_newest_model(training_dataset_version=None)\n</code></pre> <p>Get the latest generated model using this feature view, based on explicit provenance. Search only through the accessible models. For more items use the base method - get_models_provenance</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: Filter generated models based on the used training dataset version.</li> </ul> <p>Returns</p> <p><code>Model</code>: Newest Generated Model.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_parent_feature_groups","title":"get_parent_feature_groups","text":"<pre><code>FeatureView.get_parent_feature_groups()\n</code></pre> <p>Get the parents of this feature view, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_tag","title":"get_tag","text":"<pre><code>FeatureView.get_tag(name)\n</code></pre> <p>Get the tags of a feature view.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get a tag of a feature view\nname = feature_view.get_tag('tag_name')\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_tags","title":"get_tags","text":"<pre><code>FeatureView.get_tags()\n</code></pre> <p>Returns all tags attached to a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get tags\nlist_tags = feature_view.get_tags()\n</code></pre> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_train_test_split","title":"get_train_test_split","text":"<pre><code>FeatureView.get_train_test_split(\n    training_dataset_version,\n    read_options=None,\n    primary_key=False,\n    event_time=False,\n    training_helper_columns=False,\n    dataframe_type=\"default\",\n    **kwargs\n)\n</code></pre> <p>Get training data created by <code>feature_view.create_train_test_split</code> or <code>feature_view.train_test_split</code>.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(training_dataset_version=1)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>read_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     For python engine:<ul> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code> Defaults to <code>{}</code>.</li> </ul> </li> <li>primary_key <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view or during     materializing training dataset in the file system then<code>training_helper_columns=True</code> will not have     any effect. Defaults to <code>False</code>, no training helper columns.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p>(X_train, X_test, y_train, y_test):     Tuple of dataframe of features and labels</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_train_validation_test_split","title":"get_train_validation_test_split","text":"<pre><code>FeatureView.get_train_validation_test_split(\n    training_dataset_version,\n    read_options=None,\n    primary_key=False,\n    event_time=False,\n    training_helper_columns=False,\n    dataframe_type=\"default\",\n    **kwargs\n)\n</code></pre> <p>Get training data created by <code>feature_view.create_train_validation_test_split</code> or <code>feature_view.train_validation_test_split</code>.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.get_train_validation_test_splits(training_dataset_version=1)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>read_options <code>Dict[str, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     For python engine:<ul> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code> Defaults to <code>{}</code>.</li> </ul> </li> <li>primary_key <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view or during     materializing training dataset in the file system then<code>training_helper_columns=True</code> will not have     any effect. Defaults to <code>False</code>, no training helper columns.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p>(X_train, X_val, X_test, y_train, y_val, y_test):     Tuple of dataframe of features and labels</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_training_data","title":"get_training_data","text":"<pre><code>FeatureView.get_training_data(\n    training_dataset_version,\n    read_options=None,\n    primary_key=False,\n    event_time=False,\n    training_helper_columns=False,\n    dataframe_type=\"default\",\n    **kwargs\n)\n</code></pre> <p>Get training data created by <code>feature_view.create_training_data</code> or <code>feature_view.training_data</code>.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nfeatures_df, labels_df = feature_view.get_training_data(training_dataset_version=1)\n</code></pre> <p>External Storage Support</p> <p>Reading training data that was written to external storage using a Storage Connector other than S3 can currently not be read using HSFS APIs with Python as Engine, instead you will have to use the storage's native client.</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>read_options <code>Dict[str, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     For python engine:<ul> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code> Defaults to <code>{}</code>.</li> </ul> </li> <li>primary_key <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view or during     materializing training dataset in the file system then<code>training_helper_columns=True</code> will not have     any effect. Defaults to <code>False</code>, no training helper columns.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p>(X, y): Tuple of dataframe of features and labels</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_training_dataset_schema","title":"get_training_dataset_schema","text":"<pre><code>FeatureView.get_training_dataset_schema(training_dataset_version=None)\n</code></pre> <p>Function that returns the schema of the training dataset that is generated from a feature view. It provides the schema of the features after all transformation functions have been applied.</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: Specifies the version of the training dataset for which the schema should be generated.     By default, this is set to None. However, if the <code>one_hot_encoder</code> transformation function is used, the training dataset version must be provided.     This is because the schema will then depend on the statistics of the training data used.</li> </ul> <p>Example</p> <pre><code>schema = feature_view.get_training_dataset_schema(training_dataset_version=1)\n</code></pre> <p>Returns</p> <p><code>List[training_dataset_feature.TrainingDatasetFeature]</code>: List of training dataset features objects.</p> <p>Raises</p> <p><code>ValueError</code> if the  training dataset version provided cannot be found.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_training_dataset_statistics","title":"get_training_dataset_statistics","text":"<pre><code>FeatureView.get_training_dataset_statistics(\n    training_dataset_version, before_transformation=False, feature_names=None\n)\n</code></pre> <p>Get statistics of a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training dataset statistics\nstatistics = feature_view.get_training_dataset_statistics(training_dataset_version=1)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: Training dataset version</li> <li>before_transformation <code>bool</code>: Whether the statistics were computed before transformation functions or not.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code></p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_training_dataset_tag","title":"get_training_dataset_tag","text":"<pre><code>FeatureView.get_training_dataset_tag(training_dataset_version, name)\n</code></pre> <p>Get the tags of a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get a training dataset tag\ntag_str = feature_view.get_training_dataset_tag(\n    training_dataset_version=1,\n     name=\"tag_schema\"\n)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>name <code>str</code>: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_training_dataset_tags","title":"get_training_dataset_tags","text":"<pre><code>FeatureView.get_training_dataset_tags(training_dataset_version)\n</code></pre> <p>Returns all tags attached to a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get a training dataset tags\nlist_tags = feature_view.get_training_dataset_tags(\n    training_dataset_version=1\n)\n</code></pre> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_training_datasets","title":"get_training_datasets","text":"<pre><code>FeatureView.get_training_datasets()\n</code></pre> <p>Returns the metadata of all training datasets created with this feature view.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get all training dataset metadata\nlist_tds_meta = feature_view.get_training_datasets()\n</code></pre> <p>Returns</p> <p><code>List[TrainingDatasetBase]</code> List of training datasets metadata.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the training datasets metadata.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#init_batch_scoring","title":"init_batch_scoring","text":"<pre><code>FeatureView.init_batch_scoring(training_dataset_version=None)\n</code></pre> <p>Initialise feature view to retrieve feature vector from offline feature store.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# initialise feature view to retrieve feature vector from offline feature store\nfeature_view.init_batch_scoring(training_dataset_version=1)\n\n# get batch data\nbatch_data = feature_view.get_batch_data(...)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: int, optional. Default to be None. Transformation statistics     are fetched from training dataset and applied to the feature vector.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#init_serving","title":"init_serving","text":"<pre><code>FeatureView.init_serving(\n    training_dataset_version=None,\n    external=None,\n    options=None,\n    init_sql_client=None,\n    init_rest_client=False,\n    reset_rest_client=False,\n    config_rest_client=None,\n    default_client=None,\n    feature_logger=None,\n    **kwargs\n)\n</code></pre> <p>Initialise feature view to retrieve feature vector from online and offline feature store.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# initialise feature view to retrieve a feature vector\nfeature_view.init_serving(training_dataset_version=1)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: int, optional. Default to be 1 for online feature store.     Transformation statistics are fetched from training dataset and applied to the feature vector.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hopsworks.login()</code> method.     If set to False, the online feature store storage connector is used which relies on the private IP.     Defaults to True if connection to Hopsworks is established from external environment (e.g AWS     Sagemaker or Google Colab), otherwise to False.</li> <li>init_sql_client <code>bool | None</code>: boolean, optional. By default the sql client is initialised if no     client is specified to match legacy behaviour. If set to True, this ensure the online store     sql client is initialised, otherwise if init_rest_client is set to true it will     skip initialising the sql client.</li> <li>init_rest_client <code>bool</code>: boolean, defaults to False. By default the rest client is not initialised.     If set to True, this ensure the online store rest client is initialised. Pass additional configuration     options via the rest_config parameter. Set reset_rest_client to True to reset the rest client.</li> <li>default_client <code>Literal['sql', 'rest'] | None</code>: string, optional. Which client to default to if both are initialised. Defaults to None.</li> <li>options <code>Dict[str, Any] | None</code>: Additional options as key/value pairs for configuring online serving engine.<ul> <li>key: kwargs of SqlAlchemy engine creation (See: https://docs.sqlalchemy.org/en/20/core/engines.html#sqlalchemy.create_engine).   For example: <code>{\"pool_size\": 10}</code></li> </ul> </li> <li>reset_rest_client <code>bool</code>: boolean, defaults to False. If set to True, the rest client will be reset and reinitialised with provided configuration.</li> <li>config_rest_client <code>Dict[str, Any] | None</code>: dictionary, optional. Additional configuration options for the rest client. If the client is already initialised,     this will be ignored. Options include:<ul> <li><code>host</code>: string, optional. The host of the online store. Dynamically set if not provided.</li> <li><code>port</code>: int, optional. The port of the online store. Defaults to 4406.</li> <li><code>verify_certs</code>: boolean, optional. Verify the certificates of the online store server. Defaults to True.</li> <li><code>api_key</code>: string, optional. The API key to authenticate with the online store. The api key must be     provided if initialising the rest client in an internal environment.</li> <li><code>timeout</code>: int, optional. The timeout for the rest client in seconds. Defaults to 2.</li> <li><code>use_ssl</code>: boolean, optional. Use SSL to connect to the online store. Defaults to True.</li> </ul> </li> <li>feature_logger <code>hsfs.feature_logger.FeatureLogger | None</code>: Custom feature logger which <code>feature_view.log()</code> uses to log feature vectors. If provided,     feature vectors will not be inserted to logging feature group automatically when <code>feature_view.log()</code> is called.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#json","title":"json","text":"<pre><code>FeatureView.json()\n</code></pre> <p>Convert class into its json serialized form.</p> <p>Returns</p> <p><code>str</code>: Json serialized object.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#log","title":"log","text":"<pre><code>FeatureView.log(\n    untransformed_features=None,\n    predictions=None,\n    transformed_features=None,\n    write_options=None,\n    training_dataset_version=None,\n    model=None,\n)\n</code></pre> <p>Log features and optionally predictions for the current feature view. The logged features are written periodically to the offline store. If you need it to be available immediately, call <code>materialize_log</code>.</p> <p>Note: If features is a <code>pyspark.Dataframe</code>, prediction needs to be provided as columns in the dataframe,     values in <code>predictions</code> will be ignored.</p> <p>Arguments</p> <ul> <li>untransformed_features <code>pandas.DataFrame | list[list] | numpy.ndarray | hsfs.feature_view.pyspark.sql.DataFrame | None</code>: The untransformed features to be logged. Can be a pandas DataFrame, a list of lists, or a numpy ndarray.</li> <li>prediction: The predictions to be logged. Can be a pandas DataFrame, a list of lists, or a numpy ndarray. Defaults to None.</li> <li>transformed_features <code>pandas.DataFrame | list[list] | numpy.ndarray | hsfs.feature_view.pyspark.sql.DataFrame | None</code>: The transformed features to be logged. Can be a pandas DataFrame, a list of lists, or a numpy ndarray.</li> <li>write_options <code>Dict[str, Any] | None</code>: Options for writing the log. Defaults to None.</li> <li>training_dataset_version <code>int | None</code>: Version of the training dataset. If training dataset version is definied in     <code>init_serving</code> or <code>init_batch_scoring</code>, or model has training dataset version,     or training dataset version was cached, then the version will be used, otherwise defaults to None.</li> <li>model <code>hsml.model.Model | None</code>: <code>hsml.model.Model</code> Hopsworks model associated with the log. Defaults to None.</li> </ul> <p>Returns</p> <p><code>list[Job]</code> job information for feature insertion if python engine is used</p> <p>Example</p> <pre><code># log untransformed features\nfeature_view.log(features)\n# log features and predictions\nfeature_view.log(features, prediction)\n</code></pre> <pre><code># log both untransformed and transformed features\nfeature_view.log(\n    untransformed_features=features,\n    transformed_features=transformed_features\n)\n</code></pre> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to log features.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#materialize_log","title":"materialize_log","text":"<pre><code>FeatureView.materialize_log(wait=False, transformed=None)\n</code></pre> <p>Materialize the log for the current feature view.</p> <p>Arguments</p> <ul> <li>wait <code>bool</code>: Whether to wait for the materialization to complete. Defaults to False.</li> <li>transformed <code>bool | None</code>: Whether to materialize transformed or untrasformed logs. Defaults to None, in which case the returned list contains a job for materialization of transformed features and then a job for untransformed features. Otherwise the list contains only transformed jobs if transformed is True and untransformed jobs if it is False.</li> </ul> <p>Example</p> <pre><code># materialize log\nmaterialization_result = feature_view.materialize_log(wait=True)\n</code></pre> <p>Returns</p> <p>List[<code>Job</code>] Job information for the materialization jobs of transformed and untransformed features.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to materialize the log.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#pause_logging","title":"pause_logging","text":"<pre><code>FeatureView.pause_logging()\n</code></pre> <p>Pause scheduled materialization job for the current feature view.</p> <p>Example</p> <pre><code># pause logging\nfeature_view.pause_logging()\n</code></pre> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to pause feature logging.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#purge_all_training_data","title":"purge_all_training_data","text":"<pre><code>FeatureView.purge_all_training_data()\n</code></pre> <p>Delete all training datasets (data only).</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# purge all training data\nfeature_view.purge_all_training_data()\n</code></pre> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the training datasets.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#purge_training_data","title":"purge_training_data","text":"<pre><code>FeatureView.purge_training_data(training_dataset_version)\n</code></pre> <p>Delete a training dataset (data only).</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# purge training data\nfeature_view.purge_training_data(training_dataset_version=1)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: Version of the training dataset to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#read_log","title":"read_log","text":"<pre><code>FeatureView.read_log(\n    start_time=None,\n    end_time=None,\n    filter=None,\n    transformed=False,\n    training_dataset_version=None,\n    model=None,\n)\n</code></pre> <p>Read the log entries for the current feature view.     Optionally, filter can be applied to start/end time, training dataset version, hsml model,     and custom fitler.</p> <p>Arguments</p> <ul> <li>start_time <code>str | int | datetime.datetime | .datetime.date | None</code>: Start time for the log entries. Can be a string, integer, datetime, or date. Defaults to None.</li> <li>end_time <code>str | int | datetime.datetime | .datetime.date | None</code>: End time for the log entries. Can be a string, integer, datetime, or date. Defaults to None.</li> <li>filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Filter to apply on the log entries. Can be a Filter or Logic object. Defaults to None.</li> <li>transformed <code>bool | None</code>: Whether to include transformed logs. Defaults to False.</li> <li>training_dataset_version <code>int | None</code>: Version of the training dataset. Defaults to None.</li> <li>model <code>hsml.model.Model | None</code>: HSML model associated with the log. Defaults to None.</li> </ul> <p>Example</p> <pre><code># read all log entries\nlog_entries = feature_view.read_log()\n# read log entries within time ranges\nlog_entries = feature_view.read_log(start_time=\"2022-01-01\", end_time=\"2022-01-31\")\n# read log entries of a specific training dataset version\nlog_entries = feature_view.read_log(training_dataset_version=1)\n# read log entries of a specific hopsworks model\nlog_entries = feature_view.read_log(model=Model(1, \"dummy\", version=1))\n# read log entries by applying filter on features of feature group `fg` in the feature view\nlog_entries = feature_view.read_log(filter=fg.feature1 &gt; 10)\n</code></pre> <p>Returns</p> <p><code>DataFrame</code>: The spark dataframe containing the feature data. <code>pyspark.DataFrame</code>. A Spark DataFrame. <code>pandas.DataFrame</code>. A Pandas DataFrame. <code>polars.DataFrame</code>. A Polars DataFrame.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to read the log entries.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#recreate_training_dataset","title":"recreate_training_dataset","text":"<pre><code>FeatureView.recreate_training_dataset(\n    training_dataset_version, statistics_config=None, write_options=None, spine=None\n)\n</code></pre> <p>Recreate a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# recreate a training dataset that has been deleted\nfeature_view.recreate_training_dataset(training_dataset_version=1)\n</code></pre> <p>Info</p> <p>If a materialised training data has deleted. Use <code>recreate_training_dataset()</code> to recreate the training data.</p> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>write_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>use_spark</code> and value <code>True</code> to materialize training dataset   with Spark instead of Hopsworks Feature Query Service.</li> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> </ul> <p>Returns</p> <p><code>Job</code>: When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to create the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#resume_logging","title":"resume_logging","text":"<pre><code>FeatureView.resume_logging()\n</code></pre> <p>Resume scheduled materialization job for the current feature view.</p> <p>Example</p> <pre><code># resume logging\nfeature_view.resume_logging()\n</code></pre> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to pause feature logging.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#to_dict","title":"to_dict","text":"<pre><code>FeatureView.to_dict()\n</code></pre> <p>Convert class into a dictionary.</p> <p>Returns</p> <p><code>Dict</code>: Dictionary that contains all data required to json serialize the object.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#train_test_split","title":"train_test_split","text":"<pre><code>FeatureView.train_test_split(\n    test_size=None,\n    train_start=\"\",\n    train_end=\"\",\n    test_start=\"\",\n    test_end=\"\",\n    description=\"\",\n    extra_filter=None,\n    statistics_config=None,\n    read_options=None,\n    spine=None,\n    primary_key=False,\n    event_time=False,\n    training_helper_columns=False,\n    dataframe_type=\"default\",\n    **kwargs\n)\n</code></pre> <p>Create the metadata for a training dataset and get the corresponding training data from the offline feature store. This returns the training data in memory and does not materialise data in storage. The training data is split into train and test set at random or according to time ranges. The training data can be recreated by calling <code>feature_view.get_train_test_split</code> with the metadata created.</p> <p>Create random train/test splits</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nX_train, X_test, y_train, y_test = feature_view.train_test_split(\n    test_size=0.2\n)\n</code></pre> <p>Create time-series train/test splits</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\ntrain_start = \"2022-05-01 00:00:00\"\ntrain_end = \"2022-06-04 23:59:59\"\ntest_start = \"2022-07-01 00:00:00\"\ntest_end= \"2022-08-04 23:59:59\"\n# you can also pass dates as datetime objects\n\n# get training data\nX_train, X_test, y_train, y_test = feature_view.train_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset'\n)\n</code></pre> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>test_size <code>float | None</code>: size of test set. Should be between 0 and 1.</li> <li>train_start <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the train split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>train_end <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the train split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_start <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the test split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_end <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the test split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>extra_filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Additional filters to be attached to the training dataset.     The filters will be also applied in <code>get_batch_data</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>read_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, read_options can contain the     following entries:<ul> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> <li>key <code>spark</code> and value an object of type   hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_key <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view     then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper     columns.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p>(X_train, X_test, y_train, y_test):     Tuple of dataframe of features and labels</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#train_validation_test_split","title":"train_validation_test_split","text":"<pre><code>FeatureView.train_validation_test_split(\n    validation_size=None,\n    test_size=None,\n    train_start=\"\",\n    train_end=\"\",\n    validation_start=\"\",\n    validation_end=\"\",\n    test_start=\"\",\n    test_end=\"\",\n    description=\"\",\n    extra_filter=None,\n    statistics_config=None,\n    read_options=None,\n    spine=None,\n    primary_key=False,\n    event_time=False,\n    training_helper_columns=False,\n    dataframe_type=\"default\",\n    **kwargs\n)\n</code></pre> <p>Create the metadata for a training dataset and get the corresponding training data from the offline feature store. This returns the training data in memory and does not materialise data in storage. The training data is split into train, validation, and test set at random or according to time ranges. The training data can be recreated by calling <code>feature_view.get_train_validation_test_split</code> with the metadata created.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.train_validation_test_split(\n    validation_size=0.3,\n    test_size=0.2\n)\n</code></pre> <p>Time Series split</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nstart_time_train = '2017-01-01 00:00:01'\nend_time_train = '2018-02-01 23:59:59'\n\nstart_time_val = '2018-02-02 23:59:59'\nend_time_val = '2019-02-01 23:59:59'\n\nstart_time_test = '2019-02-02 23:59:59'\nend_time_test = '2020-02-01 23:59:59'\n# you can also pass dates as datetime objects\n\n# get training data\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.train_validation_test_split(\n    train_start=start_time_train,\n    train_end=end_time_train,\n    validation_start=start_time_val,\n    validation_end=end_time_val,\n    test_start=start_time_test,\n    test_end=end_time_test\n)\n</code></pre> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>validation_size <code>float | None</code>: size of validation set. Should be between 0 and 1.</li> <li>test_size <code>float | None</code>: size of test set. Should be between 0 and 1.</li> <li>train_start <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the train split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>train_end <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the train split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>validation_start <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the validation split query, inclusive. Strings     should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>validation_end <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the validation split query, exclusive. Strings     should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_start <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the test split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_end <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the test split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>extra_filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Additional filters to be attached to the training dataset.     The filters will be also applied in <code>get_batch_data</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>read_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, read_options can contain the     following entries:<ul> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> <li>key <code>spark</code> and value an object of type   hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_key <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view     then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper     columns.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p>(X_train, X_val, X_test, y_train, y_val, y_test):     Tuple of dataframe of features and labels</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#training_data","title":"training_data","text":"<pre><code>FeatureView.training_data(\n    start_time=None,\n    end_time=None,\n    description=\"\",\n    extra_filter=None,\n    statistics_config=None,\n    read_options=None,\n    spine=None,\n    primary_key=False,\n    event_time=False,\n    training_helper_columns=False,\n    dataframe_type=\"default\",\n    **kwargs\n)\n</code></pre> <p>Create the metadata for a training dataset and get the corresponding training data from the offline feature store. This returns the training data in memory and does not materialise data in storage. The training data can be recreated by calling <code>feature_view.get_training_data</code> with the metadata created.</p> <p>Create random splits</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nfeatures_df, labels_df  = feature_view.training_data(\n    description='Descriprion of a dataset',\n)\n</code></pre> <p>Create time-series based splits</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up a date\nstart_time = \"2022-05-01 00:00:00\"\nend_time = \"2022-06-04 23:59:59\"\n# you can also pass dates as datetime objects\n\n# get training data\nfeatures_df, labels_df = feature_view.training_data(\n    start_time=start_time,\n    end_time=end_time,\n    description='Description of a dataset'\n)\n</code></pre> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>start_time <code>int | str | datetime.datetime | datetime.date | None</code>: Start event time for the training dataset query, inclusive. Strings should be formatted in one of the following     formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>end_time <code>int | str | datetime.datetime | datetime.date | None</code>: End event time for the training dataset query, exclusive. Strings should be formatted in one of the following     formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>extra_filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Additional filters to be attached to the training dataset.     The filters will be also applied in <code>get_batch_data</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>read_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, read_options can contain the     following entries:<ul> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code>.</li> <li>key <code>spark</code> and value an object of type   hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_key <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view     then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper     columns.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p>(X, y): Tuple of dataframe of features and labels. If there are no labels, y returns <code>None</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#transform","title":"transform","text":"<pre><code>FeatureView.transform(feature_vector, external=None)\n</code></pre> <p>Transform the input feature vector by applying Model-dependent transformations attached to the feature view.</p> <p>List input must match the schema of the feature view</p> <pre><code>    If features are provided as a List to the transform function. Make sure that the input are ordered to match the schema\n    in the feature view.\n</code></pre> <p>Arguments</p> <ul> <li>feature_vector <code>List[Any] | List[List[Any]] | pandas.DataFrame | polars.dataframe.frame.DataFrame</code>: <code>Union[List[Any], List[List[Any]], pd.DataFrame, pl.DataFrame]</code>. The feature vector to be transformed.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hopsworks.login()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> </ul> <p>Returns</p> <p><code>Union[List[Any], List[List[Any]], pd.DataFrame, pl.DataFrame]</code>: The transformed feature vector obtained by applying Model-Dependent Transformations.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#update","title":"update","text":"<pre><code>FeatureView.update()\n</code></pre> <p>Update the description of the feature view.</p> <p>Update the feature view with a new description.</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\nfeature_view.description = \"new description\"\nfeature_view.update()\n\n# Description is updated in the metadata. Below should return \"new description\".\nfs.get_feature_view(\"feature_view_name\", 1).description\n</code></pre> <p>Returns</p> <p><code>FeatureView</code> Updated feature view.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#update_from_response_json","title":"update_from_response_json","text":"<pre><code>FeatureView.update_from_response_json(json_dict)\n</code></pre> <p>Function that updates the class object from its json serialization.</p> <p>Arguments</p> <ul> <li>json_dict <code>Dict[str, Any]</code>: <code>Dict[str, Any]</code>. Json serialized dictionary for the class.</li> </ul> <p>Returns</p> <p><code>TransformationFunction</code>: Json deserialized class object.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#update_last_accessed_training_dataset","title":"update_last_accessed_training_dataset","text":"<pre><code>FeatureView.update_last_accessed_training_dataset(version)\n</code></pre>"},{"location":"generated/api/flink_cluster/","title":"FlinkCluster API","text":""},{"location":"generated/api/flink_cluster/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/api/flink_cluster/#get_flink_cluster_api","title":"get_flink_cluster_api","text":"<pre><code>Project.get_flink_cluster_api()\n</code></pre> <p>Get the flink cluster API for the project.</p> <p>Returns</p> <p><code>FlinkClusterApi</code>: The Flink Cluster Api handle</p>"},{"location":"generated/api/flink_cluster/#setup-the-cluster","title":"Setup the cluster","text":"<p>[source]</p>"},{"location":"generated/api/flink_cluster/#setup_cluster","title":"setup_cluster","text":"<pre><code>FlinkClusterApi.setup_cluster(name, config=None)\n</code></pre> <p>Create a new flink job representing a flink cluster, or update an existing one.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nflink_cluster_api = project.get_flink_cluster_api()\n\nflink_config = flink_cluster_api.get_configuration()\n\nflink_config['appName'] = \"myFlinkCluster\"\n\nflink_cluster = flink_cluster_api.setup_cluster(name=\"myFlinkCluster\", config=flink_config)\n</code></pre> Arguments</p> <ul> <li>name <code>str</code>: Name of the cluster.</li> <li>config: Configuration of the cluster.</li> </ul> <p>Returns</p> <p><code>FlinkCluster</code>: The FlinkCluster object representing the cluster</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the flink cluster object</li> </ul>"},{"location":"generated/api/flink_cluster/#get-the-cluster","title":"Get the cluster","text":"<p>[source]</p>"},{"location":"generated/api/flink_cluster/#get_cluster","title":"get_cluster","text":"<pre><code>FlinkClusterApi.get_cluster(name)\n</code></pre> <p>Get the job corresponding to the flink cluster. <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nflink_cluster_api = project.get_flink_cluster_api()\n\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n</code></pre></p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the cluster.</li> </ul> <p>Returns</p> <p><code>FlinkCluster</code>: The FlinkCluster object representing the cluster</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the flink cluster object</li> </ul>"},{"location":"generated/api/flink_cluster/#start-the-cluster","title":"Start the cluster","text":"<p>[source]</p>"},{"location":"generated/api/flink_cluster/#start","title":"start","text":"<pre><code>FlinkCluster.start(await_time=1800)\n</code></pre> <p>Start the flink cluster and wait until it reaches RUNNING state.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nflink_cluster_api = project.get_flink_cluster_api()\n\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\nflink_cluster.start()\n</code></pre> Arguments</p> <ul> <li>await_time: defaults to 1800 seconds to account for auto-scale mechanisms.</li> </ul> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to start the flink cluster.</li> </ul>"},{"location":"generated/api/flink_cluster/#submit-job-to-cluster","title":"Submit job to cluster","text":"<p>[source]</p>"},{"location":"generated/api/flink_cluster/#submit_job","title":"submit_job","text":"<pre><code>FlinkCluster.submit_job(jar_id, main_class, job_arguments=None)\n</code></pre> <p>Submit job using the specific jar file uploaded to the flink cluster. <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# upload jar file to this cluster\nmain_class = \"com.example.Main\"\njob_arguments = \"-arg1 arg1 -arg2 arg2\"\njar_file_path = \"./flink-example.jar\"\nflink_cluster.upload_jar(jar_file_path)\n\n#get jar file metadata (and select the 1st one for demo purposes)\njar_metadata = flink_cluster.get_jars()[0]\njar_id = jar_metadata[\"id\"]\nflink_cluster.submit_job(jar_id, main_class, job_arguments=job_arguments)\n</code></pre></p> <p>Arguments</p> <ul> <li>jar_id: id if the jar file</li> <li>main_class: path to the main class of the jar file</li> <li>job_arguments: Job arguments (if any), defaults to none.</li> </ul> <p>Returns</p> <p><code>str</code>:  job id.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to submit the job.</li> </ul>"},{"location":"generated/api/flink_cluster/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/flink_cluster/#config","title":"config","text":"<p>Configuration for the cluster</p> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#creation_time","title":"creation_time","text":"<p>Date of creation for the cluster</p> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#creator","title":"creator","text":"<p>Creator of the cluster</p> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#id","title":"id","text":"<p>Id of the cluster</p> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#name","title":"name","text":"<p>Name of the cluster</p> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#state","title":"state","text":"<p>State of the cluster</p>"},{"location":"generated/api/flink_cluster/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/flink_cluster/#get_jars","title":"get_jars","text":"<pre><code>FlinkCluster.get_jars()\n</code></pre> <p>Get already uploaded jars from the flink cluster. <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# get jar files from this cluster\nflink_cluster.get_jars()\n</code></pre></p> <p>Returns</p> <p><code>List[Dict]</code>: The array of dicts with jar metadata.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get jars from the flink cluster.</li> </ul> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#get_job","title":"get_job","text":"<pre><code>FlinkCluster.get_job(job_id)\n</code></pre> <p>Get specific job from the flink cluster. <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# get jobs from this cluster\njob_id = '113a2af5b724a9b92085dc2d9245e1d6'\nflink_cluster.get_job(job_id)\n</code></pre></p> <p>Arguments</p> <ul> <li>job_id: id of the job within this cluster</li> </ul> <p>Returns</p> <p><code>Dict</code>: Dict with flink job id and status of the job.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the job from the cluster</li> </ul> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#get_jobs","title":"get_jobs","text":"<pre><code>FlinkCluster.get_jobs()\n</code></pre> <p>Get jobs from the flink cluster. <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# get jobs from this flink cluster\nflink_cluster.get_jobs()\n</code></pre></p> <p>Returns</p> <p><code>List[Dict]</code>: The array of dicts with flink job id and status of the job.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the jobs from the cluster</li> </ul> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#get_url","title":"get_url","text":"<pre><code>FlinkCluster.get_url()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#job_state","title":"job_state","text":"<pre><code>FlinkCluster.job_state(job_id)\n</code></pre> <p>Gets state of the job submitted to the flink cluster. <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# get jobs from this flink cluster\njob_id = '113a2af5b724a9b92085dc2d9245e1d6'\nflink_cluster.job_state(job_id)\n</code></pre></p> <p>Arguments</p> <ul> <li>job_id: id of the job within this flink cluster</li> </ul> <p>Returns</p> <p><code>str</code>: status of the job. Possible states:  \"INITIALIZING\", \"CREATED\", \"RUNNING\", \"FAILING\", \"FAILED\", \"CANCELLING\", \"CANCELED\",  \"FINISHED\", \"RESTARTING\", \"SUSPENDED\", \"RECONCILING\".</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the job state from the flink cluster.</li> </ul> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#start_1","title":"start","text":"<pre><code>FlinkCluster.start(await_time=1800)\n</code></pre> <p>Start the flink cluster and wait until it reaches RUNNING state.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nflink_cluster_api = project.get_flink_cluster_api()\n\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\nflink_cluster.start()\n</code></pre> Arguments</p> <ul> <li>await_time: defaults to 1800 seconds to account for auto-scale mechanisms.</li> </ul> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to start the flink cluster.</li> </ul> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#stop","title":"stop","text":"<pre><code>FlinkCluster.stop()\n</code></pre> <p>Stop this cluster. <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\n\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\nflink_cluster.stop()\n</code></pre></p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to stop the flink cluster.</li> </ul> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#stop_job","title":"stop_job","text":"<pre><code>FlinkCluster.stop_job(job_id)\n</code></pre> <p>Stop specific job in the flink cluster. <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# stop the job\njob_id = '113a2af5b724a9b92085dc2d9245e1d6'\nflink_cluster.stop_job(job_id)\n</code></pre></p> <p>Arguments</p> <ul> <li>job_id: id of the job within this flink cluster.</li> </ul> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to stop the job</li> </ul> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#submit_job_1","title":"submit_job","text":"<pre><code>FlinkCluster.submit_job(jar_id, main_class, job_arguments=None)\n</code></pre> <p>Submit job using the specific jar file uploaded to the flink cluster. <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# upload jar file to this cluster\nmain_class = \"com.example.Main\"\njob_arguments = \"-arg1 arg1 -arg2 arg2\"\njar_file_path = \"./flink-example.jar\"\nflink_cluster.upload_jar(jar_file_path)\n\n#get jar file metadata (and select the 1st one for demo purposes)\njar_metadata = flink_cluster.get_jars()[0]\njar_id = jar_metadata[\"id\"]\nflink_cluster.submit_job(jar_id, main_class, job_arguments=job_arguments)\n</code></pre></p> <p>Arguments</p> <ul> <li>jar_id: id if the jar file</li> <li>main_class: path to the main class of the jar file</li> <li>job_arguments: Job arguments (if any), defaults to none.</li> </ul> <p>Returns</p> <p><code>str</code>:  job id.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to submit the job.</li> </ul> <p>[source]</p>"},{"location":"generated/api/flink_cluster/#upload_jar","title":"upload_jar","text":"<pre><code>FlinkCluster.upload_jar(jar_file)\n</code></pre> <p>Upload jar file to the flink cluster. <pre><code># log in to hopsworks\nimport hopsworks\nproject = hopsworks.login()\n\n# fetch flink cluster handle\nflink_cluster_api = project.get_flink_cluster_api()\nflink_cluster = flink_cluster_api.get_cluster(name=\"myFlinkCluster\")\n\n# upload jar file to this cluster\njar_file_path = \"./flink-example.jar\"\nflink_cluster.upload_jar(jar_file_path)\n</code></pre></p> <p>Arguments</p> <ul> <li>jar_file: path to the jar file.</li> </ul> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to upload jar file</li> </ul>"},{"location":"generated/api/git_provider/","title":"GitProvider API","text":""},{"location":"generated/api/git_provider/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/api/git_provider/#get_git_api","title":"get_git_api","text":"<pre><code>Project.get_git_api()\n</code></pre> <p>Get the git repository api for the project.</p> <p>Returns</p> <p><code>GitApi</code>: The Git Api handle</p>"},{"location":"generated/api/git_provider/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/git_provider/#set_provider","title":"set_provider","text":"<pre><code>GitApi.set_provider(provider, username, token)\n</code></pre> <p>Configure a Git provider</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n\ngit_api.set_provider(\"GitHub\", \"my_user\", \"my_token\")\n</code></pre> Arguments</p> <ul> <li>provider <code>str</code>: Name of git provider. Valid values are \"GitHub\", \"GitLab\" and \"BitBucket\".</li> <li>username <code>str</code>: Username for the git provider service</li> <li>token <code>str</code>: Token to set for the git provider service</li> </ul> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to configure the git provider</li> </ul>"},{"location":"generated/api/git_provider/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/git_provider/#get_provider","title":"get_provider","text":"<pre><code>GitApi.get_provider(provider)\n</code></pre> <p>Get the configured Git provider</p> <p>Arguments</p> <ul> <li>provider <code>str</code>: Name of git provider. Valid values are \"GitHub\", \"GitLab\" and \"BitBucket\".</li> </ul> <p>Returns</p> <p><code>GitProvider</code>: The git provider</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the git provider</li> </ul> <p>[source]</p>"},{"location":"generated/api/git_provider/#get_providers","title":"get_providers","text":"<pre><code>GitApi.get_providers()\n</code></pre> <p>Get the configured Git providers</p> <p>Returns</p> <p><code>List[GitProvider]</code>: List of git provider objects</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the git providers</li> </ul>"},{"location":"generated/api/git_provider/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/git_provider/#git_provider","title":"git_provider","text":"<p>Name of the provider, can be GitHub, GitLab or BitBucket</p> <p>[source]</p>"},{"location":"generated/api/git_provider/#username","title":"username","text":"<p>Username set for the provider</p>"},{"location":"generated/api/git_provider/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/git_provider/#delete","title":"delete","text":"<pre><code>GitProvider.delete()\n</code></pre> <p>Remove the git provider configuration.</p> <p>Raises</p> <p><code>RestAPIError</code>.</p>"},{"location":"generated/api/git_remote/","title":"GitRemote API","text":""},{"location":"generated/api/git_remote/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/api/git_remote/#get_git_api","title":"get_git_api","text":"<pre><code>Project.get_git_api()\n</code></pre> <p>Get the git repository api for the project.</p> <p>Returns</p> <p><code>GitApi</code>: The Git Api handle</p>"},{"location":"generated/api/git_remote/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/git_remote/#add_remote","title":"add_remote","text":"<pre><code>GitRepo.add_remote(name, url)\n</code></pre> <p>Add a remote for the repo</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n\nrepo = git_api.get_repo(\"my_repo\")\n\nrepo.add_remote(\"upstream\", \"https://github.com/organization/repo.git\")\n</code></pre> Arguments</p> <ul> <li>name <code>str</code>: name of the remote</li> <li>url <code>str</code>: url of the remote</li> </ul> <p>Returns</p> <p><code>GitRemote</code></p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to add the remote.</p>"},{"location":"generated/api/git_remote/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/git_remote/#get_remote","title":"get_remote","text":"<pre><code>GitRepo.get_remote(name)\n</code></pre> <p>Get a remote by name for the repo.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: name of the remote</li> </ul> <p>Returns</p> <p><code>GitRemote</code></p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to get the remote.</p> <p>[source]</p>"},{"location":"generated/api/git_remote/#get_remotes","title":"get_remotes","text":"<pre><code>GitRepo.get_remotes()\n</code></pre> <p>Get the configured remotes for the repo.</p> <p>Returns</p> <p><code>List[GitRemote]</code></p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve the remotes.</p>"},{"location":"generated/api/git_remote/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/git_remote/#name","title":"name","text":"<p>Name of the remote</p> <p>[source]</p>"},{"location":"generated/api/git_remote/#url","title":"url","text":"<p>Url of the remote</p>"},{"location":"generated/api/git_remote/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/git_remote/#delete","title":"delete","text":"<pre><code>GitRemote.delete()\n</code></pre> <p>Remove the git remote from the repo.</p> <p>Raises</p> <p><code>RestAPIError</code>.</p>"},{"location":"generated/api/git_repo/","title":"GitRepo API","text":""},{"location":"generated/api/git_repo/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/api/git_repo/#get_git_api","title":"get_git_api","text":"<pre><code>Project.get_git_api()\n</code></pre> <p>Get the git repository api for the project.</p> <p>Returns</p> <p><code>GitApi</code>: The Git Api handle</p>"},{"location":"generated/api/git_repo/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/git_repo/#clone","title":"clone","text":"<pre><code>GitApi.clone(url, path, provider=None, branch=None)\n</code></pre> <p>Clone a new Git Repo in to Hopsworks Filesystem.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n\ngit_repo = git_api.clone(\"https://github.com/logicalclocks/hops-examples.git\", \"Resources\", \"GitHub\")\n</code></pre> Arguments</p> <ul> <li>url <code>str</code>: Url to the git repository</li> <li>path <code>str</code>: Path in Hopsworks Filesystem to clone the repo to</li> <li>provider <code>str</code>: The git provider where the repo is currently hosted. Valid values are \"GitHub\", \"GitLab\" and \"BitBucket\".</li> <li>branch <code>str</code>: Optional branch to clone, defaults to configured main branch</li> </ul> <p>Returns</p> <p><code>GitRepo</code>: Git repository object</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to clone the git repository.</li> </ul>"},{"location":"generated/api/git_repo/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/git_repo/#get_repo","title":"get_repo","text":"<pre><code>GitApi.get_repo(name, path=None)\n</code></pre> <p>Get the cloned Git repository</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of git repository</li> <li>path <code>str</code>: Optional path to specify if multiple git repos with the same name exists in the project</li> </ul> <p>Returns</p> <p><code>GitRepo</code>: The git repository</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the git repository</li> </ul> <p>[source]</p>"},{"location":"generated/api/git_repo/#get_repos","title":"get_repos","text":"<pre><code>GitApi.get_repos()\n</code></pre> <p>Get the existing Git repositories</p> <p>Returns</p> <p><code>List[GitRepo]</code>: List of git repository objects</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the repositories</li> </ul>"},{"location":"generated/api/git_repo/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/git_repo/#creator","title":"creator","text":"<p>Creator of the git repo</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#current_branch","title":"current_branch","text":"<p>The current branch for the git repo</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#current_commit","title":"current_commit","text":"<p>The current commit for the git repo</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#id","title":"id","text":"<p>Id of the git repo</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#name","title":"name","text":"<p>Name of the git repo</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#path","title":"path","text":"<p>Path to the git repo in the Hopsworks Filesystem</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#provider","title":"provider","text":"<p>Git provider for the repo, can be GitHub, GitLab or BitBucket</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#read_only","title":"read_only","text":"<p>If True then the repository functions <code>GitRepo.commit</code>, <code>GitRepo.push</code> and <code>GitRepo.checkout_files</code> are forbidden.</p>"},{"location":"generated/api/git_repo/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/git_repo/#add_remote","title":"add_remote","text":"<pre><code>GitRepo.add_remote(name, url)\n</code></pre> <p>Add a remote for the repo</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\ngit_api = project.get_git_api()\n\nrepo = git_api.get_repo(\"my_repo\")\n\nrepo.add_remote(\"upstream\", \"https://github.com/organization/repo.git\")\n</code></pre> Arguments</p> <ul> <li>name <code>str</code>: name of the remote</li> <li>url <code>str</code>: url of the remote</li> </ul> <p>Returns</p> <p><code>GitRemote</code></p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to add the remote.</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#checkout_branch","title":"checkout_branch","text":"<pre><code>GitRepo.checkout_branch(branch, create=False)\n</code></pre> <p>Checkout a branch</p> <p>Arguments</p> <ul> <li>branch <code>str</code>: name of the branch</li> <li>create <code>bool</code>: if true will create a new branch and check it out</li> </ul> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve the commits.</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#checkout_commit","title":"checkout_commit","text":"<pre><code>GitRepo.checkout_commit(commit)\n</code></pre> <p>Checkout a commit</p> <p>Arguments</p> <ul> <li>commit <code>str</code>: hash of the commit</li> </ul> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve the commits.</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#checkout_files","title":"checkout_files","text":"<pre><code>GitRepo.checkout_files(files)\n</code></pre> <p>Checkout a list of files</p> <p>Arguments</p> <ul> <li>files <code>List[str] | List[hopsworks_common.git_file_status.GitFileStatus]</code>: list of files or GitFileStatus objects to checkout</li> </ul> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to checkout the files.</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#commit","title":"commit","text":"<pre><code>GitRepo.commit(message, all=True, files=None)\n</code></pre> <p>Add changes and new files, and then commit them</p> <p>Arguments</p> <ul> <li>message <code>str</code>: name of the remote</li> <li>all <code>bool</code>: automatically stage files that have been modified and deleted, but new files are not affected</li> <li>files <code>List[str]</code>: list of new files to add and commit</li> </ul> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to perform the commit.</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#delete","title":"delete","text":"<pre><code>GitRepo.delete()\n</code></pre> <p>Delete the git repo from the filesystem.</p> <p>Potentially dangerous operation</p> <p>This operation deletes the cloned git repository from the filesystem.</p> <p>Raises</p> <p><code>RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#delete_branch","title":"delete_branch","text":"<pre><code>GitRepo.delete_branch(branch)\n</code></pre> <p>Delete a branch from local repository</p> <p>Arguments</p> <ul> <li>branch <code>str</code>: name of the branch</li> </ul> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to delete the branch.</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#get_commits","title":"get_commits","text":"<pre><code>GitRepo.get_commits(branch)\n</code></pre> <p>Get the commits for the repo and branch.</p> <p>Arguments</p> <ul> <li>branch <code>str</code>: name of the branch</li> </ul> <p>Returns</p> <p><code>List[GitCommit]</code></p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve the commits.</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#get_remote","title":"get_remote","text":"<pre><code>GitRepo.get_remote(name)\n</code></pre> <p>Get a remote by name for the repo.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: name of the remote</li> </ul> <p>Returns</p> <p><code>GitRemote</code></p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to get the remote.</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#get_remotes","title":"get_remotes","text":"<pre><code>GitRepo.get_remotes()\n</code></pre> <p>Get the configured remotes for the repo.</p> <p>Returns</p> <p><code>List[GitRemote]</code></p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve the remotes.</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#pull","title":"pull","text":"<pre><code>GitRepo.pull(branch, remote=\"origin\")\n</code></pre> <p>Pull changes from remote branch</p> <p>Arguments</p> <ul> <li>branch <code>str</code>: name of the branch</li> <li>remote <code>str</code>: name of the remote</li> </ul> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve the commits.</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#push","title":"push","text":"<pre><code>GitRepo.push(branch, remote=\"origin\")\n</code></pre> <p>Push changes to the remote branch</p> <p>Arguments</p> <ul> <li>branch <code>str</code>: name of the branch</li> <li>remote <code>str</code>: name of the remote</li> </ul> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve the commits.</p> <p>[source]</p>"},{"location":"generated/api/git_repo/#status","title":"status","text":"<pre><code>GitRepo.status()\n</code></pre> <p>Get the status of the repo.</p> <p>Returns</p> <p><code>List[GitFileStatus]</code></p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve the status.</p>"},{"location":"generated/api/hopsworks_udf/","title":"HopsworksUDF","text":"<p>[source]</p>"},{"location":"generated/api/hopsworks_udf/#hopsworksudf_1","title":"HopsworksUdf","text":"<pre><code>hsfs.hopsworks_udf.HopsworksUdf(\n    func,\n    return_types,\n    execution_mode,\n    name=None,\n    transformation_features=None,\n    transformation_function_argument_names=None,\n    dropped_argument_names=None,\n    dropped_feature_names=None,\n    feature_name_prefix=None,\n)\n</code></pre> <p>Meta data for user defined functions.</p> <p>Stores meta data required to execute the user defined function in both spark and python engine. The class generates uses the metadata to dynamically generate user defined functions based on the engine it is executed in.</p> <p>Arguments</p> <ul> <li>func : <code>Union[Callable, str]</code>. The transformation function object or the source code of the transformation function.</li> <li>return_types : <code>Union[List[type], type, List[str], str]</code>. A python type or a list of python types that denotes the data types of the columns output from the transformation functions.</li> <li>name : <code>Optional[str]</code>. Name of the transformation function.</li> <li>transformation_features : <code>Optional[List[TransformationFeature]]</code>. A list of objects of <code>TransformationFeature</code> that maps the feature used for transformation to their corresponding statistics argument names if any</li> <li>transformation_function_argument_names : <code>Optional[List[TransformationFeature]]</code>. The argument names of the transformation function.</li> <li>dropped_argument_names : <code>Optional[List[str]]</code>. The arguments to be dropped from the finial DataFrame after the transformation functions are applied.</li> <li>dropped_feature_names : <code>Optional[List[str]]</code>. The feature name corresponding to the arguments names that are dropped</li> <li>feature_name_prefix <code>str | None</code>: <code>Optional[str]</code> = None. Prefixes if any used in the feature view.</li> </ul>"},{"location":"generated/api/hopsworks_udf/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/hopsworks_udf/#dropped_features","title":"dropped_features","text":"<p>List of features that will be dropped after the UDF is applied.</p> <p>[source]</p>"},{"location":"generated/api/hopsworks_udf/#execution_mode","title":"execution_mode","text":"<p>[source]</p>"},{"location":"generated/api/hopsworks_udf/#function_name","title":"function_name","text":"<p>Get the function name of the UDF</p> <p>[source]</p>"},{"location":"generated/api/hopsworks_udf/#output_column_names","title":"output_column_names","text":"<p>Output columns names of the transformation function</p> <p>[source]</p>"},{"location":"generated/api/hopsworks_udf/#return_types","title":"return_types","text":"<p>Get the output types of the UDF</p> <p>[source]</p>"},{"location":"generated/api/hopsworks_udf/#statistics_features","title":"statistics_features","text":"<p>List of feature names that require statistics</p> <p>[source]</p>"},{"location":"generated/api/hopsworks_udf/#statistics_required","title":"statistics_required","text":"<p>Get if statistics for any feature is required by the UDF</p> <p>[source]</p>"},{"location":"generated/api/hopsworks_udf/#transformation_features","title":"transformation_features","text":"<p>List of feature names to be used in the User Defined Function.</p> <p>[source]</p>"},{"location":"generated/api/hopsworks_udf/#transformation_statistics","title":"transformation_statistics","text":"<p>Feature statistics required for the defined UDF</p>"},{"location":"generated/api/hopsworks_udf/#transformationfeature","title":"TransformationFeature","text":"<p>[source]</p>"},{"location":"generated/api/hopsworks_udf/#transformationfeature_1","title":"TransformationFeature","text":"<pre><code>hsfs.hopsworks_udf.TransformationFeature(feature_name, statistic_argument_name)\n</code></pre> <p>Mapping of feature names to their corresponding statistics argument names in the code.</p> <p>The statistic_argument_name for a feature name would be None if the feature does not need statistics.</p> <p>Arguments</p> <ul> <li>feature_name : <code>str</code>. Name of the feature.</li> <li>statistic_argument_name : <code>str</code>. Name of the statistics argument in the code for the feature specified in the feature name.</li> </ul>"},{"location":"generated/api/jobs/","title":"Jobs API","text":""},{"location":"generated/api/jobs/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/api/jobs/#get_job_api","title":"get_job_api","text":"<pre><code>Project.get_job_api()\n</code></pre> <p>Get the job API for the project.</p> <p>Returns</p> <p><code>JobApi</code>: The Job Api handle</p>"},{"location":"generated/api/jobs/#configuration","title":"Configuration","text":"<p>[source]</p>"},{"location":"generated/api/jobs/#get_configuration","title":"get_configuration","text":"<pre><code>JobApi.get_configuration(type)\n</code></pre> <p>Get configuration for the specific job type.</p> <p>Arguments</p> <ul> <li>type <code>str</code>: Type of the job. Currently, supported types include: SPARK, PYSPARK, PYTHON, DOCKER, FLINK.</li> </ul> <p>Returns</p> <p><code>dict</code>: Default job configuration</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the job configuration</li> </ul> <p>[source]</p>"},{"location":"generated/api/jobs/#jobconfiguration","title":"JobConfiguration","text":"<pre><code>hopsworks_common.core.job_configuration.JobConfiguration(\n    driver_memory=2048,\n    driver_cores=1,\n    executor_memory=4096,\n    executor_cores=1,\n    executor_instances=1,\n    dynamic_allocation=True,\n    dynamic_min_executors=1,\n    dynamic_max_executors=2,\n    environment_name=\"spark-feature-pipeline\",\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/jobs/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/jobs/#create_job","title":"create_job","text":"<pre><code>JobApi.create_job(name, config)\n</code></pre> <p>Create a new job or update an existing one.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\njob_api = project.get_job_api()\n\nspark_config = job_api.get_configuration(\"PYSPARK\")\n\nspark_config['appPath'] = \"/Resources/my_app.py\"\n\njob = job_api.create_job(\"my_spark_job\", spark_config)\n</code></pre> Arguments</p> <ul> <li>name <code>str</code>: Name of the job.</li> <li>config <code>dict</code>: Configuration of the job.</li> </ul> <p>Returns</p> <p><code>Job</code>: The Job object</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to create the job</li> </ul>"},{"location":"generated/api/jobs/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/jobs/#get_job","title":"get_job","text":"<pre><code>JobApi.get_job(name)\n</code></pre> <p>Get a job.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the job.</li> </ul> <p>Returns</p> <p><code>Job</code>: The Job object</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the job</li> </ul> <p>[source]</p>"},{"location":"generated/api/jobs/#get_jobs","title":"get_jobs","text":"<pre><code>JobApi.get_jobs()\n</code></pre> <p>Get all jobs.</p> <p>Returns</p> <p><code>List[Job]</code>: List of Job objects</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the jobs</li> </ul>"},{"location":"generated/api/jobs/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/jobs/#config","title":"config","text":"<p>Configuration for the job</p> <p>[source]</p>"},{"location":"generated/api/jobs/#creation_time","title":"creation_time","text":"<p>Date of creation for the job</p> <p>[source]</p>"},{"location":"generated/api/jobs/#creator","title":"creator","text":"<p>Creator of the job</p> <p>[source]</p>"},{"location":"generated/api/jobs/#executions","title":"executions","text":"<p>[source]</p>"},{"location":"generated/api/jobs/#href","title":"href","text":"<p>[source]</p>"},{"location":"generated/api/jobs/#id","title":"id","text":"<p>Id of the job</p> <p>[source]</p>"},{"location":"generated/api/jobs/#job_schedule","title":"job_schedule","text":"<p>Return the Job schedule</p> <p>[source]</p>"},{"location":"generated/api/jobs/#job_type","title":"job_type","text":"<p>Type of the job</p> <p>[source]</p>"},{"location":"generated/api/jobs/#name","title":"name","text":"<p>Name of the job</p>"},{"location":"generated/api/jobs/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/jobs/#delete","title":"delete","text":"<pre><code>Job.delete()\n</code></pre> <p>Delete the job</p> <p>Potentially dangerous operation</p> <p>This operation deletes the job and all executions.</p> <p>Raises</p> <p><code>RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/jobs/#get_executions","title":"get_executions","text":"<pre><code>Job.get_executions()\n</code></pre> <p>Retrieves all executions for the job ordered by submission time.</p> <p>Returns</p> <p><code>List[Execution]</code></p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve executions.</p> <p>[source]</p>"},{"location":"generated/api/jobs/#get_final_state","title":"get_final_state","text":"<pre><code>Job.get_final_state()\n</code></pre> <p>Get the final state of the job.</p> <p>Returns</p> <p><code>final_state</code>. Final state of the job, which can be one of the following: <code>UNDEFINED</code>, <code>FINISHED</code>, <code>FAILED</code>, <code>KILLED</code>, <code>FRAMEWORK_FAILURE</code>, <code>APP_MASTER_START_FAILED</code>, <code>INITIALIZATION_FAILED</code>. <code>UNDEFINED</code> indicates  that the job is still running.</p> <p>[source]</p>"},{"location":"generated/api/jobs/#get_state","title":"get_state","text":"<pre><code>Job.get_state()\n</code></pre> <p>Get the state of the job.</p> <p>Returns</p> <p><code>state</code>. Current state of the job, which can be one of the following: <code>INITIALIZING</code>, <code>INITIALIZATION_FAILED</code>, <code>FINISHED</code>, <code>RUNNING</code>, <code>ACCEPTED</code>, <code>FAILED</code>, <code>KILLED</code>, <code>NEW</code>, <code>NEW_SAVING</code>, <code>SUBMITTED</code>, <code>AGGREGATING_LOGS</code>, <code>FRAMEWORK_FAILURE</code>, <code>STARTING_APP_MASTER</code>, <code>APP_MASTER_START_FAILED</code>, <code>GENERATING_SECURITY_MATERIAL</code>, <code>CONVERTING_NOTEBOOK</code>. If no executions are found for the job, a warning is raised and it returns <code>UNDEFINED</code>.</p> <p>[source]</p>"},{"location":"generated/api/jobs/#get_url","title":"get_url","text":"<pre><code>Job.get_url()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/jobs/#pause_schedule","title":"pause_schedule","text":"<pre><code>Job.pause_schedule()\n</code></pre> <p>Pauses the schedule of a Job execution</p> <p>[source]</p>"},{"location":"generated/api/jobs/#resume_schedule","title":"resume_schedule","text":"<pre><code>Job.resume_schedule()\n</code></pre> <p>Resumes the schedule of a Job execution</p> <p>[source]</p>"},{"location":"generated/api/jobs/#run","title":"run","text":"<pre><code>Job.run(args=None, await_termination=True)\n</code></pre> <p>Run the job.</p> <p>Run the job, by default awaiting its completion, with the option of passing runtime arguments.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instances\nfg = fs.get_or_create_feature_group(...)\n\n# insert in to feature group\njob, _ = fg.insert(df, write_options={\"start_offline_materialization\": False})\n\n# run job\nexecution = job.run()\n\n# True if job executed successfully\nprint(execution.success)\n\n# Download logs\nout_log_path, err_log_path = execution.download_logs()\n</code></pre> <p>Arguments</p> <ul> <li>args <code>str</code>: Optional runtime arguments for the job.</li> <li>await_termination <code>bool</code>: Identifies if the client should wait for the job to complete, defaults to True.</li> </ul> <p>Returns</p> <p><code>Execution</code>. The execution object for the submitted run.</p> <p>[source]</p>"},{"location":"generated/api/jobs/#save","title":"save","text":"<pre><code>Job.save()\n</code></pre> <p>Save the job.</p> <p>This function should be called after changing a property such as the job configuration to save it persistently.</p> <p><pre><code>job.config['appPath'] = \"Resources/my_app.py\"\njob.save()\n</code></pre> Returns</p> <p><code>Job</code>. The updated job object.</p> <p>[source]</p>"},{"location":"generated/api/jobs/#schedule","title":"schedule","text":"<pre><code>Job.schedule(cron_expression, start_time=None, end_time=None)\n</code></pre> <p>Schedule the execution of the job.</p> <p>If a schedule for this job already exists, the method updates it.</p> <pre><code># Schedule the job\njob.schedule(\n    cron_expression=\"0 */5 * ? * * *\",\n    start_time=datetime.datetime.now(tz=timezone.utc)\n)\n\n# Retrieve the next execution time\nprint(job.job_schedule.next_execution_date_time)\n</code></pre> <p>Arguments</p> <ul> <li>cron_expression <code>str</code>: The quartz cron expression</li> <li>start_time <code>datetime.datetime | None</code>: The schedule start time in UTC. If None, the current time is used. The start_time can be a value in the past.</li> <li>end_time <code>datetime.datetime | None</code>: The schedule end time in UTC. If None, the schedule will continue running indefinitely. The end_time can be a value in the past.</li> </ul> <p>Returns</p> <p><code>JobSchedule</code>. The schedule of the job</p> <p>[source]</p>"},{"location":"generated/api/jobs/#unschedule","title":"unschedule","text":"<pre><code>Job.unschedule()\n</code></pre> <p>Unschedule the exceution of a Job</p>"},{"location":"generated/api/kafka_schema/","title":"KafkaSchema API","text":""},{"location":"generated/api/kafka_schema/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/api/kafka_schema/#get_kafka_api","title":"get_kafka_api","text":"<pre><code>Project.get_kafka_api()\n</code></pre> <p>Get the kafka api for the project.</p> <p>Returns</p> <p><code>KafkaApi</code>: The Kafka Api handle</p>"},{"location":"generated/api/kafka_schema/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/kafka_schema/#create_schema","title":"create_schema","text":"<pre><code>KafkaApi.create_schema(subject, schema)\n</code></pre> <p>Create a new kafka schema.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n\navro_schema = {\n  \"type\": \"record\",\n  \"name\": \"tutorial\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"data\",\n      \"type\": \"string\"\n    }\n  ]\n}\n\nkafka_topic = kafka_api.create_schema(\"my_schema\", avro_schema)\n</code></pre> Arguments</p> <ul> <li>subject <code>str</code>: subject name of the schema</li> <li>schema <code>dict</code>: avro schema definition</li> </ul> <p>Returns</p> <p><code>KafkaSchema</code>: The KafkaSchema object</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to create the schema</li> </ul>"},{"location":"generated/api/kafka_schema/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/kafka_schema/#get_schema","title":"get_schema","text":"<pre><code>KafkaApi.get_schema(subject, version)\n</code></pre> <p>Get schema given subject name and version.</p> <p>Arguments</p> <ul> <li>subject <code>str</code>: subject name</li> <li>version <code>int</code>: version number</li> </ul> <p>Returns</p> <p><code>KafkaSchema</code>: KafkaSchema object</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the schema</li> </ul> <p>[source]</p>"},{"location":"generated/api/kafka_schema/#get_schemas","title":"get_schemas","text":"<pre><code>KafkaApi.get_schemas(subject)\n</code></pre> <p>Get all schema versions for the subject.</p> <p>Arguments</p> <ul> <li>subject <code>str</code>: subject name</li> </ul> <p>Returns</p> <p><code>List[KafkaSchema]</code>: List of KafkaSchema objects</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the schemas</li> </ul> <p>[source]</p>"},{"location":"generated/api/kafka_schema/#get_subjects","title":"get_subjects","text":"<pre><code>KafkaApi.get_subjects()\n</code></pre> <p>Get all subjects.</p> <p>Returns</p> <p><code>List[str]</code>: List of registered subjects</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the subjects</li> </ul>"},{"location":"generated/api/kafka_schema/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/kafka_schema/#id","title":"id","text":"<p>Id of the kafka schema</p> <p>[source]</p>"},{"location":"generated/api/kafka_schema/#schema","title":"schema","text":"<p>Schema definition</p> <p>[source]</p>"},{"location":"generated/api/kafka_schema/#subject","title":"subject","text":"<p>Name of the subject for the schema</p> <p>[source]</p>"},{"location":"generated/api/kafka_schema/#version","title":"version","text":"<p>Version of the schema</p>"},{"location":"generated/api/kafka_schema/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/kafka_schema/#delete","title":"delete","text":"<pre><code>KafkaSchema.delete()\n</code></pre> <p>Delete the schema</p> <p>Potentially dangerous operation</p> <p>This operation deletes the schema.</p> <p>Raises</p> <p><code>RestAPIError</code>.</p>"},{"location":"generated/api/kafka_topic/","title":"KafkaTopic API","text":""},{"location":"generated/api/kafka_topic/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/api/kafka_topic/#get_kafka_api","title":"get_kafka_api","text":"<pre><code>Project.get_kafka_api()\n</code></pre> <p>Get the kafka api for the project.</p> <p>Returns</p> <p><code>KafkaApi</code>: The Kafka Api handle</p>"},{"location":"generated/api/kafka_topic/#configuration","title":"Configuration","text":"<p>[source]</p>"},{"location":"generated/api/kafka_topic/#get_default_config","title":"get_default_config","text":"<pre><code>KafkaApi.get_default_config()\n</code></pre> <p>Get the configuration to set up a Producer or Consumer for a Kafka broker using confluent-kafka.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n\nkafka_conf = kafka_api.get_default_config()\n\nfrom confluent_kafka import Producer\n\nproducer = Producer(kafka_conf)\n</code></pre> Returns</p> <p><code>dict</code>: The kafka configuration</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the kafka configuration.</li> </ul>"},{"location":"generated/api/kafka_topic/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/kafka_topic/#create_topic","title":"create_topic","text":"<pre><code>KafkaApi.create_topic(name, schema, schema_version, replicas=1, partitions=1)\n</code></pre> <p>Create a new kafka topic.</p> <p><pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nkafka_api = project.get_kafka_api()\n\nkafka_topic = kafka_api.create_topic(\"my_topic\", \"my_schema\", 1)\n</code></pre> Arguments</p> <ul> <li>name <code>str</code>: name of the topic</li> <li>schema <code>str</code>: subject name of the schema</li> <li>schema_version <code>int</code>: version of the schema</li> <li>replicas <code>int</code>: replication factor for the topic</li> <li>partitions <code>int</code>: partitions for the topic</li> </ul> <p>Returns</p> <p><code>KafkaTopic</code>: The KafkaTopic object</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to create the topic</li> </ul>"},{"location":"generated/api/kafka_topic/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/kafka_topic/#get_topic","title":"get_topic","text":"<pre><code>KafkaApi.get_topic(name)\n</code></pre> <p>Get kafka topic by name.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: name of the topic</li> </ul> <p>Returns</p> <p><code>KafkaTopic</code>: The KafkaTopic object</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the topic</li> </ul> <p>[source]</p>"},{"location":"generated/api/kafka_topic/#get_topics","title":"get_topics","text":"<pre><code>KafkaApi.get_topics()\n</code></pre> <p>Get all kafka topics.</p> <p>Returns</p> <p><code>List[KafkaTopic]</code>: List of KafkaTopic objects</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the topics</li> </ul>"},{"location":"generated/api/kafka_topic/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/kafka_topic/#name","title":"name","text":"<p>Name of the topic</p> <p>[source]</p>"},{"location":"generated/api/kafka_topic/#partitions","title":"partitions","text":"<p>Number of partitions for the topic</p> <p>[source]</p>"},{"location":"generated/api/kafka_topic/#replicas","title":"replicas","text":"<p>Replication factor for the topic</p> <p>[source]</p>"},{"location":"generated/api/kafka_topic/#schema","title":"schema","text":"<p>Schema for the topic</p>"},{"location":"generated/api/kafka_topic/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/kafka_topic/#delete","title":"delete","text":"<pre><code>KafkaTopic.delete()\n</code></pre> <p>Delete the topic</p> <p>Potentially dangerous operation</p> <p>This operation deletes the topic.</p> <p>Raises</p> <p><code>RestAPIError</code>.</p>"},{"location":"generated/api/links/","title":"Provenance Links","text":"<p>Provenance Links are objects returned by methods such as get_feature_groups_provenance, get_storage_connector_provenance, get_parent_feature_group, get_generated_feature_groups, get_generated_feature_views get_models_provenance and represent sections of the provenance graph, depending on the method invoked.</p>"},{"location":"generated/api/links/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/links/#accessible","title":"accessible","text":"<p>List of [StorageConnectors|FeatureGroups|FeatureViews|Models] objects which are part of the provenance graph requested. These entities exist in the feature store/model registry and the user has access to them.</p> <p>[source]</p>"},{"location":"generated/api/links/#deleted","title":"deleted","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (storage connectors, feature groups, feature views, models) they represent. These entities have been removed from the feature store/model registry.</p> <p>[source]</p>"},{"location":"generated/api/links/#faulty","title":"faulty","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (storage connectors, feature groups, feature views, models) they represent. These entities exist in the feature store/model registry, however they are corrupted.</p> <p>[source]</p>"},{"location":"generated/api/links/#inaccessible","title":"inaccessible","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (storage connectors, feature groups, feature views, models) they represent. These entities exist in the feature store/model registry, however the user does not have access to them anymore.</p>"},{"location":"generated/api/links/#artifact","title":"Artifact","text":"<p>Artifacts objects are part of the provenance graph and contain a minimal set of information regarding the entities (feature groups, feature views) they represent. The provenance graph contains Artifact objects when the underlying entities have been deleted or they are corrupted or they are not accessible by the user.</p> <p>[source]</p>"},{"location":"generated/api/links/#feature_store_name","title":"feature_store_name","text":"<p>Name of the feature store in which the artifact is located.</p> <p>[source]</p>"},{"location":"generated/api/links/#name","title":"name","text":"<p>Name of the artifact.</p> <p>[source]</p>"},{"location":"generated/api/links/#version","title":"version","text":"<p>Version of the artifact</p>"},{"location":"generated/api/login/","title":"Login API","text":"<p>[source]</p>"},{"location":"generated/api/login/#login","title":"login","text":"<pre><code>hopsworks.login(\n    host=None,\n    port=443,\n    project=None,\n    api_key_value=None,\n    api_key_file=None,\n    hostname_verification=False,\n    trust_store_path=None,\n    engine=None,\n)\n</code></pre> <p>Connect to Serverless Hopsworks by calling the <code>hopsworks.login()</code> function with no arguments.</p> <p>Connect to Serverless</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n</code></pre> <p>Alternatively, connect to your own Hopsworks installation by specifying the host, port and api key.</p> <p>Connect to your Hopsworks cluster</p> <pre><code>import hopsworks\n\nproject = hopsworks.login(host=\"my.hopsworks.server\",\n                          port=8181,\n                          api_key_value=\"DKN8DndwaAjdf98FFNSxwdVKx\")\n</code></pre> <p>In addition to setting function arguments directly, <code>hopsworks.login()</code> also reads the environment variables: HOPSWORKS_HOST, HOPSWORKS_PORT, HOPSWORKS_PROJECT, HOPSWORKS_API_KEY, HOPSWORKS_HOSTNAME_VERIFICATION and HOPSWORKS_TRUST_STORE_PATH.</p> <p>The function arguments do however take precedence over the environment variables in case both are set.</p> <p>Arguments</p> <ul> <li>host <code>str | None</code>: The hostname of the Hopsworks instance, defaults to <code>None</code>.</li> <li>port <code>int</code>: The port on which the Hopsworks instance can be reached,     defaults to <code>443</code>.</li> <li>project <code>str | None</code>: Name of the project to access. If used inside a Hopsworks environment it always gets the current project. If not provided you will be prompted to enter it.</li> <li>api_key_value <code>str | None</code>: Value of the Api Key</li> <li>api_key_file <code>str | None</code>: Path to file wih Api Key</li> <li>hostname_verification <code>bool</code>: Whether to verify Hopsworks' certificate</li> <li>trust_store_path <code>str | None</code>: Path on the file system containing the Hopsworks certificates</li> <li>engine <code>None | Literal['spark'] | Literal['python'] | Literal['training']</code>: Which engine to use, <code>\"spark\"</code>, <code>\"python\"</code> or <code>\"training\"</code>. Defaults to <code>None</code>,     which initializes the engine to Spark if the environment provides Spark, for     example on Hopsworks and Databricks, or falls back to Python if Spark is not     available, e.g. on local Python environments or AWS SageMaker. This option     allows you to override this behaviour. <code>\"training\"</code> engine is useful when only     feature store metadata is needed, for example training dataset location and label     information when Hopsworks training experiment is conducted.</li> </ul> <p>Returns</p> <p><code>Project</code>: The Project object to perform operations on</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to connect to Hopsworks</li> <li><code>HopsworksSSLClientError</code>: If SSLError is raised from underlying requests library</li> </ul> <p>[source]</p>"},{"location":"generated/api/login/#get_current_project","title":"get_current_project","text":"<pre><code>hopsworks.get_current_project()\n</code></pre> <p>Get a reference to the current logged in project.</p> <p>Example for getting the project reference</p> <pre><code>import hopsworks\n\nhopsworks.login()\n\nproject = hopsworks.get_current_project()\n</code></pre> <p>Returns</p> <p><code>Project</code>. The Project object to perform operations on</p>"},{"location":"generated/api/login/#feature-store-api","title":"Feature Store API","text":"<p>[source]</p>"},{"location":"generated/api/login/#get_feature_store","title":"get_feature_store","text":"<pre><code>Project.get_feature_store(name=None)\n</code></pre> <p>Connect to Project's Feature Store.</p> <p>Defaulting to the project name of default feature store. To get a shared feature store, the project name of the feature store is required.</p> <p>Example for getting the Feature Store API of a project</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nfs = project.get_feature_store()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: Project name of the feature store.</li> </ul> <p>Returns</p> <p><code>hsfs.feature_store.FeatureStore</code>: The Feature Store API</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to connect</li> </ul>"},{"location":"generated/api/login/#model-registry-api","title":"Model Registry API","text":"<p>[source]</p>"},{"location":"generated/api/login/#get_model_registry","title":"get_model_registry","text":"<pre><code>Project.get_model_registry()\n</code></pre> <p>Connect to Project's Model Registry API.</p> <p>Example for getting the Model Registry API of a project</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nmr = project.get_model_registry()\n</code></pre> <p>Returns</p> <p><code>hsml.model_registry.ModelRegistry</code>: The Model Registry API</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to connect</li> </ul>"},{"location":"generated/api/login/#model-serving-api","title":"Model Serving API","text":"<p>[source]</p>"},{"location":"generated/api/login/#get_model_serving","title":"get_model_serving","text":"<pre><code>Project.get_model_serving()\n</code></pre> <p>Connect to Project's Model Serving API.</p> <p>Example for getting the Model Serving API of a project</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> <p>Returns</p> <p><code>hsml.model_serving.ModelServing</code>: The Model Serving API</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to connect</li> </ul>"},{"location":"generated/api/opensearch/","title":"OpenSearch API","text":""},{"location":"generated/api/opensearch/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/api/opensearch/#get_opensearch_api","title":"get_opensearch_api","text":"<pre><code>Project.get_opensearch_api()\n</code></pre> <p>Get the opensearch api for the project.</p> <p>Returns</p> <p><code>OpenSearchApi</code>: The OpenSearch Api handle</p>"},{"location":"generated/api/opensearch/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/opensearch/#get_default_py_config","title":"get_default_py_config","text":"<pre><code>OpenSearchApi.get_default_py_config()\n</code></pre> <p>Get the required opensearch configuration to setup a connection using the opensearch-py library.</p> <p><pre><code>import hopsworks\nfrom opensearchpy import OpenSearch\n\nproject = hopsworks.login()\n\nopensearch_api = project.get_opensearch_api()\n\nclient = OpenSearch(**opensearch_api.get_default_py_config())\n</code></pre> Returns:     A dictionary with required configuration.</p> <p>[source]</p>"},{"location":"generated/api/opensearch/#get_project_index","title":"get_project_index","text":"<pre><code>OpenSearchApi.get_project_index(index)\n</code></pre> <p>This helper method prefixes the supplied index name with the project name to avoid index name clashes.</p> <p>Args:     :index: the opensearch index to interact with.</p> <p>Returns:     A valid opensearch index name.</p>"},{"location":"generated/api/projects/","title":"Projects API","text":""},{"location":"generated/api/projects/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/projects/#create_project","title":"create_project","text":"<pre><code>hopsworks.create_project(name, description=None, feature_store_topic=None)\n</code></pre> <p>Create a new project.</p> <p>Not supported</p> <p>This is not supported if you are connected to Serverless Hopsworks</p> <p>Example for creating a new project</p> <pre><code>import hopsworks\n\nhopsworks.login(...)\n\nhopsworks.create_project(\"my_project\", description=\"An example Hopsworks project\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: The name of the project.</li> <li>description <code>str | None</code>: optional description of the project</li> <li>feature_store_topic <code>str | None</code>: optional feature store topic name</li> </ul> <p>Returns</p> <p><code>Project</code>. The Project object to perform operations on</p>"},{"location":"generated/api/projects/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/projects/#created","title":"created","text":"<p>Timestamp when the project was created</p> <p>[source]</p>"},{"location":"generated/api/projects/#description","title":"description","text":"<p>Description of the project</p> <p>[source]</p>"},{"location":"generated/api/projects/#id","title":"id","text":"<p>Id of the project</p> <p>[source]</p>"},{"location":"generated/api/projects/#name","title":"name","text":"<p>Name of the project</p> <p>[source]</p>"},{"location":"generated/api/projects/#owner","title":"owner","text":"<p>Owner of the project</p> <p>[source]</p>"},{"location":"generated/api/projects/#project_namespace","title":"project_namespace","text":"<p>Kubernetes namespace used by project</p>"},{"location":"generated/api/projects/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/projects/#get_dataset_api","title":"get_dataset_api","text":"<pre><code>Project.get_dataset_api()\n</code></pre> <p>Get the dataset api for the project.</p> <p>Returns</p> <p><code>DatasetApi</code>: The Datasets Api handle</p> <p>[source]</p>"},{"location":"generated/api/projects/#get_environment_api","title":"get_environment_api","text":"<pre><code>Project.get_environment_api()\n</code></pre> <p>Get the Python environment AP</p> <p>Returns</p> <p><code>EnvironmentApi</code>: The Python Environment Api handle</p> <p>[source]</p>"},{"location":"generated/api/projects/#get_feature_store","title":"get_feature_store","text":"<pre><code>Project.get_feature_store(name=None)\n</code></pre> <p>Connect to Project's Feature Store.</p> <p>Defaulting to the project name of default feature store. To get a shared feature store, the project name of the feature store is required.</p> <p>Example for getting the Feature Store API of a project</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nfs = project.get_feature_store()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: Project name of the feature store.</li> </ul> <p>Returns</p> <p><code>hsfs.feature_store.FeatureStore</code>: The Feature Store API</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to connect</li> </ul> <p>[source]</p>"},{"location":"generated/api/projects/#get_flink_cluster_api","title":"get_flink_cluster_api","text":"<pre><code>Project.get_flink_cluster_api()\n</code></pre> <p>Get the flink cluster API for the project.</p> <p>Returns</p> <p><code>FlinkClusterApi</code>: The Flink Cluster Api handle</p> <p>[source]</p>"},{"location":"generated/api/projects/#get_git_api","title":"get_git_api","text":"<pre><code>Project.get_git_api()\n</code></pre> <p>Get the git repository api for the project.</p> <p>Returns</p> <p><code>GitApi</code>: The Git Api handle</p> <p>[source]</p>"},{"location":"generated/api/projects/#get_job_api","title":"get_job_api","text":"<pre><code>Project.get_job_api()\n</code></pre> <p>Get the job API for the project.</p> <p>Returns</p> <p><code>JobApi</code>: The Job Api handle</p> <p>[source]</p>"},{"location":"generated/api/projects/#get_jobs_api","title":"get_jobs_api","text":"<pre><code>Project.get_jobs_api()\n</code></pre> <p>Deprecated, use get_job_api instead.</p> <p>[source]</p>"},{"location":"generated/api/projects/#get_kafka_api","title":"get_kafka_api","text":"<pre><code>Project.get_kafka_api()\n</code></pre> <p>Get the kafka api for the project.</p> <p>Returns</p> <p><code>KafkaApi</code>: The Kafka Api handle</p> <p>[source]</p>"},{"location":"generated/api/projects/#get_model_registry","title":"get_model_registry","text":"<pre><code>Project.get_model_registry()\n</code></pre> <p>Connect to Project's Model Registry API.</p> <p>Example for getting the Model Registry API of a project</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nmr = project.get_model_registry()\n</code></pre> <p>Returns</p> <p><code>hsml.model_registry.ModelRegistry</code>: The Model Registry API</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to connect</li> </ul> <p>[source]</p>"},{"location":"generated/api/projects/#get_model_serving","title":"get_model_serving","text":"<pre><code>Project.get_model_serving()\n</code></pre> <p>Connect to Project's Model Serving API.</p> <p>Example for getting the Model Serving API of a project</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> <p>Returns</p> <p><code>hsml.model_serving.ModelServing</code>: The Model Serving API</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to connect</li> </ul> <p>[source]</p>"},{"location":"generated/api/projects/#get_opensearch_api","title":"get_opensearch_api","text":"<pre><code>Project.get_opensearch_api()\n</code></pre> <p>Get the opensearch api for the project.</p> <p>Returns</p> <p><code>OpenSearchApi</code>: The OpenSearch Api handle</p> <p>[source]</p>"},{"location":"generated/api/projects/#get_url","title":"get_url","text":"<pre><code>Project.get_url()\n</code></pre>"},{"location":"generated/api/query_api/","title":"Query","text":"<p>Query objects are strictly generated by HSFS APIs called on Feature Group objects. Users will never construct a Query object using the constructor of the class. For this reason we do not provide the full documentation of the class here.</p>"},{"location":"generated/api/query_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/query_api/#append_feature","title":"append_feature","text":"<pre><code>Query.append_feature(feature)\n</code></pre> <p>Append a feature to the query.</p> <p>Arguments</p> <ul> <li>feature <code>str | hsfs.feature.Feature</code>: <code>[str, Feature]</code>. Name of the feature to append to the query.</li> </ul> <p>[source]</p>"},{"location":"generated/api/query_api/#as_of","title":"as_of","text":"<pre><code>Query.as_of(wallclock_time=None, exclude_until=None)\n</code></pre> <p>Perform time travel on the given Query.</p> <p>Pyspark/Spark Only</p> <p>Apache HUDI exclusively supports Time Travel and Incremental Query via Spark Context</p> <p>This method returns a new Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset.</p> <p>Reading features at a specific point in time:</p> <pre><code>fs = connection.get_feature_store();\nquery = fs.get_feature_group(\"example_feature_group\", 1).select_all()\nquery.as_of(\"2020-10-20 07:34:11\").read().show()\n</code></pre> <p>Reading commits incrementally between specified points in time:</p> <pre><code>fs = connection.get_feature_store();\nquery = fs.get_feature_group(\"example_feature_group\", 1).select_all()\nquery.as_of(\"2020-10-20 07:34:11\", exclude_until=\"2020-10-19 07:34:11\").read().show()\n</code></pre> <p>The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit.</p> <p>Reading only the changes from a single commit</p> <pre><code>fs = connection.get_feature_store();\nquery = fs.get_feature_group(\"example_feature_group\", 1).select_all()\nquery.as_of(\"2020-10-20 07:31:38\", exclude_until=\"2020-10-20 07:31:37\").read().show()\n</code></pre> <p>When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded.</p> <p>Reading the latest state of features, excluding commits before a specified point in time</p> <pre><code>fs = connection.get_feature_store();\nquery = fs.get_feature_group(\"example_feature_group\", 1).select_all()\nquery.as_of(None, exclude_until=\"2020-10-20 07:31:38\").read().show()\n</code></pre> <p>Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion: <pre><code>query1.as_of(..., ...)\n    .join(query2.as_of(..., ...))\n</code></pre></p> <p>If instead you apply another <code>as_of</code> selection after the join, all joined feature groups will be queried with this interval: <pre><code>query1.as_of(..., ...)  # as_of is not applied\n    .join(query2.as_of(..., ...))  # as_of is not applied\n    .as_of(..., ...)\n</code></pre></p> <p>Warning</p> <p>This function only works for queries on feature groups with time_travel_format='HUDI'.</p> <p>Warning</p> <p>Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: <code>hoodie.keep.min.commits</code> and <code>hoodie.keep.max.commits</code> when calling the <code>insert()</code> method.</p> <p>Arguments</p> <ul> <li>wallclock_time <code>str | int | datetime.datetime | datetime.date | None</code>: Read data as of this point in time.     Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, or <code>%Y-%m-%d %H:%M:%S</code>.</li> <li>exclude_until <code>str | int | datetime.datetime | datetime.date | None</code>: Exclude commits until this point in time. Strings should be formatted in one of the     following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, or <code>%Y-%m-%d %H:%M:%S</code>.</li> </ul> <p>Returns</p> <p><code>Query</code>. The query object with the applied time travel condition.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#filter","title":"filter","text":"<pre><code>Query.filter(f)\n</code></pre> <p>Apply filter to the feature group.</p> <p>Selects all features and returns the resulting <code>Query</code> with the applied filter.</p> <p>Example</p> <pre><code>from hsfs.feature import Feature\n\nquery.filter(Feature(\"weekly_sales\") &gt; 1000)\nquery.filter(Feature(\"name\").like(\"max%\"))\n</code></pre> <p>If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: <pre><code>query.filter(fg.feature1 == 1).show(10)\n</code></pre></p> <p>Composite filters require parenthesis and symbols for logical operands (e.g. <code>&amp;</code>, <code>|</code>, ...): <pre><code>query.filter((fg.feature1 == 1) | (fg.feature2 &gt;= 2))\n</code></pre></p> <p>Filters are fully compatible with joins</p> <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\nfg3 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all()\n    .join(fg2.select_all(), on=[\"date\", \"location_id\"])\n    .join(fg3.select_all(), left_on=[\"location_id\"], right_on=[\"id\"], join_type=\"left\")\n    .filter((fg1.location_id == 10) | (fg1.location_id == 20))\n</code></pre> <p>Filters can be applied at any point of the query</p> <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\nfg3 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all()\n    .join(fg2.select_all().filter(fg2.avg_temp &gt;= 22), on=[\"date\", \"location_id\"])\n    .join(fg3.select_all(), left_on=[\"location_id\"], right_on=[\"id\"], join_type=\"left\")\n    .filter(fg1.location_id == 10)\n</code></pre> <p>Arguments</p> <ul> <li>f <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic</code>: Filter object.</li> </ul> <p>Returns</p> <p><code>Query</code>. The query object with the applied filter.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#from_response_json","title":"from_response_json","text":"<pre><code>Query.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/query_api/#get_feature","title":"get_feature","text":"<pre><code>Query.get_feature(feature_name)\n</code></pre> <p>Get a feature by name.</p> <p>Arguments</p> <ul> <li>feature_name <code>str</code>: <code>str</code>. Name of the feature to get.</li> </ul> <p>Returns</p> <p><code>Feature</code>. Feature object.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#is_cache_feature_group_only","title":"is_cache_feature_group_only","text":"<pre><code>Query.is_cache_feature_group_only()\n</code></pre> <p>Query contains only cached feature groups</p> <p>[source]</p>"},{"location":"generated/api/query_api/#is_time_travel","title":"is_time_travel","text":"<pre><code>Query.is_time_travel()\n</code></pre> <p>Query contains time travel</p> <p>[source]</p>"},{"location":"generated/api/query_api/#join","title":"join","text":"<pre><code>Query.join(sub_query, on=None, left_on=None, right_on=None, join_type=\"left\", prefix=None)\n</code></pre> <p>Join Query with another Query.</p> <p>If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no nested joins.</p> <p>Join two feature groups</p> <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all().join(fg2.select_all())\n</code></pre> <p>More complex join</p> <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\nfg3 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all()\n        .join(fg2.select_all(), on=[\"date\", \"location_id\"])\n        .join(fg3.select_all(), left_on=[\"location_id\"], right_on=[\"id\"], join_type=\"left\")\n</code></pre> <p>Arguments</p> <ul> <li>sub_query <code>hsfs.constructor.query.Query</code>: Right-hand side query to join.</li> <li>on <code>List[str] | None</code>: List of feature names to join on if they are available in both     feature groups. Defaults to <code>[]</code>.</li> <li>left_on <code>List[str] | None</code>: List of feature names to join on from the left feature group of the     join. Defaults to <code>[]</code>.</li> <li>right_on <code>List[str] | None</code>: List of feature names to join on from the right feature group of     the join. Defaults to <code>[]</code>.</li> <li>join_type <code>str | None</code>: Type of join to perform, can be <code>\"inner\"</code>, <code>\"outer\"</code>, <code>\"left\"</code> or     <code>\"right\"</code>. Defaults to \"inner\".</li> <li>prefix <code>str | None</code>: User provided prefix to avoid feature name clash. Prefix is applied to the right     feature group of the query. Defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>Query</code>: A new Query object representing the join.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#pull_changes","title":"pull_changes","text":"<pre><code>Query.pull_changes(wallclock_start_time, wallclock_end_time)\n</code></pre> <p>Deprecated</p> <p><code>pull_changes</code> method is deprecated. Use `as_of(end_wallclock_time, exclude_until=start_wallclock_time) instead.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#read","title":"read","text":"<pre><code>Query.read(online=False, dataframe_type=\"default\", read_options=None)\n</code></pre> <p>Read the specified query into a DataFrame.</p> <p>It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists).</p> <p>External Feature Group Engine Support</p> <p>Spark only</p> <p>Reading a Query containing an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups.</p> <p>Arguments</p> <ul> <li>online <code>bool</code>: Read from online storage. Defaults to <code>False</code>.</li> <li>dataframe_type <code>str</code>: DataFrame type to return. Defaults to <code>\"default\"</code>.</li> <li>read_options <code>Dict[str, Any] | None</code>: Dictionary of read options for Spark in spark engine.     Only for python engine:<ul> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code> Defaults to <code>{}</code>.</li> </ul> </li> </ul> <p>Returns</p> <p><code>DataFrame</code>: DataFrame depending on the chosen type.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#show","title":"show","text":"<pre><code>Query.show(n, online=False)\n</code></pre> <p>Show the first N rows of the Query.</p> <p>Show the first 10 rows</p> <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all().join(fg2.select_all())\n\nquery.show(10)\n</code></pre> <p>Arguments</p> <ul> <li>n <code>int</code>: Number of rows to show.</li> <li>online <code>bool</code>: Show from online storage. Defaults to <code>False</code>.</li> </ul> <p>[source]</p>"},{"location":"generated/api/query_api/#to_string","title":"to_string","text":"<pre><code>Query.to_string(online=False, arrow_flight=False)\n</code></pre> <p>Example</p> <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all().join(fg2.select_all())\n\nquery.to_string()\n</code></pre>"},{"location":"generated/api/query_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/query_api/#featuregroups","title":"featuregroups","text":"<p>List of feature groups used in the query</p> <p>[source]</p>"},{"location":"generated/api/query_api/#features","title":"features","text":"<p>List of all features in the query</p> <p>[source]</p>"},{"location":"generated/api/query_api/#filters","title":"filters","text":"<p>All filters used in the query</p> <p>[source]</p>"},{"location":"generated/api/query_api/#joins","title":"joins","text":"<p>List of joins in the query</p> <p>[source]</p>"},{"location":"generated/api/query_api/#left_feature_group_end_time","title":"left_feature_group_end_time","text":"<p>End time of time travel for the left feature group.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#left_feature_group_start_time","title":"left_feature_group_start_time","text":"<p>Start time of time travel for the left feature group.</p>"},{"location":"generated/api/rule_api/","title":"Rule","text":"<p>{{rule}}</p>"},{"location":"generated/api/rule_api/#properties","title":"Properties","text":"<p>{{rule_properties}}</p>"},{"location":"generated/api/rule_definition_api/","title":"Rule Definition","text":"<p>{{ruledefinition}}</p>"},{"location":"generated/api/rule_definition_api/#properties","title":"Properties","text":"<p>{{ruledefinition_properties}}</p>"},{"location":"generated/api/rule_definition_api/#retrieval","title":"Retrieval","text":"<p>{{ruledefinition_getall}}</p> <p>{{ruledefinition_get}}</p>"},{"location":"generated/api/secrets/","title":"Secrets API","text":""},{"location":"generated/api/secrets/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/api/secrets/#get_secrets_api","title":"get_secrets_api","text":"<pre><code>hopsworks.get_secrets_api()\n</code></pre> <p>Get the secrets api.</p> <p>Returns</p> <p><code>SecretsApi</code>: The Secrets Api handle</p>"},{"location":"generated/api/secrets/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/secrets/#create_secret","title":"create_secret","text":"<pre><code>SecretsApi.create_secret(name, value, project=None)\n</code></pre> <p>Create a new secret.</p> <p><pre><code>import hopsworks\n\nconnection = hopsworks.connection()\n\nsecrets_api = connection.get_secrets_api()\n\nsecret = secrets_api.create_secret(\"my_secret\", \"Fk3MoPlQXCQvPo\")\n</code></pre> Arguments</p> <ul> <li>name <code>str</code>: Name of the secret.</li> <li>value <code>str</code>: The secret value.</li> <li>project <code>str | None</code>: Name of the project to share the secret with.</li> </ul> <p>Returns</p> <p><code>Secret</code>: The Secret object</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to create the secret</li> </ul>"},{"location":"generated/api/secrets/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/secrets/#get","title":"get","text":"<pre><code>SecretsApi.get(name, owner=None)\n</code></pre> <p>Get the secret's value. If the secret does not exist, it prompts the user to create the secret if the application is running interactively</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the secret.</li> <li>owner <code>str | None</code>: email of the owner for a secret shared with the current project.</li> </ul> <p>Returns</p> <p><code>str</code>: The secret value</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the secret</li> </ul> <p>[source]</p>"},{"location":"generated/api/secrets/#get_secret","title":"get_secret","text":"<pre><code>SecretsApi.get_secret(name, owner=None)\n</code></pre> <p>Get a secret.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the secret.</li> <li>owner <code>str | None</code>: username of the owner for a secret shared with the current project. Users can find their username in the Account Settings &gt; Profile section.</li> </ul> <p>Returns</p> <p><code>Secret</code>: The Secret object</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the secret</li> </ul> <p>[source]</p>"},{"location":"generated/api/secrets/#get_secrets","title":"get_secrets","text":"<pre><code>SecretsApi.get_secrets()\n</code></pre> <p>Get all secrets</p> <p>Returns</p> <p><code>List[Secret]</code>: List of all accessible secrets</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to get the secrets</li> </ul>"},{"location":"generated/api/secrets/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/secrets/#created","title":"created","text":"<p>Date when secret was created</p> <p>[source]</p>"},{"location":"generated/api/secrets/#name","title":"name","text":"<p>Name of the secret</p> <p>[source]</p>"},{"location":"generated/api/secrets/#owner","title":"owner","text":"<p>Owner of the secret</p> <p>[source]</p>"},{"location":"generated/api/secrets/#scope","title":"scope","text":"<p>Scope of the secret</p> <p>[source]</p>"},{"location":"generated/api/secrets/#value","title":"value","text":"<p>Value of the secret</p> <p>[source]</p>"},{"location":"generated/api/secrets/#visibility","title":"visibility","text":"<p>Visibility of the secret</p>"},{"location":"generated/api/secrets/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/secrets/#delete","title":"delete","text":"<pre><code>Secret.delete()\n</code></pre> <p>Delete the secret</p> <p>Potentially dangerous operation</p> <p>This operation deletes the secret and may break applications using it.</p> <p>Raises</p> <p><code>RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/secrets/#get_url","title":"get_url","text":"<pre><code>Secret.get_url()\n</code></pre>"},{"location":"generated/api/similarity_function_type_api/","title":"SimilarityFunctionType","text":"<p>[source]</p>"},{"location":"generated/api/similarity_function_type_api/#similarityfunctiontype_1","title":"SimilarityFunctionType","text":"<pre><code>hsfs.embedding.SimilarityFunctionType()\n</code></pre> <p>Enumeration class representing different types of similarity functions.</p> <p>Attributes</p> <ul> <li>L2 (str): Represents L2 norm similarity function.</li> <li>COSINE (str): Represents cosine similarity function.</li> <li>DOT_PRODUCT (str): Represents dot product similarity function.</li> </ul>"},{"location":"generated/api/spine_group_api/","title":"SpineGroup","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#spinegroup_1","title":"SpineGroup","text":"<pre><code>hsfs.feature_group.SpineGroup(\n    storage_connector=None,\n    query=None,\n    data_format=None,\n    path=None,\n    options=None,\n    name=None,\n    version=None,\n    description=None,\n    primary_key=None,\n    featurestore_id=None,\n    featurestore_name=None,\n    created=None,\n    creator=None,\n    id=None,\n    features=None,\n    location=None,\n    statistics_config=None,\n    event_time=None,\n    expectation_suite=None,\n    online_enabled=False,\n    href=None,\n    online_topic_name=None,\n    topic_name=None,\n    spine=True,\n    dataframe=None,\n    deprecated=False,\n    online_config=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/spine_group_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_or_create_spine_group","title":"get_or_create_spine_group","text":"<pre><code>FeatureStore.get_or_create_spine_group(\n    name,\n    version=None,\n    description=\"\",\n    primary_key=None,\n    event_time=None,\n    features=None,\n    dataframe=None,\n)\n</code></pre> <p>Create a spine group metadata object.</p> <p>Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nspine_df = pd.Dataframe()\n\nspine_group = fs.get_or_create_spine_group(\n                    name=\"sales\",\n                    version=1,\n                    description=\"Physical shop sales features\",\n                    primary_key=['ss_store_sk'],\n                    event_time='sale_date',\n                    dataframe=spine_df\n                    )\n</code></pre> <p>Note that you can inspect the dataframe in the spine group, or replace the dataframe:</p> <pre><code>spine_group.dataframe.show()\n\nspine_group.dataframe = new_df\n</code></pre> <p>The spine can then be used to construct queries, with only one speciality:</p> <p>Note</p> <p>Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against.</p> <p>If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving.</p> <p>These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again.</p> <p>For example, to generate training data:</p> <pre><code>X_train, X_test, y_train, y_test = feature_view_spine.train_test_split(0.2, spine=training_data_entities)\n</code></pre> <p>Or to get batches of fresh data for batch scoring: <pre><code>feature_view_spine.get_batch_data(spine=scoring_entities_df).show()\n</code></pre></p> <p>Here you have the chance to pass a different set of entities to generate the training dataset.</p> <p>Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column.</p> <pre><code>feature_view.get_batch_data(spine=spine_group)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the spine group to create.</li> <li>version <code>int | None</code>: Version of the spine group to retrieve, defaults to <code>None</code> and     will create the spine group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the spine group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     spine group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the spine group won't have any primary key.</li> <li>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this spine group. If event_time is set     the spine group can be used for point-in-time joins. Defaults to <code>None</code>.</li> <li> <p>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the spine group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame resulting by executing the provided query     against the data source.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ dataframe__: DataFrame, RDD, Ndarray, list. Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features.</p> </li> </ul> <p>Returns</p> <p><code>SpineGroup</code>. The spine group metadata object.</p>"},{"location":"generated/api/spine_group_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_or_create_spine_group_1","title":"get_or_create_spine_group","text":"<pre><code>FeatureStore.get_or_create_spine_group(\n    name,\n    version=None,\n    description=\"\",\n    primary_key=None,\n    event_time=None,\n    features=None,\n    dataframe=None,\n)\n</code></pre> <p>Create a spine group metadata object.</p> <p>Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nspine_df = pd.Dataframe()\n\nspine_group = fs.get_or_create_spine_group(\n                    name=\"sales\",\n                    version=1,\n                    description=\"Physical shop sales features\",\n                    primary_key=['ss_store_sk'],\n                    event_time='sale_date',\n                    dataframe=spine_df\n                    )\n</code></pre> <p>Note that you can inspect the dataframe in the spine group, or replace the dataframe:</p> <pre><code>spine_group.dataframe.show()\n\nspine_group.dataframe = new_df\n</code></pre> <p>The spine can then be used to construct queries, with only one speciality:</p> <p>Note</p> <p>Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against.</p> <p>If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving.</p> <p>These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again.</p> <p>For example, to generate training data:</p> <pre><code>X_train, X_test, y_train, y_test = feature_view_spine.train_test_split(0.2, spine=training_data_entities)\n</code></pre> <p>Or to get batches of fresh data for batch scoring: <pre><code>feature_view_spine.get_batch_data(spine=scoring_entities_df).show()\n</code></pre></p> <p>Here you have the chance to pass a different set of entities to generate the training dataset.</p> <p>Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column.</p> <pre><code>feature_view.get_batch_data(spine=spine_group)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the spine group to create.</li> <li>version <code>int | None</code>: Version of the spine group to retrieve, defaults to <code>None</code> and     will create the spine group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the spine group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     spine group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the spine group won't have any primary key.</li> <li>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this spine group. If event_time is set     the spine group can be used for point-in-time joins. Defaults to <code>None</code>.</li> <li> <p>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the spine group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame resulting by executing the provided query     against the data source.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ dataframe__: DataFrame, RDD, Ndarray, list. Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features.</p> </li> </ul> <p>Returns</p> <p><code>SpineGroup</code>. The spine group metadata object.</p>"},{"location":"generated/api/spine_group_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#avro_schema","title":"avro_schema","text":"<p>Avro schema representation of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#dataframe","title":"dataframe","text":"<p>Spine dataframe with primary key, event time and label column to use for point in time join when fetching features.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#deprecated","title":"deprecated","text":"<p>Setting if the feature group is deprecated.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#embedding_index","title":"embedding_index","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#event_time","title":"event_time","text":"<p>Event time feature in the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#feature_store","title":"feature_store","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#feature_store_id","title":"feature_store_id","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#features","title":"features","text":"<p>Feature Group schema (alias)</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#name","title":"name","text":"<p>Name of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#notification_topic_name","title":"notification_topic_name","text":"<p>The topic used for feature group notifications.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#path","title":"path","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#primary_key","title":"primary_key","text":"<p>List of features building the primary key.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#schema","title":"schema","text":"<p>Feature Group schema</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#storage_connector","title":"storage_connector","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#topic_name","title":"topic_name","text":"<p>The topic used for feature group data ingestion.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#version","title":"version","text":"<p>Version number of the feature group.</p>"},{"location":"generated/api/spine_group_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#add_tag","title":"add_tag","text":"<pre><code>SpineGroup.add_tag(name, value)\n</code></pre> <p>Attach a tag to a feature group.</p> <p>A tag consists of a  pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.add_tag(name=\"example_tag\", value=\"42\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value <code>Any</code>: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to add the tag.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#check_deprecated","title":"check_deprecated","text":"<pre><code>SpineGroup.check_deprecated()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#create_feature_monitoring","title":"create_feature_monitoring","text":"<pre><code>SpineGroup.create_feature_monitoring(\n    name,\n    feature_name,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # Data inserted last week on the same day\n    time_offset=\"1w1d\",\n    window_length=\"1d\",\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str</code>: Name of the feature to monitor.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#create_statistics_monitoring","title":"create_statistics_monitoring","text":"<pre><code>SpineGroup.create_statistics_monitoring(\n    name,\n    feature_name=None,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable statistics monitoring\nmy_config = fg.create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str | None</code>: Name of the feature to monitor. If not specified, statistics     will be computed for all features.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#delete","title":"delete","text":"<pre><code>SpineGroup.delete()\n</code></pre> <p>Drop the entire feature group along with its feature data.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(\n        name='bitcoin_price',\n        version=1\n        )\n\n# delete the feature group\nfg.delete()\n</code></pre> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#delete_tag","title":"delete_tag","text":"<pre><code>SpineGroup.delete_tag(name)\n</code></pre> <p>Delete a tag attached to a feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.delete_tag(\"example_tag\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#filter","title":"filter","text":"<pre><code>SpineGroup.filter(f)\n</code></pre> <p>Apply filter to the feature group.</p> <p>Selects all features and returns the resulting <code>Query</code> with the applied filter.</p> <p>Example</p> <pre><code>from hsfs.feature import Feature\n\n# connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.filter(Feature(\"weekly_sales\") &gt; 1000)\n</code></pre> <p>If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group:</p> <p>Example</p> <pre><code>fg.filter(fg.feature1 == 1).show(10)\n</code></pre> <p>Composite filters require parenthesis and symbols for logical operands (e.g. <code>&amp;</code>, <code>|</code>, ...):</p> <p>Example</p> <pre><code>fg.filter((fg.feature1 == 1) | (fg.feature2 &gt;= 2))\n</code></pre> <p>Arguments</p> <ul> <li>f <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic</code>: Filter object.</li> </ul> <p>Returns</p> <p><code>Query</code>. The query object with the applied filter.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_all_statistics","title":"get_all_statistics","text":"<pre><code>SpineGroup.get_all_statistics(computation_time=None, feature_names=None)\n</code></pre> <p>Returns all the statistics metadata computed before a specific time for the current feature group.</p> <p>If <code>computation_time</code> is <code>None</code>, all the statistics metadata are returned.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_statistics = fg.get_statistics(computation_time=None)\n</code></pre> <p>Arguments</p> <ul> <li>computation_time <code>str | int | float | datetime.datetime | datetime.date | None</code>: Date and time when statistics were computed. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. Statistics object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_complex_features","title":"get_complex_features","text":"<pre><code>SpineGroup.get_complex_features()\n</code></pre> <p>Returns the names of all features with a complex data type in this feature group.</p> <p>Example</p> <pre><code>complex_dtype_features = fg.get_complex_features()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_feature","title":"get_feature","text":"<pre><code>SpineGroup.get_feature(name)\n</code></pre> <p>Retrieve a <code>Feature</code> object from the schema of the feature group.</p> <p>There are several ways to access features of a feature group:</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# get Feature instanse\nfg.feature1\nfg[\"feature1\"]\nfg.get_feature(\"feature1\")\n</code></pre> <p>Note</p> <p>Attribute access to features works only for non-reserved names. For example features named <code>id</code> or <code>name</code> will not be accessible via <code>fg.name</code>, instead this will return the name of the feature group itself. Fall back on using the <code>get_feature</code> method.</p> <p>Arguments:</p> <p>name: The name of the feature to retrieve</p> <p>Returns:</p> <p>Feature: The feature object</p> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_feature_monitoring_configs","title":"get_feature_monitoring_configs","text":"<pre><code>SpineGroup.get_feature_monitoring_configs(name=None, feature_name=None, config_id=None)\n</code></pre> <p>Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch all feature monitoring configs attached to the feature group\nfm_configs = fg.get_feature_monitoring_configs()\n\n# fetch a single feature monitoring config by name\nfm_config = fg.get_feature_monitoring_configs(name=\"my_config\")\n\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fg.get_feature_monitoring_configs(feature_name=\"my_feature\")\n\n# fetch a single feature monitoring config with a given id\nfm_config = fg.get_feature_monitoring_configs(config_id=1)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: If provided fetch only the feature monitoring config with the given name.     Defaults to None.</li> <li>feature_name <code>str | None</code>: If provided, fetch only configs attached to a particular feature.     Defaults to None.</li> <li>config_id <code>int | None</code>: If provided, fetch only the feature monitoring config with the given id.     Defaults to None.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both name and feature_name are provided. - TypeError: if name or feature_name are not string or None.</p> <p>Return</p> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None]     A list of feature monitoring configs. If name provided,     returns either a single config or None if not found.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_feature_monitoring_history","title":"get_feature_monitoring_history","text":"<pre><code>SpineGroup.get_feature_monitoring_history(\n    config_name=None, config_id=None, start_time=None, end_time=None, with_statistics=True\n)\n</code></pre> <p>Fetch feature monitoring history for a given feature monitoring config.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch feature monitoring history for a given feature monitoring config\nfm_history = fg.get_feature_monitoring_history(\n    config_name=\"my_config\",\n    start_time=\"2020-01-01\",\n)\n\n# fetch feature monitoring history for a given feature monitoring config id\nfm_history = fg.get_feature_monitoring_history(\n    config_id=1,\n    start_time=datetime.now() - timedelta(weeks=2),\n    end_time=datetime.now() - timedelta(weeks=1),\n    with_statistics=False,\n)\n</code></pre> <p>Arguments</p> <ul> <li>config_name <code>str | None</code>: The name of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>config_id <code>int | None</code>: The id of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>start_time <code>int | str | datetime.datetime | datetime.date | None</code>: The start date of the feature monitoring history to fetch.     Defaults to None.</li> <li>end_time <code>int | str | datetime.datetime | datetime.date | None</code>: The end date of the feature monitoring history to fetch.     Defaults to None.</li> <li>with_statistics <code>bool | None</code>: Whether to include statistics in the feature monitoring history.     Defaults to True. If False, only metadata about the monitoring will be fetched.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both config_name and config_id are provided. - TypeError: if config_name or config_id are not respectively string, int or None.</p> <p>Return</p> <p>List[<code>FeatureMonitoringResult</code>]     A list of feature monitoring results containing the monitoring metadata     as well as the computed statistics for the detection and reference window     if requested.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_fg_name","title":"get_fg_name","text":"<pre><code>SpineGroup.get_fg_name()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_generated_feature_groups","title":"get_generated_feature_groups","text":"<pre><code>SpineGroup.get_generated_feature_groups()\n</code></pre> <p>Get the generated feature groups using this feature group, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_generated_feature_views","title":"get_generated_feature_views","text":"<pre><code>SpineGroup.get_generated_feature_views()\n</code></pre> <p>Get the generated feature view using this feature group, based on explicit provenance. These feature views can be accessible or inaccessible. Explicit provenance does not track deleted generated feature view links, so deleted will always be empty. For inaccessible feature views, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_parent_feature_groups","title":"get_parent_feature_groups","text":"<pre><code>SpineGroup.get_parent_feature_groups()\n</code></pre> <p>Get the parents of this feature group, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_storage_connector","title":"get_storage_connector","text":"<pre><code>SpineGroup.get_storage_connector()\n</code></pre> <p>Get the storage connector using this feature group, based on explicit provenance. Only the accessible storage connector is returned. For more items use the base method - get_storage_connector_provenance</p> <p>Returns</p> <p>`StorageConnector: Storage connector.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_storage_connector_provenance","title":"get_storage_connector_provenance","text":"<pre><code>SpineGroup.get_storage_connector_provenance()\n</code></pre> <p>Get the parents of this feature group, based on explicit provenance. Parents are storage connectors. These storage connector can be accessible, deleted or inaccessible. For deleted and inaccessible storage connector, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the storage connector used to generated this feature group</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_tag","title":"get_tag","text":"<pre><code>SpineGroup.get_tag(name)\n</code></pre> <p>Get the tags of a feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_tag_value = fg.get_tag(\"example_tag\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_tags","title":"get_tags","text":"<pre><code>SpineGroup.get_tags()\n</code></pre> <p>Retrieves all tags attached to a feature group.</p> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#json","title":"json","text":"<pre><code>SpineGroup.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#prepare_spark_location","title":"prepare_spark_location","text":"<pre><code>SpineGroup.prepare_spark_location()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#select","title":"select","text":"<pre><code>SpineGroup.select(features)\n</code></pre> <p>Select a subset of features of the feature group and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\n# construct query\nquery = fg.select([\"id\", \"f1\"])\nquery.features\n# [Feature('id', ...), Feature('f1', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>features <code>List[str | hsfs.feature.Feature]</code>: A list of <code>Feature</code> objects or feature names as     strings to be selected.</li> </ul> <p>Returns</p> <p><code>Query</code>: A query object with the selected features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#select_all","title":"select_all","text":"<pre><code>SpineGroup.select_all(include_primary_key=True, include_event_time=True)\n</code></pre> <p>Select all features along with primary key and event time from the feature group and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instances\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\n# construct the query\nquery = fg1.select_all().join(fg2.select_all())\n\n# show first 5 rows\nquery.show(5)\n\n\n# select all features exclude primary key and event time\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\nquery = fg.select_all()\nquery.features\n# [Feature('id', ...), Feature('ts', ...), Feature('f1', ...), Feature('f2', ...)]\n\nquery = fg.select_all(include_primary_key=False, include_event_time=False)\nquery.features\n# [Feature('f1', ...), Feature('f2', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>include_primary_key <code>bool | None</code>: If True, include primary key of the feature group     to the feature list. Defaults to True.</li> <li>include_event_time <code>bool | None</code>: If True, include event time of the feature group     to the feature list. Defaults to True.</li> </ul> <p>Returns</p> <p><code>Query</code>. A query object with all features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#select_except","title":"select_except","text":"<pre><code>SpineGroup.select_except(features=None)\n</code></pre> <p>Select all features including primary key and event time feature of the feature group except provided <code>features</code> and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\n# construct query\nquery = fg.select_except([\"ts\", \"f1\"])\nquery.features\n# [Feature('id', ...), Feature('f1', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>features <code>List[str | hsfs.feature.Feature] | None</code>: A list of <code>Feature</code> objects or feature names as     strings to be excluded from the selection. Defaults to [],     selecting all features.</li> </ul> <p>Returns</p> <p><code>Query</code>: A query object with the selected features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#select_features","title":"select_features","text":"<pre><code>SpineGroup.select_features()\n</code></pre> <p>Select all the features in the feature group and return a query object.</p> <p>Queries define the schema of Feature View objects which can be used to create Training Datasets, read from the Online Feature Store, and more. They can also be composed to create more complex queries using the <code>join</code> method.</p> <p>Info</p> <p>This method does not select the primary key and event time of the feature group. Use <code>select_all</code> to include them. Note that primary keys do not need to be included in the query to allow joining on them.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = hopsworks.login().get_feature_store()\n\n# Some dataframe to create the feature group with\n# both an event time and a primary key column\nmy_df.head()\n+------------+------------+------------+------------+\n|    id      | feature_1  |    ...     |    ts      |\n+------------+------------+------------+------------+\n|     8      |     8      |            |    15      |\n|     3      |     3      |    ...     |    6       |\n|     1      |     1      |            |    18      |\n+------------+------------+------------+------------+\n\n# Create the Feature Group instances\nfg1 = fs.create_feature_group(\n        name = \"fg1\",\n        version=1,\n        primary_key=[\"id\"],\n        event_time=\"ts\",\n    )\n\n# Insert data to the feature group.\nfg1.insert(my_df)\n\n# select all features from `fg1` excluding primary key and event time\nquery = fg1.select_features()\n\n# show first 3 rows\nquery.show(3)\n\n# Output, no id or ts columns\n+------------+------------+------------+\n| feature_1  | feature_2  | feature_3  |\n+------------+------------+------------+\n|     8      |     7      |    15      |\n|     3      |     1      |     6      |\n|     1      |     2      |    18      |\n+------------+------------+------------+\n</code></pre> <p>Example</p> <pre><code># connect to the Feature Store\nfs = hopsworks.login().get_feature_store()\n\n# Get the Feature Group from the previous example\nfg1 = fs.get_feature_group(\"fg1\", 1)\n\n# Some dataframe to create another feature group\n# with a primary key column\n+------------+------------+------------+\n|    id_2    | feature_6  | feature_7  |\n+------------+------------+------------+\n|     8      |     11     |            |\n|     3      |     4      |    ...     |\n|     1      |     9      |            |\n+------------+------------+------------+\n\n# join the two feature groups on their indexes, `id` and `id_2`\n# but does not include them in the query\nquery = fg1.select_features().join(fg2.select_features(), left_on=\"id\", right_on=\"id_2\")\n\n# show first 5 rows\nquery.show(3)\n\n# Output\n+------------+------------+------------+------------+------------+\n| feature_1  | feature_2  | feature_3  | feature_6  | feature_7  |\n+------------+------------+------------+------------+------------+\n|     8      |     7      |    15      |    11      |    15      |\n|     3      |     1      |     6      |     4      |     3      |\n|     1      |     2      |    18      |     9      |    20      |\n+------------+------------+------------+------------+------------+\n</code></pre> <p>Returns</p> <p><code>Query</code>. A query object with all features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#to_dict","title":"to_dict","text":"<pre><code>SpineGroup.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#update_deprecated","title":"update_deprecated","text":"<pre><code>SpineGroup.update_deprecated(deprecate=True)\n</code></pre> <p>Deprecate the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_deprecated(deprecate=True)\n</code></pre> <p>Safe update</p> <p>This method updates the feature group safely. In case of failure your local metadata object will be kept unchanged.</p> <p>Arguments</p> <ul> <li>deprecate <code>bool</code>: Boolean value identifying if the feature group should be deprecated. Defaults to True.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#update_description","title":"update_description","text":"<pre><code>SpineGroup.update_description(description)\n</code></pre> <p>Update the description of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_description(description=\"Much better description.\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature group description safely. In case of failure your local metadata object will keep the old description.</p> <p>Arguments</p> <ul> <li>description <code>str</code>: New description string.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#update_feature_description","title":"update_feature_description","text":"<pre><code>SpineGroup.update_feature_description(feature_name, description)\n</code></pre> <p>Update the description of a single feature in this feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_feature_description(feature_name=\"min_temp\",\n                              description=\"Much better feature description.\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature description safely. In case of failure your local metadata object will keep the old description.</p> <p>Arguments</p> <ul> <li>feature_name <code>str</code>: Name of the feature to be updated.</li> <li>description <code>str</code>: New description string.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#update_features","title":"update_features","text":"<pre><code>SpineGroup.update_features(features)\n</code></pre> <p>Update metadata of features in this feature group.</p> <p>Currently it's only supported to update the description of a feature.</p> <p>Unsafe update</p> <p>Note that if you use an existing <code>Feature</code> object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.feature.Feature | List[hsfs.feature.Feature]</code>: <code>Feature</code> or list of features. A feature object or list thereof to     be updated.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#update_notification_topic_name","title":"update_notification_topic_name","text":"<pre><code>SpineGroup.update_notification_topic_name(notification_topic_name)\n</code></pre> <p>Update the notification topic name of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_notification_topic_name(notification_topic_name=\"notification_topic_name\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature group notification topic name safely. In case of failure your local metadata object will keep the old notification topic name.</p> <p>Arguments</p> <ul> <li>notification_topic_name <code>str</code>: Name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If set to None no notifications are sent.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p>"},{"location":"generated/api/split_statistics_api/","title":"Split Statistics","text":"<p>[source]</p>"},{"location":"generated/api/split_statistics_api/#splitstatistics","title":"SplitStatistics","text":"<pre><code>hsfs.split_statistics.SplitStatistics(\n    name,\n    feature_descriptive_statistics,\n    href=None,\n    expand=None,\n    items=None,\n    count=None,\n    type=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/split_statistics_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/split_statistics_api/#feature_descriptive_statistics","title":"feature_descriptive_statistics","text":"<p>List of feature descriptive statistics.</p> <p>[source]</p>"},{"location":"generated/api/split_statistics_api/#name","title":"name","text":"<p>Name of the training dataset split.</p>"},{"location":"generated/api/statistics_api/","title":"Statistics","text":"<p>[source]</p>"},{"location":"generated/api/statistics_api/#statistics_1","title":"Statistics","text":"<pre><code>hsfs.statistics.Statistics(\n    computation_time,\n    row_percentage=1.0,\n    feature_descriptive_statistics=None,\n    feature_group_id=None,\n    window_start_commit_time=None,\n    window_end_commit_time=None,\n    feature_view_name=None,\n    feature_view_version=None,\n    training_dataset_version=None,\n    split_statistics=None,\n    before_transformation=False,\n    href=None,\n    expand=None,\n    items=None,\n    count=None,\n    type=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/statistics_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/statistics_api/#before_transformation","title":"before_transformation","text":"<p>Whether or not the statistics were computed on feature values before applying model-dependent transformations.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#computation_time","title":"computation_time","text":"<p>Time at which the statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#feature_descriptive_statistics","title":"feature_descriptive_statistics","text":"<p>List of feature descriptive statistics.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#feature_group_id","title":"feature_group_id","text":"<p>Id of the feature group on whose data the statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#feature_view_name","title":"feature_view_name","text":"<p>Name of the feature view whose query was used to retrieve the data on which the statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#feature_view_version","title":"feature_view_version","text":"<p>Id of the feature view whose query was used to retrieve the data on which the statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#row_percentage","title":"row_percentage","text":"<p>Percentage of data on which statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#split_statistics","title":"split_statistics","text":"<p>List of statistics computed on each split of a training dataset.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#training_dataset_version","title":"training_dataset_version","text":"<p>Version of the training dataset on which statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#window_end_commit_time","title":"window_end_commit_time","text":"<p>End time of the window of data on which statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#window_start_commit_time","title":"window_start_commit_time","text":"<p>Start time of the window of data on which statistics were computed.</p>"},{"location":"generated/api/statistics_config_api/","title":"StatisticsConfig","text":"<p>[source]</p>"},{"location":"generated/api/statistics_config_api/#statisticsconfig_1","title":"StatisticsConfig","text":"<pre><code>hsfs.statistics_config.StatisticsConfig(\n    enabled=True,\n    correlations=False,\n    histograms=False,\n    exact_uniqueness=False,\n    columns=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/statistics_config_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/statistics_config_api/#columns","title":"columns","text":"<p>Specify a subset of columns to compute statistics for.</p> <p>[source]</p>"},{"location":"generated/api/statistics_config_api/#correlations","title":"correlations","text":"<p>Enable correlations as an additional statistic to be computed for each feature pair.</p> <p>[source]</p>"},{"location":"generated/api/statistics_config_api/#enabled","title":"enabled","text":"<p>Enable statistics, by default this computes only descriptive statistics.</p> <p>[source]</p>"},{"location":"generated/api/statistics_config_api/#exact_uniqueness","title":"exact_uniqueness","text":"<p>Enable exact uniqueness as an additional statistic to be computed for each feature.</p> <p>[source]</p>"},{"location":"generated/api/statistics_config_api/#histograms","title":"histograms","text":"<p>Enable histograms as an additional statistic to be computed for each feature.</p>"},{"location":"generated/api/storage_connector_api/","title":"Storage Connector","text":""},{"location":"generated/api/storage_connector_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_storage_connector","title":"get_storage_connector","text":"<pre><code>FeatureStore.get_storage_connector(name)\n</code></pre> <p>Get a previously created storage connector from the feature store.</p> <p>Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS.</p> <p>If you want to connect to the online feature store, see the <code>get_online_storage_connector</code> method to get the JDBC connector for the Online Feature Store.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nsc = fs.get_storage_connector(\"demo_fs_meb10000_Training_Datasets\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the storage connector to retrieve.</li> </ul> <p>Returns</p> <p><code>StorageConnector</code>. Storage connector object.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_online_storage_connector","title":"get_online_storage_connector","text":"<pre><code>FeatureStore.get_online_storage_connector()\n</code></pre> <p>Get the storage connector for the Online Feature Store of the respective project's feature store.</p> <p>The returned storage connector depends on the project that you are connected to.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nonline_storage_connector = fs.get_online_storage_connector()\n</code></pre> <p>Returns</p> <p><code>StorageConnector</code>. JDBC storage connector to the Online Feature Store.</p>"},{"location":"generated/api/storage_connector_api/#hopsfs","title":"HopsFS","text":""},{"location":"generated/api/storage_connector_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name","title":"name","text":"<p>Name of the storage connector.</p>"},{"location":"generated/api/storage_connector_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options","title":"connector_options","text":"<pre><code>HopsFSConnector.connector_options()\n</code></pre> <p>Return prepared options to be passed to an external connector library. Not implemented for this connector type.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups","title":"get_feature_groups","text":"<pre><code>HopsFSConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance","title":"get_feature_groups_provenance","text":"<pre><code>HopsFSConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark","title":"prepare_spark","text":"<pre><code>HopsFSConnector.prepare_spark(path=None)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read","title":"read","text":"<pre><code>HopsFSConnector.read(\n    query=None, data_format=None, options=None, path=None, dataframe_type=\"default\"\n)\n</code></pre> <p>Reads a query or a path into a dataframe using the storage connector.</p> <p>Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake.</p> <p>Arguments</p> <ul> <li>query <code>str | None</code>: By default, the storage connector will read the table configured together     with the connector, if any. It's possible to overwrite this by passing a SQL     query here. Defaults to <code>None</code>.</li> <li>data_format <code>str | None</code>: When reading from object stores such as S3, HopsFS and ADLS, specify     the file format to be read, e.g. <code>csv</code>, <code>parquet</code>.</li> <li>options <code>Dict[str, Any] | None</code>: Any additional key/value options to be passed to the connector.</li> <li>path <code>str | None</code>: Path to be read from within the bucket of the storage connector. Not relevant     for JDBC or database based connectors such as Snowflake, JDBC or Redshift.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch","title":"refetch","text":"<pre><code>HopsFSConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options","title":"spark_options","text":"<pre><code>HopsFSConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict","title":"to_dict","text":"<pre><code>HopsFSConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json","title":"update_from_response_json","text":"<pre><code>HopsFSConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#jdbc","title":"JDBC","text":""},{"location":"generated/api/storage_connector_api/#properties_1","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#arguments","title":"arguments","text":"<p>Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the <code>driver</code> argument to <code>com.mysql.cj.jdbc.Driver</code> when creating the Storage Connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connection_string","title":"connection_string","text":"<p>JDBC connection string.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_1","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_1","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_1","title":"name","text":"<p>Name of the storage connector.</p>"},{"location":"generated/api/storage_connector_api/#methods_1","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_1","title":"connector_options","text":"<pre><code>JdbcConnector.connector_options()\n</code></pre> <p>Return prepared options to be passed to an external connector library. Not implemented for this connector type.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_1","title":"get_feature_groups","text":"<pre><code>JdbcConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_1","title":"get_feature_groups_provenance","text":"<pre><code>JdbcConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_1","title":"prepare_spark","text":"<pre><code>JdbcConnector.prepare_spark(path=None)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_1","title":"read","text":"<pre><code>JdbcConnector.read(query, data_format=None, options=None, path=None, dataframe_type=\"default\")\n</code></pre> <p>Reads a query into a dataframe using the storage connector.</p> <p>Arguments</p> <ul> <li>query <code>str</code>: A SQL query to be read.</li> <li>data_format <code>str | None</code>: Not relevant for JDBC based connectors.</li> <li>options <code>Dict[str, Any] | None</code>: Any additional key/value options to be passed to the JDBC connector.</li> <li>path <code>str | None</code>: Not relevant for JDBC based connectors.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_1","title":"refetch","text":"<pre><code>JdbcConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_1","title":"spark_options","text":"<pre><code>JdbcConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_1","title":"to_dict","text":"<pre><code>JdbcConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_1","title":"update_from_response_json","text":"<pre><code>JdbcConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#s3","title":"S3","text":""},{"location":"generated/api/storage_connector_api/#properties_2","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#access_key","title":"access_key","text":"<p>Access key.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#arguments_1","title":"arguments","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#bucket","title":"bucket","text":"<p>Return the bucket for S3 connectors.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_2","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#iam_role","title":"iam_role","text":"<p>IAM role.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_2","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_2","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#path","title":"path","text":"<p>If the connector refers to a path (e.g. S3) - return the path of the connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#region","title":"region","text":"<p>Return the region for S3 connectors.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#secret_key","title":"secret_key","text":"<p>Secret key.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#server_encryption_algorithm","title":"server_encryption_algorithm","text":"<p>Encryption algorithm if server-side S3 bucket encryption is enabled.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#server_encryption_key","title":"server_encryption_key","text":"<p>Encryption key if server-side S3 bucket encryption is enabled.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#session_token","title":"session_token","text":"<p>Session token.</p>"},{"location":"generated/api/storage_connector_api/#methods_2","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_2","title":"connector_options","text":"<pre><code>S3Connector.connector_options()\n</code></pre> <p>Return options to be passed to an external S3 connector library</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_2","title":"get_feature_groups","text":"<pre><code>S3Connector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_2","title":"get_feature_groups_provenance","text":"<pre><code>S3Connector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_2","title":"prepare_spark","text":"<pre><code>S3Connector.prepare_spark(path=None)\n</code></pre> <p>Prepare Spark to use this Storage Connector.</p> <pre><code>conn.prepare_spark()\n\nspark.read.format(\"json\").load(\"s3a://[bucket]/path\")\n\n# or\nspark.read.format(\"json\").load(conn.prepare_spark(\"s3a://[bucket]/path\"))\n</code></pre> <p>Arguments</p> <ul> <li>path <code>str | None</code>: Path to prepare for reading from cloud storage. Defaults to <code>None</code>.</li> </ul> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_2","title":"read","text":"<pre><code>S3Connector.read(query=None, data_format=None, options=None, path=\"\", dataframe_type=\"default\")\n</code></pre> <p>Reads a query or a path into a dataframe using the storage connector.</p> <p>Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake.</p> <p>Arguments</p> <ul> <li>query <code>str | None</code>: Not relevant for S3 connectors.</li> <li>data_format <code>str | None</code>: The file format of the files to be read, e.g. <code>csv</code>, <code>parquet</code>.</li> <li>options <code>Dict[str, Any] | None</code>: Any additional key/value options to be passed to the S3 connector.</li> <li>path <code>str</code>: Path within the bucket to be read.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_2","title":"refetch","text":"<pre><code>S3Connector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_2","title":"spark_options","text":"<pre><code>S3Connector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_2","title":"to_dict","text":"<pre><code>S3Connector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_2","title":"update_from_response_json","text":"<pre><code>S3Connector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#redshift","title":"Redshift","text":""},{"location":"generated/api/storage_connector_api/#properties_3","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#arguments_2","title":"arguments","text":"<p>Additional JDBC, REDSHIFT, or Snowflake arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#auto_create","title":"auto_create","text":"<p>Database username for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#cluster_identifier","title":"cluster_identifier","text":"<p>Cluster identifier for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_driver","title":"database_driver","text":"<p>Database endpoint for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_endpoint","title":"database_endpoint","text":"<p>Database endpoint for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_group","title":"database_group","text":"<p>Database username for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_name","title":"database_name","text":"<p>Database name for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_password","title":"database_password","text":"<p>Database password for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_port","title":"database_port","text":"<p>Database port for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_user_name","title":"database_user_name","text":"<p>Database username for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_3","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#expiration","title":"expiration","text":"<p>Cluster temporary credential expiration time.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#iam_role_1","title":"iam_role","text":"<p>IAM role.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_3","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_3","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#table_name","title":"table_name","text":"<p>Table name for redshift cluster.</p>"},{"location":"generated/api/storage_connector_api/#methods_3","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_3","title":"connector_options","text":"<pre><code>RedshiftConnector.connector_options()\n</code></pre> <p>Return prepared options to be passed to an external connector library. Not implemented for this connector type.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_3","title":"get_feature_groups","text":"<pre><code>RedshiftConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_3","title":"get_feature_groups_provenance","text":"<pre><code>RedshiftConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_3","title":"prepare_spark","text":"<pre><code>RedshiftConnector.prepare_spark(path=None)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_3","title":"read","text":"<pre><code>RedshiftConnector.read(\n    query=None, data_format=None, options=None, path=None, dataframe_type=\"default\"\n)\n</code></pre> <p>Reads a table or query into a dataframe using the storage connector.</p> <p>Arguments</p> <ul> <li>query <code>str | None</code>: By default, the storage connector will read the table configured together     with the connector, if any. It's possible to overwrite this by passing a SQL     query here. Defaults to <code>None</code>.</li> <li>data_format <code>str | None</code>: Not relevant for JDBC based connectors such as Redshift.</li> <li>options <code>Dict[str, Any] | None</code>: Any additional key/value options to be passed to the JDBC connector.</li> <li>path <code>str | None</code>: Not relevant for JDBC based connectors such as Redshift.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_3","title":"refetch","text":"<pre><code>RedshiftConnector.refetch()\n</code></pre> <p>Refetch storage connector in order to retrieve updated temporary credentials.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_3","title":"spark_options","text":"<pre><code>RedshiftConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_3","title":"to_dict","text":"<pre><code>RedshiftConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_3","title":"update_from_response_json","text":"<pre><code>RedshiftConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#azure-data-lake-storage","title":"Azure Data Lake Storage","text":""},{"location":"generated/api/storage_connector_api/#properties_4","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#account_name","title":"account_name","text":"<p>Account name of the ADLS storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#application_id","title":"application_id","text":"<p>Application ID of the ADLS storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#container_name","title":"container_name","text":"<p>Container name of the ADLS storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_4","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#directory_id","title":"directory_id","text":"<p>Directory ID of the ADLS storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#generation","title":"generation","text":"<p>Generation of the ADLS storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_4","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_4","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#path_1","title":"path","text":"<p>If the connector refers to a path (e.g. ADLS) - return the path of the connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#service_credential","title":"service_credential","text":"<p>Service credential of the ADLS storage connector</p>"},{"location":"generated/api/storage_connector_api/#methods_4","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_4","title":"connector_options","text":"<pre><code>AdlsConnector.connector_options()\n</code></pre> <p>Return prepared options to be passed to an external connector library. Not implemented for this connector type.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_4","title":"get_feature_groups","text":"<pre><code>AdlsConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_4","title":"get_feature_groups_provenance","text":"<pre><code>AdlsConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_4","title":"prepare_spark","text":"<pre><code>AdlsConnector.prepare_spark(path=None)\n</code></pre> <p>Prepare Spark to use this Storage Connector.</p> <pre><code>conn.prepare_spark()\n\nspark.read.format(\"json\").load(\"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\")\n\n# or\nspark.read.format(\"json\").load(conn.prepare_spark(\"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\"))\n</code></pre> <p>Arguments</p> <ul> <li>path <code>str | None</code>: Path to prepare for reading from cloud storage. Defaults to <code>None</code>.</li> </ul> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_4","title":"read","text":"<pre><code>AdlsConnector.read(\n    query=None, data_format=None, options=None, path=\"\", dataframe_type=\"default\"\n)\n</code></pre> <p>Reads a path into a dataframe using the storage connector. Arguments</p> <ul> <li>query <code>str | None</code>: Not relevant for ADLS connectors.</li> <li>data_format <code>str | None</code>: The file format of the files to be read, e.g. <code>csv</code>, <code>parquet</code>.</li> <li>options <code>Dict[str, Any] | None</code>: Any additional key/value options to be passed to the ADLS connector.</li> <li>path <code>str</code>: Path within the bucket to be read. For example, path=<code>path</code> will read directly from the container specified on connector by constructing the URI as 'abfss://[container-name]@[account_name].dfs.core.windows.net/[path]'. If no path is specified default container path will be used from connector.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_4","title":"refetch","text":"<pre><code>AdlsConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_4","title":"spark_options","text":"<pre><code>AdlsConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_4","title":"to_dict","text":"<pre><code>AdlsConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_4","title":"update_from_response_json","text":"<pre><code>AdlsConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#snowflake","title":"Snowflake","text":""},{"location":"generated/api/storage_connector_api/#properties_5","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#account","title":"account","text":"<p>Account of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#application","title":"application","text":"<p>Application of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database","title":"database","text":"<p>Database of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_5","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_5","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_5","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#options","title":"options","text":"<p>Additional options for the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#password","title":"password","text":"<p>Password of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#role","title":"role","text":"<p>Role of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#schema","title":"schema","text":"<p>Schema of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#table","title":"table","text":"<p>Table of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#token","title":"token","text":"<p>OAuth token of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#url","title":"url","text":"<p>URL of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#user","title":"user","text":"<p>User of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#warehouse","title":"warehouse","text":"<p>Warehouse of the Snowflake storage connector</p>"},{"location":"generated/api/storage_connector_api/#methods_5","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_5","title":"connector_options","text":"<pre><code>SnowflakeConnector.connector_options()\n</code></pre> <p>In order to use the <code>snowflake.connector</code> Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database.</p> <pre><code>import snowflake.connector\n\nsc = fs.get_storage_connector(\"snowflake_conn\")\nctx = snowflake.connector.connect(**sc.connector_options())\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_5","title":"get_feature_groups","text":"<pre><code>SnowflakeConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_5","title":"get_feature_groups_provenance","text":"<pre><code>SnowflakeConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_5","title":"prepare_spark","text":"<pre><code>SnowflakeConnector.prepare_spark(path=None)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_5","title":"read","text":"<pre><code>SnowflakeConnector.read(\n    query=None, data_format=None, options=None, path=None, dataframe_type=\"default\"\n)\n</code></pre> <p>Reads a table or query into a dataframe using the storage connector.</p> <p>Arguments</p> <ul> <li>query <code>str | None</code>: By default, the storage connector will read the table configured together     with the connector, if any. It's possible to overwrite this by passing a SQL     query here. Defaults to <code>None</code>.</li> <li>data_format <code>str | None</code>: Not relevant for Snowflake connectors.</li> <li>options <code>Dict[str, Any] | None</code>: Any additional key/value options to be passed to the engine.</li> <li>path <code>str | None</code>: Not relevant for Snowflake connectors.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_5","title":"refetch","text":"<pre><code>SnowflakeConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#snowflake_connector_options","title":"snowflake_connector_options","text":"<pre><code>SnowflakeConnector.snowflake_connector_options()\n</code></pre> <p>Alias for <code>connector_options</code></p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_5","title":"spark_options","text":"<pre><code>SnowflakeConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_5","title":"to_dict","text":"<pre><code>SnowflakeConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_5","title":"update_from_response_json","text":"<pre><code>SnowflakeConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#google-cloud-storage","title":"Google Cloud Storage","text":"<p>This storage connector provides integration to Google Cloud Storage (GCS). Once you create a connector in FeatureStore, you can transact data from a GCS bucket into a spark dataframe by calling the <code>read</code> API.</p> <p>Authentication to GCP is handled by uploading the <code>JSON keyfile for service account</code> to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation.</p> <p>The connector also supports the optional encryption method <code>Customer Supplied Encryption Key</code> by Google. The encryption details are stored as <code>Secrets</code> in the FeatureStore for keeping it secure. Read more about encryption on Google Documentation.</p> <p>The storage connector uses the Google <code>gcs-connector-hadoop</code> behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop</p>"},{"location":"generated/api/storage_connector_api/#properties_6","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#algorithm","title":"algorithm","text":"<p>Encryption Algorithm</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#bucket_1","title":"bucket","text":"<p>GCS Bucket</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_6","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#encryption_key","title":"encryption_key","text":"<p>Encryption Key</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#encryption_key_hash","title":"encryption_key_hash","text":"<p>Encryption Key Hash</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_6","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#key_path","title":"key_path","text":"<p>JSON keyfile for service account</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_6","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#path_2","title":"path","text":"<p>the path of the connector along with gs file system prefixed</p>"},{"location":"generated/api/storage_connector_api/#methods_6","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_6","title":"connector_options","text":"<pre><code>GcsConnector.connector_options()\n</code></pre> <p>Return prepared options to be passed to an external connector library. Not implemented for this connector type.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_6","title":"get_feature_groups","text":"<pre><code>GcsConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_6","title":"get_feature_groups_provenance","text":"<pre><code>GcsConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_6","title":"prepare_spark","text":"<pre><code>GcsConnector.prepare_spark(path=None)\n</code></pre> <p>Prepare Spark to use this Storage Connector.</p> <pre><code>conn.prepare_spark()\nspark.read.format(\"json\").load(\"gs://bucket/path\")\n# or\nspark.read.format(\"json\").load(conn.prepare_spark(\"gs://bucket/path\"))\n</code></pre> <p>Arguments</p> <ul> <li>path <code>str | None</code>: Path to prepare for reading from Google cloud storage. Defaults to <code>None</code>.</li> </ul> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_6","title":"read","text":"<pre><code>GcsConnector.read(\n    query=None, data_format=None, options=None, path=\"\", dataframe_type=\"default\"\n)\n</code></pre> <p>Reads GCS path into a dataframe using the storage connector.</p> <p>To read directly from the default bucket, you can omit the path argument: <pre><code>conn.read(data_format='spark_formats')\n</code></pre> Or to read objects from default bucket provide the object path without gsUtil URI schema. For example, following will read from a path gs://bucket_on_connector/Path/object : <pre><code>conn.read(data_format='spark_formats', paths='Path/object')\n</code></pre> Or to read with full gsUtil URI path, <pre><code>conn.read(data_format='spark_formats',path='gs://BUCKET/DATA')\n</code></pre> Arguments</p> <ul> <li>query <code>str | None</code>: Not relevant for GCS connectors.</li> <li>data_format <code>str | None</code>: Spark data format. Defaults to <code>None</code>.</li> <li>options <code>Dict[str, Any] | None</code>: Spark options. Defaults to <code>None</code>.</li> <li>path <code>str</code>: GCS path. Defaults to <code>None</code>.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Raises</p> <ul> <li><code>ValueError</code>: Malformed arguments.</li> </ul> <p>Returns</p> <p><code>Dataframe</code>: A Spark dataframe.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_6","title":"refetch","text":"<pre><code>GcsConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_6","title":"spark_options","text":"<pre><code>GcsConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_6","title":"to_dict","text":"<pre><code>GcsConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_6","title":"update_from_response_json","text":"<pre><code>GcsConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#bigquery","title":"BigQuery","text":"<p>The BigQuery storage connector provides integration to Google Cloud BigQuery. You can use it to run bigquery on your GCP cluster and load results into spark dataframe by calling the <code>read</code> API.</p> <p>Authentication to GCP is handled by uploading the <code>JSON keyfile for service account</code> to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation.</p> <p>The storage connector uses the Google <code>spark-bigquery-connector</code> behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery.</p>"},{"location":"generated/api/storage_connector_api/#properties_7","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#arguments_3","title":"arguments","text":"<p>Additional spark options</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#dataset","title":"dataset","text":"<p>BigQuery dataset (The dataset containing the table)</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_7","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_7","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#key_path_1","title":"key_path","text":"<p>JSON keyfile for service account</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#materialization_dataset","title":"materialization_dataset","text":"<p>BigQuery materialization dataset (The dataset where the materialized view is going to be created, used in case of query)</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_7","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#parent_project","title":"parent_project","text":"<p>BigQuery parent project (Google Cloud Project ID of the table to bill for the export)</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#query_project","title":"query_project","text":"<p>BigQuery project (The Google Cloud Project ID of the table)</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#query_table","title":"query_table","text":"<p>BigQuery table name</p>"},{"location":"generated/api/storage_connector_api/#methods_7","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_7","title":"connector_options","text":"<pre><code>BigQueryConnector.connector_options()\n</code></pre> <p>Return options to be passed to an external BigQuery connector library</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_7","title":"get_feature_groups","text":"<pre><code>BigQueryConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_7","title":"get_feature_groups_provenance","text":"<pre><code>BigQueryConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_7","title":"prepare_spark","text":"<pre><code>BigQueryConnector.prepare_spark(path=None)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_7","title":"read","text":"<pre><code>BigQueryConnector.read(\n    query=None, data_format=None, options=None, path=None, dataframe_type=\"default\"\n)\n</code></pre> <p>Reads results from BigQuery into a spark dataframe using the storage connector.</p> <p>Reading from bigquery is done via either specifying the BigQuery table or BigQuery query.   For example, to read from a BigQuery table, set the BigQuery project, dataset and table on storage connector   and read directly from the corresponding path.     <pre><code>conn.read()\n</code></pre>   OR, to read results from a BigQuery query, set <code>Materialization Dataset</code> on storage connector,    and pass your SQL to <code>query</code> argument.     <pre><code>conn.read(query='SQL')\n</code></pre>   Optionally, passing <code>query</code> argument will take priority at runtime if the table options were also set   on the storage connector. This allows user to run from both a query or table with same connector, assuming   all fields were set.   Also, user can set the <code>path</code> argument to a bigquery table path to read at runtime,    if table options were not set initially while creating the connector.     <pre><code>conn.read(path='project.dataset.table')\n</code></pre></p> <p>Arguments</p> <ul> <li>query <code>str | None</code>: BigQuery query. Defaults to <code>None</code>.</li> <li>data_format <code>str | None</code>: Spark data format. Defaults to <code>None</code>.</li> <li>options <code>Dict[str, Any] | None</code>: Spark options. Defaults to <code>None</code>.</li> <li>path <code>str | None</code>: BigQuery table path. Defaults to <code>None</code>.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Raises</p> <ul> <li><code>ValueError</code>: Malformed arguments.</li> </ul> <p>Returns</p> <p><code>Dataframe</code>: A Spark dataframe.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_7","title":"refetch","text":"<pre><code>BigQueryConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_7","title":"spark_options","text":"<pre><code>BigQueryConnector.spark_options()\n</code></pre> <p>Return spark options to be set for BigQuery spark connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_7","title":"to_dict","text":"<pre><code>BigQueryConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_7","title":"update_from_response_json","text":"<pre><code>BigQueryConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#kafka","title":"Kafka","text":""},{"location":"generated/api/storage_connector_api/#properties_8","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#bootstrap_servers","title":"bootstrap_servers","text":"<p>Bootstrap servers string.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_8","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_8","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_8","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#options_1","title":"options","text":"<p>Bootstrap servers string.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#security_protocol","title":"security_protocol","text":"<p>Bootstrap servers string.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#ssl_endpoint_identification_algorithm","title":"ssl_endpoint_identification_algorithm","text":"<p>Bootstrap servers string.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#ssl_keystore_location","title":"ssl_keystore_location","text":"<p>Bootstrap servers string.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#ssl_truststore_location","title":"ssl_truststore_location","text":"<p>Bootstrap servers string.</p>"},{"location":"generated/api/storage_connector_api/#methods_8","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#confluent_options","title":"confluent_options","text":"<pre><code>KafkaConnector.confluent_options()\n</code></pre> <p>Return prepared options to be passed to confluent_kafka, based on the provided apache spark configuration. Right now only producer values with Importance &gt;= medium are implemented. https://docs.confluent.io/platform/current/clients/librdkafka/html/md_CONFIGURATION.html</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_8","title":"connector_options","text":"<pre><code>KafkaConnector.connector_options()\n</code></pre> <p>Return prepared options to be passed to an external connector library. Not implemented for this connector type.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_8","title":"get_feature_groups","text":"<pre><code>KafkaConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_8","title":"get_feature_groups_provenance","text":"<pre><code>KafkaConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#kafka_options","title":"kafka_options","text":"<pre><code>KafkaConnector.kafka_options()\n</code></pre> <p>Return prepared options to be passed to kafka, based on the additional arguments. https://kafka.apache.org/documentation/</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_8","title":"prepare_spark","text":"<pre><code>KafkaConnector.prepare_spark(path=None)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_8","title":"read","text":"<pre><code>KafkaConnector.read(\n    query=None, data_format=None, options=None, path=None, dataframe_type=\"default\"\n)\n</code></pre> <p>NOT SUPPORTED.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_stream","title":"read_stream","text":"<pre><code>KafkaConnector.read_stream(\n    topic,\n    topic_pattern=False,\n    message_format=\"avro\",\n    schema=None,\n    options=None,\n    include_metadata=False,\n)\n</code></pre> <p>Reads a Kafka stream from a topic or multiple topics into a Dataframe.</p> <p>Engine Support</p> <p>Spark only</p> <p>Reading from data streams using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming.</p> <p>Arguments</p> <ul> <li>topic <code>str</code>: Name or pattern of the topic(s) to subscribe to.</li> <li>topic_pattern <code>bool</code>: Flag to indicate if <code>topic</code> string is a pattern.     Defaults to <code>False</code>.</li> <li>message_format <code>str</code>: The format of the messages to use for decoding.     Can be <code>\"avro\"</code> or <code>\"json\"</code>. Defaults to <code>\"avro\"</code>.</li> <li>schema <code>str | None</code>: Optional schema, to use for decoding, can be an Avro schema string for     <code>\"avro\"</code> message format, or for JSON encoding a Spark StructType schema,     or a DDL formatted string. Defaults to <code>None</code>.</li> <li>options <code>Dict[str, Any] | None</code>: Additional options as key/value string pairs to be passed to Spark.     Defaults to <code>{}</code>.</li> <li>include_metadata <code>bool</code>: Indicate whether to return additional metadata fields from     messages in the stream. Otherwise, only the decoded value fields are     returned. Defaults to <code>False</code>.</li> </ul> <p>Raises</p> <ul> <li><code>ValueError</code>: Malformed arguments.</li> </ul> <p>Returns</p> <p><code>StreamingDataframe</code>: A Spark streaming dataframe.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_8","title":"refetch","text":"<pre><code>KafkaConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_8","title":"spark_options","text":"<pre><code>KafkaConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments. This is done by just adding 'kafka.' prefix to kafka_options. https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#kafka-specific-configurations</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_8","title":"to_dict","text":"<pre><code>KafkaConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_8","title":"update_from_response_json","text":"<pre><code>KafkaConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/training_dataset_api/","title":"Training Dataset","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#trainingdataset","title":"TrainingDataset","text":"<pre><code>hsfs.training_dataset.TrainingDataset(\n    name,\n    version,\n    data_format,\n    featurestore_id,\n    location=\"\",\n    event_start_time=None,\n    event_end_time=None,\n    coalesce=False,\n    description=None,\n    storage_connector=None,\n    splits=None,\n    validation_size=None,\n    test_size=None,\n    train_start=None,\n    train_end=None,\n    validation_start=None,\n    validation_end=None,\n    test_start=None,\n    test_end=None,\n    seed=None,\n    created=None,\n    creator=None,\n    features=None,\n    statistics_config=None,\n    featurestore_name=None,\n    id=None,\n    inode_id=None,\n    training_dataset_type=None,\n    from_query=None,\n    querydto=None,\n    label=None,\n    train_split=None,\n    time_split_size=None,\n    extra_filter=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/training_dataset_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#create_training_dataset","title":"create_training_dataset","text":"<pre><code>FeatureStore.create_training_dataset(\n    name,\n    version=None,\n    description=\"\",\n    data_format=\"tfrecords\",\n    coalesce=False,\n    storage_connector=None,\n    splits=None,\n    location=\"\",\n    seed=None,\n    statistics_config=None,\n    label=None,\n    transformation_functions=None,\n    train_split=None,\n)\n</code></pre> <p>Create a training dataset metadata object.</p> <p>Deprecated</p> <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead. From version 3.0 training datasets created with this API are not visibile in the API anymore.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the <code>save()</code> method with a <code>DataFrame</code> or <code>Query</code>.</p> <p>Data Formats</p> <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the training dataset to create.</li> <li>version <code>int | None</code>: Version of the training dataset to retrieve, defaults to <code>None</code> and     will create the training dataset with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>data_format <code>str | None</code>: The data format used to save the training dataset,     defaults to <code>\"tfrecords\"</code>-format.</li> <li>coalesce <code>bool | None</code>: If true the training dataset data will be coalesced into     a single partition before writing. The resulting training dataset     will be a single file per split. Default False.</li> <li>storage_connector <code>hsfs.StorageConnector | None</code>: Storage connector defining the sink location for the     training dataset, defaults to <code>None</code>, and materializes training dataset     on HopsFS.</li> <li>splits <code>Dict[str, float] | None</code>: A dictionary defining training dataset splits to be created. Keys in     the dictionary define the name of the split as <code>str</code>, values represent     percentage of samples in the split as <code>float</code>. Currently, only random     splits are supported. Defaults to empty dict<code>{}</code>, creating only a single     training dataset without splits.</li> <li>location <code>str | None</code>: Path to complement the sink storage connector with, e.g if the     storage connector points to an S3 bucket, this path can be used to     define a sub-directory inside the bucket to place the training dataset.     Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the     storage connector.</li> <li>seed <code>int | None</code>: Optionally, define a seed to create the random splits with, in order     to guarantee reproducability, defaults to <code>None</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>label <code>List[str] | None</code>: A list of feature names constituting the prediction label/feature of     the training dataset. When replaying a <code>Query</code> during model inference,     the label features can be omitted from the feature vector retrieval.     Defaults to <code>[]</code>, no label.</li> <li>transformation_functions <code>Dict[str, hsfs.transformation_function.TransformationFunction] | None</code>: A dictionary mapping tansformation functions to     to the features they should be applied to before writing out the     training data and at inference time. Defaults to <code>{}</code>, no     transformations.</li> <li>train_split <code>str | None</code>: If <code>splits</code> is set, provide the name of the split that is going     to be used for training. The statistics of this split will be used for     transformation functions if necessary. Defaults to <code>None</code>.</li> </ul> <p>Returns:</p> <p><code>TrainingDataset</code>: The training dataset metadata object.</p>"},{"location":"generated/api/training_dataset_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#get_training_dataset","title":"get_training_dataset","text":"<pre><code>FeatureStore.get_training_dataset(name, version=None)\n</code></pre> <p>Get a training dataset entity from the feature store.</p> <p>Deprecated</p> <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version.</p> <p>It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects.</p> <p>Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the training dataset to get.</li> <li>version <code>int | None</code>: Version of the training dataset to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>TrainingDataset</code>: The training dataset metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve training dataset from the feature store.</li> </ul>"},{"location":"generated/api/training_dataset_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#coalesce","title":"coalesce","text":"<p>If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#data_format","title":"data_format","text":"<p>File format of the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#description","title":"description","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#event_end_time","title":"event_end_time","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#event_start_time","title":"event_start_time","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#extra_filter","title":"extra_filter","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#feature_store_id","title":"feature_store_id","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#feature_store_name","title":"feature_store_name","text":"<p>Name of the feature store in which the feature group is located.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#id","title":"id","text":"<p>Training dataset id.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#label","title":"label","text":"<p>The label/prediction feature of the training dataset.</p> <p>Can be a composite of multiple features.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#location","title":"location","text":"<p>Path to the training dataset location. Can be an empty string if e.g. the training dataset is in-memory.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#name","title":"name","text":"<p>Name of the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#query","title":"query","text":"<p>Query to generate this training dataset from online feature store.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#schema","title":"schema","text":"<p>Training dataset schema.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#seed","title":"seed","text":"<p>Seed used to perform random split, ensure reproducibility of the random split at a later date.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#serving_keys","title":"serving_keys","text":"<p>Set of primary key names that is used as keys in input dict object for <code>get_serving_vector</code> method.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#splits","title":"splits","text":"<p>Training dataset splits. <code>train</code>, <code>test</code> or <code>eval</code> and corresponding percentages.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#statistics","title":"statistics","text":"<p>Get computed statistics for the training dataset.</p> <p>Returns</p> <p><code>Statistics</code>. Object with statistics information.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#statistics_config","title":"statistics_config","text":"<p>Statistics configuration object defining the settings for statistics computation of the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#storage_connector","title":"storage_connector","text":"<p>Storage connector.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#test_end","title":"test_end","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#test_size","title":"test_size","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#test_start","title":"test_start","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#train_end","title":"train_end","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#train_split","title":"train_split","text":"<p>Set name of training dataset split that is used for training.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#train_start","title":"train_start","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#training_dataset_type","title":"training_dataset_type","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#validation_end","title":"validation_end","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#validation_size","title":"validation_size","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#validation_start","title":"validation_start","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#version","title":"version","text":"<p>Version number of the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#write_options","title":"write_options","text":"<p>User provided options to write training dataset.</p>"},{"location":"generated/api/training_dataset_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#add_tag","title":"add_tag","text":"<pre><code>TrainingDataset.add_tag(name, value)\n</code></pre> <p>Attach a tag to a training dataset.</p> <p>A tag consists of a  pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to add the tag.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#compute_statistics","title":"compute_statistics","text":"<pre><code>TrainingDataset.compute_statistics()\n</code></pre> <p>Compute the statistics for the training dataset and save them to the feature store.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#delete","title":"delete","text":"<pre><code>TrainingDataset.delete()\n</code></pre> <p>Delete training dataset and all associated metadata.</p> <p>Drops only HopsFS data</p> <p>Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store.</p> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#delete_tag","title":"delete_tag","text":"<pre><code>TrainingDataset.delete_tag(name)\n</code></pre> <p>Delete a tag attached to a training dataset.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#from_response_json","title":"from_response_json","text":"<pre><code>TrainingDataset.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#from_response_json_single","title":"from_response_json_single","text":"<pre><code>TrainingDataset.from_response_json_single(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#get_query","title":"get_query","text":"<pre><code>TrainingDataset.get_query(online=True, with_label=False)\n</code></pre> <p>Returns the query used to generate this training dataset</p> <p>Arguments</p> <ul> <li>online <code>bool</code>: boolean, optional. Return the query for the online storage, else     for offline storage, defaults to <code>True</code> - for online storage.</li> <li>with_label <code>bool</code>: Indicator whether the query should contain features which were     marked as prediction label/feature when the training dataset was     created, defaults to <code>False</code>.</li> </ul> <p>Returns</p> <p><code>str</code>. Query string for the chosen storage used to generate this training     dataset.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#get_serving_vector","title":"get_serving_vector","text":"<pre><code>TrainingDataset.get_serving_vector(entry, external=None)\n</code></pre> <p>Returns assembled serving vector from online feature store.</p> <p>Arguments</p> <ul> <li>entry <code>Dict[str, Any]</code>: dictionary of training dataset feature group primary key names as keys and values provided by     serving application.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hopsworks.login()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> </ul> <p>Returns</p> <p><code>list</code> List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#get_serving_vectors","title":"get_serving_vectors","text":"<pre><code>TrainingDataset.get_serving_vectors(entry, external=None)\n</code></pre> <p>Returns assembled serving vectors in batches from online feature store.</p> <p>Arguments</p> <ul> <li>entry <code>Dict[str, List[Any]]</code>: dict of feature group primary key names as keys and value as list of primary keys provided by     serving application.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hopsworks.login()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> </ul> <p>Returns</p> <p><code>List[list]</code> List of lists of feature values related to provided primary keys, ordered according to positions of this features in training dataset query.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#get_tag","title":"get_tag","text":"<pre><code>TrainingDataset.get_tag(name)\n</code></pre> <p>Get the tags of a training dataset.</p> <p>Arguments</p> <ul> <li>name: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#get_tags","title":"get_tags","text":"<pre><code>TrainingDataset.get_tags()\n</code></pre> <p>Returns all tags attached to a training dataset.</p> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#init_prepared_statement","title":"init_prepared_statement","text":"<pre><code>TrainingDataset.init_prepared_statement(batch=None, external=None)\n</code></pre> <p>Initialise and cache parametrized prepared statement to    retrieve feature vector from online feature store.</p> <p>Arguments</p> <ul> <li>batch <code>bool | None</code>: boolean, optional. If set to True, prepared statements will be     initialised for retrieving serving vectors as a batch.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hopsworks.login()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> </ul> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#insert","title":"insert","text":"<pre><code>TrainingDataset.insert(features, overwrite, write_options=None)\n</code></pre> <p>Insert additional feature data into the training dataset.</p> <p>Deprecated</p> <p><code>insert</code> method is deprecated.</p> <p>This method appends data to the training dataset either from a Feature Store <code>Query</code>, a Spark or Pandas <code>DataFrame</code>, a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation.</p> <p>This can also be used to overwrite all data in an existing training dataset.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.constructor.query.Query | pandas.DataFrame | hsfs.training_dataset.pyspark.sql.DataFrame | hsfs.training_dataset.pyspark.RDD | numpy.ndarray | List[list]</code>: Feature data to be materialized.</li> <li>overwrite <code>bool</code>: Whether to overwrite the entire data in the training dataset.</li> <li>write_options <code>Dict[Any, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the insert call should return only   after the Hopsworks Job has finished. By default it waits.</li> </ul> </li> </ul> <p>Returns</p> <p><code>Job</code>: When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to create the training dataset.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: Unable to create training dataset metadata.</li> </ul> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#json","title":"json","text":"<pre><code>TrainingDataset.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#read","title":"read","text":"<pre><code>TrainingDataset.read(split=None, read_options=None)\n</code></pre> <p>Read the training dataset into a dataframe.</p> <p>It is also possible to read only a specific split.</p> <p>Arguments</p> <ul> <li>split: Name of the split to read, defaults to <code>None</code>, reading the entire     training dataset. If the training dataset has split, the <code>split</code> parameter     is mandatory.</li> <li>read_options: Additional read options as key/value pairs, defaults to <code>{}</code>.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>: The spark dataframe containing the feature data of the     training dataset.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#save","title":"save","text":"<pre><code>TrainingDataset.save(features, write_options=None)\n</code></pre> <p>Materialize the training dataset to storage.</p> <p>This method materializes the training dataset either from a Feature Store <code>Query</code>, a Spark or Pandas <code>DataFrame</code>, a Spark RDD, two-dimensional Python lists or Numpy ndarrays. From v2.5 onward, filters are saved along with the <code>Query</code>.</p> <p>Engine Support</p> <p>Creating Training Datasets from Dataframes is only supported using Spark as Engine.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.constructor.query.Query | pandas.DataFrame | hsfs.training_dataset.pyspark.sql.DataFrame | hsfs.training_dataset.pyspark.RDD | numpy.ndarray | List[list]</code>: Feature data to be materialized.</li> <li>write_options <code>Dict[Any, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits.</li> </ul> </li> </ul> <p>Returns</p> <p><code>Job</code>: When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to create the training dataset.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: Unable to create training dataset metadata.</li> </ul> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#show","title":"show","text":"<pre><code>TrainingDataset.show(n, split=None)\n</code></pre> <p>Show the first <code>n</code> rows of the training dataset.</p> <p>You can specify a split from which to retrieve the rows.</p> <p>Arguments</p> <ul> <li>n <code>int</code>: Number of rows to show.</li> <li>split <code>str | None</code>: Name of the split to show, defaults to <code>None</code>, showing the first rows     when taking all splits together.</li> </ul> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#to_dict","title":"to_dict","text":"<pre><code>TrainingDataset.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#update_from_response_json","title":"update_from_response_json","text":"<pre><code>TrainingDataset.update_from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#update_statistics_config","title":"update_statistics_config","text":"<pre><code>TrainingDataset.update_statistics_config()\n</code></pre> <p>Update the statistics configuration of the training dataset.</p> <p>Change the <code>statistics_config</code> object and persist the changes by calling this method.</p> <p>Returns</p> <p><code>TrainingDataset</code>. The updated metadata object of the training dataset.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p>"},{"location":"generated/api/transformation_functions_api/","title":"Transformation Function","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#transformationfunction","title":"TransformationFunction","text":"<pre><code>hsfs.transformation_function.TransformationFunction(\n    featurestore_id,\n    hopsworks_udf,\n    version=None,\n    id=None,\n    transformation_type=None,\n    type=None,\n    items=None,\n    count=None,\n    href=None,\n    **kwargs\n)\n</code></pre> <p>DTO class for transformation functions.</p> <p>Arguments</p> <ul> <li>featurestore_id : <code>int</code>. Id of the feature store in which the transformation function is saved.</li> <li>hopsworks_udf : <code>HopsworksUDF</code>. The meta data object for UDF in Hopsworks, which can be created using the <code>@udf</code> decorator.</li> <li>version : <code>int</code>. The version of the transformation function.</li> <li>id : <code>int</code>. The id of the transformation function in the feature store.</li> <li>transformation_type : <code>UDFType</code>. The type of the transformation function. Can be \"on-demand\" or \"model-dependent\"</li> </ul>"},{"location":"generated/api/transformation_functions_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#hopsworks_udf","title":"hopsworks_udf","text":"<p>Meta data class for the user defined transformation function.</p> <p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#id","title":"id","text":"<p>Transformation function id.</p> <p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#output_column_names","title":"output_column_names","text":"<p>Names of the output columns generated by the transformation functions</p> <p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#transformation_statistics","title":"transformation_statistics","text":"<p>Feature statistics required for the defined UDF</p> <p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#transformation_type","title":"transformation_type","text":"<p>Type of the Transformation : Can be \"model dependent\" or \"on-demand\" </p> <p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#version","title":"version","text":"<p>Version of the transformation function.</p>"},{"location":"generated/api/transformation_functions_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#delete","title":"delete","text":"<pre><code>TransformationFunction.delete()\n</code></pre> <p>Delete transformation function from backend.</p> <p>Example</p> <pre><code># import hopsworks udf decorator\nfrom hopworks import udf\n\n# define function\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n# create transformation function\nplus_one_meta = fs.create_transformation_function(\n        transformation_function=plus_one,\n        version=1\n    )\n# persist transformation function in backend\nplus_one_meta.save()\n\n# retrieve transformation function\nplus_one_fn = fs.get_transformation_function(name=\"plus_one\")\n\n# delete transformation function from backend\nplus_one_fn.delete()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#save","title":"save","text":"<pre><code>TransformationFunction.save()\n</code></pre> <p>Save a transformation function into the backend.</p> <p>Example</p> <pre><code># import hopsworks udf decorator\nfrom hopworks import udf\n\n# define function\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n# create transformation function\nplus_one_meta = fs.create_transformation_function(\n        transformation_function=plus_one,\n        version=1\n    )\n\n# persist transformation function in backend\nplus_one_meta.save()\n</code></pre>"},{"location":"generated/api/transformation_functions_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#create_transformation_function","title":"create_transformation_function","text":"<pre><code>FeatureStore.create_transformation_function(transformation_function, version=None)\n</code></pre> <p>Create a transformation function metadata object.</p> <p>Example</p> <pre><code># define the transformation function as a Hopsworks's UDF\n@udf(int)\ndef plus_one(value):\n    return value + 1\n\n# create transformation function\nplus_one_meta = fs.create_transformation_function(\n        transformation_function=plus_one,\n        version=1\n    )\n\n# persist transformation function in backend\nplus_one_meta.save()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the <code>save()</code> method of the transformation function metadata object.</p> <p>Arguments</p> <ul> <li>transformation_function <code>hsfs.hopsworks_udf.HopsworksUdf</code>: Hopsworks UDF.</li> </ul> <p>Returns:</p> <p><code>TransformationFunction</code>: The TransformationFunction metadata object.</p>"},{"location":"generated/api/transformation_functions_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#get_transformation_function","title":"get_transformation_function","text":"<pre><code>FeatureStore.get_transformation_function(name, version=None)\n</code></pre> <p>Get  transformation function metadata object.</p> <p>Get transformation function by name. This will default to version 1</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nplus_one_fn = fs.get_transformation_function(name=\"plus_one\")\n</code></pre> <p>Get built-in transformation function min max scaler</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nmin_max_scaler_fn = fs.get_transformation_function(name=\"min_max_scaler\")\n</code></pre> <p>Get transformation function by name and version</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\", version=2)\n</code></pre> <p>You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function instance. Then the transformation functions are applied when you read training data, get batch data, or get feature vector(s).</p> <p>Attach transformation functions to the feature view</p> <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# get transformation function metadata object\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\", version=1)\n\n# attach transformation functions\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    query=query,\n    labels=[\"target_column\"],\n    transformation_functions=[min_max_scaler(\"feature1\")]\n)\n</code></pre> <p>Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for <code>min_max_scaler</code>; mean and standard deviation for <code>standard_scaler</code> etc.</p> <p>Attach built-in transformation functions to the feature view</p> <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# retrieve transformation functions\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\nstandard_scaler = fs.get_transformation_function(name=\"standard_scaler\")\nrobust_scaler = fs.get_transformation_function(name=\"robust_scaler\")\nlabel_encoder = fs.get_transformation_function(name=\"label_encoder\")\n\n# attach built-in transformation functions while creating feature view\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions = [\n        label_encoder(\"category_column\"),\n        robust_scaler(\"weight\"),\n        min_max_scaler(\"age\"),\n        standard_scaler(\"salary\")\n    ]\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: name of transformation function.</li> <li>version <code>int | None</code>: version of transformation function. Optional, if not provided all functions that match to provided     name will be retrieved.</li> </ul> <p>Returns:</p> <p><code>TransformationFunction</code>: The TransformationFunction metadata object.</p> <p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#get_transformation_functions","title":"get_transformation_functions","text":"<pre><code>FeatureStore.get_transformation_functions()\n</code></pre> <p>Get  all transformation functions metadata objects.</p> <p>Get all transformation functions</p> <pre><code># get feature store instance\nfs = ...\n\n# get all transformation functions\nlist_transformation_fns = fs.get_transformation_functions()\n</code></pre> <p>Returns:</p> <p><code>List[TransformationFunction]</code>. List of transformation function instances.</p>"},{"location":"generated/api/transformation_statistics/","title":"Transformation Statistics","text":"<p>[source]</p>"},{"location":"generated/api/transformation_statistics/#transformationstatistics","title":"TransformationStatistics","text":"<pre><code>hsfs.transformation_statistics.TransformationStatistics(*features)\n</code></pre> <p>Class that stores feature transformation statistics of all features that require training dataset statistics in a transformation function.</p> <p>All statistics for a feature is initially initialized with null values and will be populated with values when training dataset is created for the soe.</p> <p>Arguments</p> <ul> <li>*features: <code>str</code>.     The features for which training dataset statistics need to be computed.</li> </ul> <p>Example</p> <pre><code># Defining transformation statistics\ntransformation_statistics = TransformationStatistics(\"feature1\", \"feature2\")\n\n# Accessing feature transformation statistics for a specific feature\nfeature_transformation_statistics_feature1 = transformation_statistics.feature1\n</code></pre>"},{"location":"generated/api/udf/","title":"Transformation Functions API","text":"<p>[source]</p>"},{"location":"generated/api/udf/#udf","title":"udf","text":"<pre><code>hopsworks.udf(return_type, drop=None, mode=\"default\")\n</code></pre> <p>Create an User Defined Function that can be and used within the Hopsworks Feature Store to create transformation functions.</p> <p>Hopsworks UDF's are user defined functions that executes as 'pandas_udf' when executing in spark engine and as pandas functions in the python engine. The pandas udf/pandas functions gets as inputs pandas Series's and can provide as output a pandas Series or a pandas DataFrame. A Hopsworks udf is defined using the <code>hopsworks_udf</code> decorator. The outputs of the defined UDF must be mentioned in the decorator as a list of python types.</p> <p>Example</p> <pre><code>from hopsworks import udf\n\n@udf(float)\ndef add_one(data1):\n    return data1 + 1\n</code></pre> <p>Arguments</p> <ul> <li>return_type <code>List[type] | type</code>: The output types of the defined UDF</li> <li>drop <code>str | List[str] | None</code>: The features to be dropped after application of transformation functions</li> </ul> <p>Returns</p> <p><code>HopsworksUdf</code>: The metadata object for hopsworks UDF's.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.FeatureStoreException</code> : If unable to create UDF.</li> </ul>"},{"location":"generated/api/validation_api/","title":"Validation","text":"<p>{{validation_result}}</p>"},{"location":"generated/api/validation_api/#properties","title":"Properties","text":"<p>{{validation_result_properties}}</p>"},{"location":"generated/api/validation_api/#methods","title":"Methods","text":"<p>{{expectation_methods}}</p>"},{"location":"generated/api/validation_api/#validate-a-dataframe","title":"Validate a dataframe","text":"<p>{{validate}}</p>"},{"location":"generated/api/validation_api/#retrieval","title":"Retrieval","text":"<p>{{validation_result_get}}</p>"},{"location":"generated/api/validation_report_api/","title":"Validation Report","text":"<p>[source]</p>"},{"location":"generated/api/validation_report_api/#validationreport","title":"ValidationReport","text":"<pre><code>hsfs.validation_report.ValidationReport(\n    success,\n    results,\n    meta,\n    statistics,\n    evaluation_parameters=None,\n    id=None,\n    full_report_path=None,\n    featurestore_id=None,\n    featuregroup_id=None,\n    validation_time=None,\n    ingestion_result=\"unknown\",\n    **kwargs\n)\n</code></pre> <p>Metadata object representing a validation report generated by Great Expectations in the Feature Store.</p>"},{"location":"generated/api/validation_report_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/validation_report_api/#validate","title":"validate","text":"<pre><code>FeatureGroup.validate(\n    dataframe=None,\n    expectation_suite=None,\n    save_report=False,\n    validation_options=None,\n    ingestion_result=\"unknown\",\n    ge_type=True,\n)\n</code></pre> <p>Run validation based on the attached expectations.</p> <p>Runs the expectation suite attached to the feature group against the provided dataframe. Raise an error if the great_expectations package is not installed.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get feature group instance\nfg = fs.get_or_create_feature_group(...)\n\nge_report = fg.validate(df, save_report=False)\n</code></pre> <p>Arguments</p> <ul> <li>dataframe <code>pandas.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | None</code>: The dataframe to run the data validation expectations against.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | None</code>: Optionally provide an Expectation Suite to override the     one that is possibly attached to the feature group. This is useful for     testing new Expectation suites. When an extra suite is provided, the results     will never be persisted. Defaults to <code>None</code>.</li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> </ul> </li> <li>ingestion_result <code>Literal['unknown', 'ingested', 'rejected', 'fg_data', 'experiement']</code>: Specify the fate of the associated data, defaults     to \"UNKNOWN\". Supported options are  \"UNKNOWN\", \"INGESTED\", \"REJECTED\",     \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation     of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\"     for testing and development and \"FG_DATA\" when validating data     already in the Feature Group.</li> <li>save_report <code>bool | None</code>: Whether to save the report to the backend. This is only possible if the Expectation suite     is initialised and attached to the Feature Group. Defaults to False.</li> <li>ge_type <code>bool</code>: Whether to return a Great Expectations object or Hopsworks own abstraction.     Defaults to <code>True</code> if Great Expectations is installed, else <code>False</code>.</li> </ul> <p>Returns</p> <p>A Validation Report produced by Great Expectations.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#insert","title":"insert","text":"<pre><code>FeatureGroup.insert(\n    features,\n    overwrite=False,\n    operation=\"upsert\",\n    storage=None,\n    write_options=None,\n    validation_options=None,\n    wait=False,\n)\n</code></pre> <p>Persist the metadata and materialize the feature group to the feature store or insert data from a dataframe into the existing feature group.</p> <p>Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is <code>online_enabled=True</code>.</p> <p>The <code>features</code> dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, a Polars DataFrame or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is <code>HUDI</code> then <code>operation</code> argument can be either <code>insert</code> or <code>upsert</code>.</p> <p>If feature group doesn't exist the insert method will create the necessary metadata the first time it is invoked and writes the specified <code>features</code> dataframe as feature group to the online/offline feature store.</p> <p>Changed in 3.3.0</p> <p><code>insert</code> and <code>save</code> methods are now async by default in non-spark clients. To achieve the old behaviour, set <code>wait</code> argument to <code>True</code>.</p> <p>Upsert new feature data with time travel format <code>HUDI</code></p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_or_create_feature_group(\n    name='bitcoin_price',\n    description='Bitcoin price aggregated for days',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\n\nfg.insert(df_bitcoin_processed)\n</code></pre> <p>Async insert</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg1 = fs.get_or_create_feature_group(\n    name='feature_group_name1',\n    description='Description of the first FG',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\n# async insertion in order not to wait till finish of the job\nfg.insert(df_for_fg1, write_options={\"wait_for_job\" : False})\n\nfg2 = fs.get_or_create_feature_group(\n    name='feature_group_name2',\n    description='Description of the second FG',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\nfg.insert(df_for_fg2)\n</code></pre> <p>Arguments</p> <ul> <li>features <code>pandas.DataFrame | polars.dataframe.frame.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | hsfs.feature_group.pyspark.RDD | numpy.ndarray | List[list]</code>: Pandas DataFrame, Polars DataFrame, RDD, Ndarray, list. Features to be saved.</li> <li>overwrite <code>bool</code>: Drop all data in the feature group before     inserting new data. This does not affect metadata, defaults to False.</li> <li>operation <code>str | None</code>: Apache Hudi operation type <code>\"insert\"</code> or <code>\"upsert\"</code>.     Defaults to <code>\"upsert\"</code>.</li> <li>storage <code>str | None</code>: Overwrite default behaviour, write to offline     storage only with <code>\"offline\"</code> or online only with <code>\"online\"</code>, defaults     to <code>None</code> (If the streaming APIs are enabled, specifying the storage option is not supported).</li> <li>write_options <code>Dict[str, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to write data into the   feature group.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the insert call should return only   after the Hopsworks Job has finished. By default it waits.</li> <li>key <code>start_offline_backfill</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. <code>start_offline_backfill</code> is deprecated. Use <code>start_offline_materialization</code> instead.</li> <li>key <code>start_offline_materialization</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. By default the materialization job gets started immediately.</li> <li>key <code>kafka_producer_config</code> and value an object of type properties   used to configure the Kafka client. To optimize for throughput in high latency connection consider   changing producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established   connectivity from you Python environment to the internal advertised   listeners of the Hopsworks Kafka Cluster. Defaults to <code>False</code> and   will use external listeners when connecting from outside of Hopsworks.</li> </ul> </li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> <li>key <code>fetch_expectation_suite</code> a boolean value, by default <code>True</code>, to control whether the expectation    suite of the feature group should be fetched before every insert.</li> </ul> </li> <li>wait <code>bool</code>: Wait for job to finish before returning, defaults to <code>False</code>.     Shortcut for read_options <code>{\"wait_for_job\": False}</code>.</li> </ul> <p>Returns</p> <p>(<code>Job</code>, <code>ValidationReport</code>) A tuple with job information if python engine is used and the validation report if validation is enabled.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. e.g fail to create feature group, dataframe schema does not match     existing feature group schema, etc. <code>hsfs.client.exceptions.DataValidationException</code>. If data validation fails and the expectation     suite <code>validation_ingestion_policy</code> is set to <code>STRICT</code>. Data is NOT ingested.</p>"},{"location":"generated/api/validation_report_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/validation_report_api/#get_latest_validation_report","title":"get_latest_validation_report","text":"<pre><code>FeatureGroup.get_latest_validation_report(ge_type=True)\n</code></pre> <p>Return the latest validation report attached to the Feature Group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nlatest_val_report = fg.get_latest_validation_report()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code> if Great Expectations is installed,     else <code>False</code>.</li> </ul> <p>Returns</p> <p><code>ValidationReport</code>. The latest validation report attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#get_all_validation_reports","title":"get_all_validation_reports","text":"<pre><code>FeatureGroup.get_all_validation_reports(ge_type=True)\n</code></pre> <p>Return the latest validation report attached to the feature group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nval_reports = fg.get_all_validation_reports()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code> if Great Expectations is installed,     else <code>False</code>.</li> </ul> <p>Returns</p> <p>Union[List[<code>ValidationReport</code>], <code>ValidationReport</code>]. All validation reports attached to the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>.</p>"},{"location":"generated/api/validation_report_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/validation_report_api/#evaluation_parameters","title":"evaluation_parameters","text":"<p>Evaluation parameters field of the validation report which store kwargs of the validation.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#id","title":"id","text":"<p>Id of the validation report, set by backend.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#ingestion_result","title":"ingestion_result","text":"<p>Overall success of the validation run together with the ingestion validation policy. Indicating if dataframe was ingested or rejected.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#meta","title":"meta","text":"<p>Meta field of the validation report to store additional informations.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#results","title":"results","text":"<p>List of expectation results obtained after validation.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#statistics","title":"statistics","text":"<p>Statistics field of the validation report which store overall statistics about the validation result, e.g number of failing/successful expectations.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#success","title":"success","text":"<p>Overall success of the validation step</p>"},{"location":"generated/api/validation_report_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/validation_report_api/#from_response_json","title":"from_response_json","text":"<pre><code>ValidationReport.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#json","title":"json","text":"<pre><code>ValidationReport.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#to_dict","title":"to_dict","text":"<pre><code>ValidationReport.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#to_ge_type","title":"to_ge_type","text":"<pre><code>ValidationReport.to_ge_type()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#to_json_dict","title":"to_json_dict","text":"<pre><code>ValidationReport.to_json_dict()\n</code></pre>"},{"location":"generated/model-registry/links/","title":"Provenance Links","text":"<p>Provenance Links are objects returned by methods such as get_feature_view_provenance, get_training_dataset_provenance. These methods use the provenance graph to return the parent feature view/training dataset of a model. These methods will return the actual instances of the feature view/training dataset if available. If the instance was deleted, or it belongs to a featurestore that the current project doesn't have access anymore, an Artifact object is returned.</p> <p>There is an additional method using the provenance graph: get_feature_view. This method wraps the <code>get_feature_view_provenance</code> and always returns a correct, usable Feature View object or throws an exception if the returned object is an Artifact. Thus an exception is thrown if the feature view was deleted or the featurestore it belongs to was unshared.</p>"},{"location":"generated/model-registry/links/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-registry/links/#accessible","title":"accessible","text":"<p>List of [FeatureView|TrainingDataset objects] objects which are part of the provenance graph requested. These entities exist in the feature store and the user has access to them.</p> <p>[source]</p>"},{"location":"generated/model-registry/links/#deleted","title":"deleted","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (feature views, training datasets) they represent. These entities have been removed from the feature store.</p> <p>[source]</p>"},{"location":"generated/model-registry/links/#faulty","title":"faulty","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (feature views, training datasets) they represent. These entities exist in the feature store, however they are corrupted.</p> <p>[source]</p>"},{"location":"generated/model-registry/links/#inaccessible","title":"inaccessible","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (feature views, training datasets) they represent. These entities exist in the feature store, however the user does not have access to them anymore.</p>"},{"location":"generated/model-registry/links/#artifact","title":"Artifact","text":"<p>Artifacts objects are part of the provenance graph and contain a minimal set of information regarding the entities (feature views, training datasets) they represent. The provenance graph contains Artifact objects when the underlying entities have been deleted or they are corrupted or they are not accessible by the current project anymore.</p> <p>[source]</p>"},{"location":"generated/model-registry/links/#model_registry_id","title":"model_registry_id","text":"<p>Id of the model registry in which the artifact is located.</p> <p>[source]</p>"},{"location":"generated/model-registry/links/#name","title":"name","text":"<p>Name of the artifact.</p> <p>[source]</p>"},{"location":"generated/model-registry/links/#version","title":"version","text":"<p>Version of the artifact</p>"},{"location":"generated/model-registry/model_api/","title":"Model","text":""},{"location":"generated/model-registry/model_api/#creation-of-a-tensorflow-model","title":"Creation of a TensorFlow model","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#create_model","title":"create_model","text":"<pre><code>hsml.model_registry.ModelRegistry.tensorflow.create_model(\n    name,\n    version=None,\n    metrics=None,\n    description=None,\n    input_example=None,\n    model_schema=None,\n    feature_view=None,\n    training_dataset_version=None,\n)\n</code></pre> <p>Create a TensorFlow model metadata object.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the <code>save()</code> method with a local file path to the directory containing the model artifacts.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to create.</li> <li>version <code>int | None</code>: Optionally version of the model to create, defaults to <code>None</code> and     will create the model with incremented version from the last     version in the model registry.</li> <li>metrics <code>dict | None</code>: Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE)</li> <li>description <code>str | None</code>: Optionally a string describing the model, defaults to empty string     <code>\"\"</code>.</li> <li>input_example <code>pandas.DataFrame | pandas.core.series.Series | numpy.ndarray | list | None</code>: Optionally an input example that represents a single input for the model, defaults to <code>None</code>.</li> <li>model_schema <code>hsml.model_schema.ModelSchema | None</code>: Optionally a model schema for the model inputs and/or outputs.</li> </ul> <p>Returns</p> <p><code>Model</code>. The model metadata object.</p>"},{"location":"generated/model-registry/model_api/#creation-of-a-torch-model","title":"Creation of a Torch model","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#create_model_1","title":"create_model","text":"<pre><code>hsml.model_registry.ModelRegistry.torch.create_model(\n    name,\n    version=None,\n    metrics=None,\n    description=None,\n    input_example=None,\n    model_schema=None,\n    feature_view=None,\n    training_dataset_version=None,\n)\n</code></pre> <p>Create a Torch model metadata object.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the <code>save()</code> method with a local file path to the directory containing the model artifacts.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to create.</li> <li>version <code>int | None</code>: Optionally version of the model to create, defaults to <code>None</code> and     will create the model with incremented version from the last     version in the model registry.</li> <li>metrics <code>dict | None</code>: Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE)</li> <li>description <code>str | None</code>: Optionally a string describing the model, defaults to empty string     <code>\"\"</code>.</li> <li>input_example <code>pandas.DataFrame | pandas.core.series.Series | numpy.ndarray | list | None</code>: Optionally an input example that represents a single input for the model, defaults to <code>None</code>.</li> <li>model_schema <code>hsml.model_schema.ModelSchema | None</code>: Optionally a model schema for the model inputs and/or outputs.</li> </ul> <p>Returns</p> <p><code>Model</code>. The model metadata object.</p>"},{"location":"generated/model-registry/model_api/#creation-of-a-scikit-learn-model","title":"Creation of a scikit-learn model","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#create_model_2","title":"create_model","text":"<pre><code>hsml.model_registry.ModelRegistry.sklearn.create_model(\n    name,\n    version=None,\n    metrics=None,\n    description=None,\n    input_example=None,\n    model_schema=None,\n    feature_view=None,\n    training_dataset_version=None,\n)\n</code></pre> <p>Create an SkLearn model metadata object.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the <code>save()</code> method with a local file path to the directory containing the model artifacts.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to create.</li> <li>version <code>int | None</code>: Optionally version of the model to create, defaults to <code>None</code> and     will create the model with incremented version from the last     version in the model registry.</li> <li>metrics <code>dict | None</code>: Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE)</li> <li>description <code>str | None</code>: Optionally a string describing the model, defaults to empty string     <code>\"\"</code>.</li> <li>input_example <code>pandas.DataFrame | pandas.core.series.Series | numpy.ndarray | list | None</code>: Optionally an input example that represents a single input for the model, defaults to <code>None</code>.</li> <li>model_schema <code>hsml.model_schema.ModelSchema | None</code>: Optionally a model schema for the model inputs and/or outputs.</li> </ul> <p>Returns</p> <p><code>Model</code>. The model metadata object.</p>"},{"location":"generated/model-registry/model_api/#creation-of-a-generic-model","title":"Creation of a generic model","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#create_model_3","title":"create_model","text":"<pre><code>hsml.model_registry.ModelRegistry.python.create_model(\n    name,\n    version=None,\n    metrics=None,\n    description=None,\n    input_example=None,\n    model_schema=None,\n    feature_view=None,\n    training_dataset_version=None,\n)\n</code></pre> <p>Create a generic Python model metadata object.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the <code>save()</code> method with a local file path to the directory containing the model artifacts.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to create.</li> <li>version <code>int | None</code>: Optionally version of the model to create, defaults to <code>None</code> and     will create the model with incremented version from the last     version in the model registry.</li> <li>metrics <code>dict | None</code>: Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE)</li> <li>description <code>str | None</code>: Optionally a string describing the model, defaults to empty string     <code>\"\"</code>.</li> <li>input_example <code>pandas.DataFrame | pandas.core.series.Series | numpy.ndarray | list | None</code>: Optionally an input example that represents a single input for the model, defaults to <code>None</code>.</li> <li>model_schema <code>hsml.model_schema.ModelSchema | None</code>: Optionally a model schema for the model inputs and/or outputs.</li> </ul> <p>Returns</p> <p><code>Model</code>. The model metadata object.</p>"},{"location":"generated/model-registry/model_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_model","title":"get_model","text":"<pre><code>ModelRegistry.get_model(name, version=None)\n</code></pre> <p>Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to get.</li> <li>version <code>int</code>: Version of the model to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>Model</code>: The model metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve model from the model registry.</li> </ul>"},{"location":"generated/model-registry/model_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#created","title":"created","text":"<p>Creation date of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#creator","title":"creator","text":"<p>Creator of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#description","title":"description","text":"<p>Description of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#environment","title":"environment","text":"<p>Input example of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#framework","title":"framework","text":"<p>framework of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#id","title":"id","text":"<p>Id of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#input_example","title":"input_example","text":"<p>input_example of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#model_files_path","title":"model_files_path","text":"<p>path of the model files including version and files folder. Resolves to /Projects/{project_name}/Models/{name}/{version}/Files</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#model_path","title":"model_path","text":"<p>path of the model with version folder omitted. Resolves to /Projects/{project_name}/Models/{name}</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#model_registry_id","title":"model_registry_id","text":"<p>model_registry_id of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#model_schema","title":"model_schema","text":"<p>model schema of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#name","title":"name","text":"<p>Name of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#program","title":"program","text":"<p>Executable used to export the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#project_name","title":"project_name","text":"<p>project_name of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#shared_registry_project_name","title":"shared_registry_project_name","text":"<p>shared_registry_project_name of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#training_dataset","title":"training_dataset","text":"<p>training_dataset of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#training_dataset_version","title":"training_dataset_version","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#training_metrics","title":"training_metrics","text":"<p>Training metrics of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#user","title":"user","text":"<p>user of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#version","title":"version","text":"<p>Version of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#version_path","title":"version_path","text":"<p>path of the model including version folder. Resolves to /Projects/{project_name}/Models/{name}/{version}</p>"},{"location":"generated/model-registry/model_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#delete","title":"delete","text":"<pre><code>Model.delete()\n</code></pre> <p>Delete the model</p> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the model and deletes the model files.</p> <p>Raises</p> <p><code>RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#delete_tag","title":"delete_tag","text":"<pre><code>Model.delete_tag(name)\n</code></pre> <p>Delete a tag attached to a model.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#deploy","title":"deploy","text":"<pre><code>Model.deploy(\n    name=None,\n    description=None,\n    artifact_version=\"CREATE\",\n    serving_tool=None,\n    script_file=None,\n    resources=None,\n    inference_logger=None,\n    inference_batcher=None,\n    transformer=None,\n    api_protocol=\"REST\",\n    environment=None,\n)\n</code></pre> <p>Deploy the model.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nmy_deployment = my_model.deploy()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: Name of the deployment.</li> <li>description <code>str | None</code>: Description of the deployment.</li> <li>artifact_version <code>str | None</code>: Version number of the model artifact to deploy, <code>CREATE</code> to create a new model artifact or <code>MODEL-ONLY</code> to reuse the shared artifact containing only the model files.</li> <li>serving_tool <code>str | None</code>: Serving tool used to deploy the model server.</li> <li>script_file <code>str | None</code>: Path to a custom predictor script implementing the Predict class.</li> <li>resources <code>hsml.resources.PredictorResources | dict | None</code>: Resources to be allocated for the predictor.</li> <li>inference_logger <code>hsml.inference_logger.InferenceLogger | dict | None</code>: Inference logger configuration.</li> <li>inference_batcher <code>hsml.inference_batcher.InferenceBatcher | dict | None</code>: Inference batcher configuration.</li> <li>transformer <code>hsml.transformer.Transformer | dict | None</code>: Transformer to be deployed together with the predictor.</li> <li>api_protocol <code>str | None</code>: API protocol to be enabled in the deployment (i.e., 'REST' or 'GRPC'). Defaults to 'REST'.</li> <li>environment <code>str | None</code>: The inference environment to use.</li> </ul> <p>Returns</p> <p><code>Deployment</code>: The deployment metadata object of a new or existing deployment.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#download","title":"download","text":"<pre><code>Model.download(local_path=None)\n</code></pre> <p>Download the model files.</p> <p>Arguments</p> <ul> <li>local_path: path where to download the model files in the local filesystem</li> </ul> <p>Returns</p> <p><code>str</code>: Absolute path to local folder containing the model files.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_feature_view","title":"get_feature_view","text":"<pre><code>Model.get_feature_view(init=True, online=None)\n</code></pre> <p>Get the parent feature view of this model, based on explicit provenance.  Only accessible, usable feature view objects are returned. Otherwise an Exception is raised.  For more details, call the base method - get_feature_view_provenance</p> <p>Returns</p> <p><code>FeatureView</code>: Feature View Object.</p> <p>Raises</p> <p><code>Exception</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_feature_view_provenance","title":"get_feature_view_provenance","text":"<pre><code>Model.get_feature_view_provenance()\n</code></pre> <p>Get the parent feature view of this model, based on explicit provenance. This feature view can be accessible, deleted or inaccessible. For deleted and inaccessible feature views, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_tag","title":"get_tag","text":"<pre><code>Model.get_tag(name)\n</code></pre> <p>Get the tags of a model.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_tags","title":"get_tags","text":"<pre><code>Model.get_tags()\n</code></pre> <p>Retrieves all tags attached to a model.</p> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_training_dataset_provenance","title":"get_training_dataset_provenance","text":"<pre><code>Model.get_training_dataset_provenance()\n</code></pre> <p>Get the parent training dataset of this model, based on explicit provenance. This training dataset can be accessible, deleted or inaccessible. For deleted and inaccessible training datasets, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_url","title":"get_url","text":"<pre><code>Model.get_url()\n</code></pre> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#save","title":"save","text":"<pre><code>Model.save(\n    model_path, await_registration=480, keep_original_files=False, upload_configuration=None\n)\n</code></pre> <p>Persist this model including model files and metadata to the model registry.</p> <p>Arguments</p> <ul> <li>model_path: Local or remote (Hopsworks file system) path to the folder where the model files are located, or path to a specific model file.</li> <li>await_registration: Awaiting time for the model to be registered in Hopsworks.</li> <li>keep_original_files: If the model files are located in hopsfs, whether to move or copy those files into the Models dataset. Default is False (i.e., model files will be moved)</li> <li>upload_configuration <code>Dict[str, Any] | None</code>: When saving a model from outside Hopsworks, the model is uploaded to the model registry using the REST APIs. Each model artifact is divided into     chunks and each chunk uploaded independently. This parameter can be used to control the upload chunk size, the parallelism and the number of retries.     <code>upload_configuration</code> can contain the following keys:<ul> <li>key <code>chunk_size</code>: size of each chunk in megabytes. Default 10.</li> <li>key <code>simultaneous_uploads</code>: number of chunks to upload in parallel. Default 3.</li> <li>key <code>max_chunk_retries</code>: number of times to retry the upload of a chunk in case of failure. Default 1.</li> </ul> </li> </ul> <p>Returns</p> <p><code>Model</code>: The model metadata object.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#set_tag","title":"set_tag","text":"<pre><code>Model.set_tag(name, value)\n</code></pre> <p>Attach a tag to a model.</p> <p>A tag consists of a  pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value <code>str | dict</code>: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to add the tag.</p>"},{"location":"generated/model-registry/model_registry_api/","title":"Model Registry","text":""},{"location":"generated/model-registry/model_registry_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#get_model_registry","title":"get_model_registry","text":"<pre><code>Connection.get_model_registry(project=None)\n</code></pre> <p>Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the <code>project</code> argument.</p> <p>Arguments</p> <ul> <li>project <code>str</code>: The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>ModelRegistry</code>. A model registry handle object to perform operations on.</p>"},{"location":"generated/model-registry/model_registry_api/#modules","title":"Modules","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#llm","title":"llm","text":"<p>Module for exporting a Large Language Model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#project_path","title":"project_path","text":"<p>Path of the project the registry is connected to.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#python","title":"python","text":"<p>Module for exporting a generic Python model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#sklearn","title":"sklearn","text":"<p>Module for exporting a sklearn model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#tensorflow","title":"tensorflow","text":"<p>Module for exporting a TensorFlow model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#torch","title":"torch","text":"<p>Module for exporting a torch model.</p>"},{"location":"generated/model-registry/model_registry_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#llm_1","title":"llm","text":"<p>Module for exporting a Large Language Model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#model_registry_id","title":"model_registry_id","text":"<p>Id of the model registry.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#project_id","title":"project_id","text":"<p>Id of the project the registry is connected to.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#project_name","title":"project_name","text":"<p>Name of the project the registry is connected to.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#project_path_1","title":"project_path","text":"<p>Path of the project the registry is connected to.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#shared_registry_project_name","title":"shared_registry_project_name","text":"<p>Name of the project the shared model registry originates from.</p>"},{"location":"generated/model-registry/model_registry_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#get_best_model","title":"get_best_model","text":"<pre><code>ModelRegistry.get_best_model(name, metric, direction)\n</code></pre> <p>Get the best performing model entity from the model registry. Getting the best performing model from the Model Registry means specifying in addition to the name, also a metric name corresponding to one of the keys in the training_metrics dict of the model and a direction. For example to get the model version with the highest accuracy, specify metric='accuracy' and direction='max'.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to get.</li> <li>metric <code>str</code>: Name of the key in the training metrics field to compare.</li> <li>direction <code>str</code>: 'max' to get the model entity with the highest value of the set metric, or 'min' for the lowest.</li> </ul> <p>Returns</p> <p><code>Model</code>: The model metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve model from the model registry.</li> </ul> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#get_model","title":"get_model","text":"<pre><code>ModelRegistry.get_model(name, version=None)\n</code></pre> <p>Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to get.</li> <li>version <code>int</code>: Version of the model to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>Model</code>: The model metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve model from the model registry.</li> </ul> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#get_models","title":"get_models","text":"<pre><code>ModelRegistry.get_models(name)\n</code></pre> <p>Get all model entities from the model registry for a specified name. Getting all models from the Model Registry for a given name returns a list of model entities, one for each version registered under the specified model name.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to get.</li> </ul> <p>Returns</p> <p><code>List[Model]</code>: A list of model metadata objects.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve model versions from the model registry.</li> </ul>"},{"location":"generated/model-registry/model_schema_api/","title":"Model Schema","text":""},{"location":"generated/model-registry/model_schema_api/#creation","title":"Creation","text":"<p>To create a ModelSchema, the schema of the Model inputs and/or Model ouputs has to be defined beforehand.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_schema_api/#schema","title":"Schema","text":"<pre><code>hsml.schema.Schema(object=None, **kwargs)\n</code></pre> <p>Create a schema for a model input or output.</p> <p>Arguments</p> <ul> <li>object <code>pandas.DataFrame | pandas.core.series.Series | hsml.schema.pyspark.sql.dataframe.DataFrame | hsml.schema.hsfs.training_dataset.TrainingDataset | numpy.ndarray | list | None</code>: The object to construct the schema from.</li> </ul> <p>Returns</p> <p><code>Schema</code>. The schema object.</p> <p>After defining the Model inputs and/or outputs schemas, a ModelSchema can be created using its class constructor.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_schema_api/#modelschema","title":"ModelSchema","text":"<pre><code>hsml.model_schema.ModelSchema(input_schema=None, output_schema=None, **kwargs)\n</code></pre> <p>Create a schema for a model.</p> <p>Arguments</p> <ul> <li>input_schema <code>hsml.schema.Schema | None</code>: Schema to describe the inputs.</li> <li>output_schema <code>hsml.schema.Schema | None</code>: Schema to describe the outputs.</li> </ul> <p>Returns</p> <p><code>ModelSchema</code>. The model schema object.</p>"},{"location":"generated/model-registry/model_schema_api/#retrieval","title":"Retrieval","text":""},{"location":"generated/model-registry/model_schema_api/#model-schema_1","title":"Model Schema","text":"<p>Model schemas can be accessed from the model metadata objects.</p> <pre><code>model.model_schema\n</code></pre>"},{"location":"generated/model-registry/model_schema_api/#model-input-ouput-schemas","title":"Model Input &amp; Ouput Schemas","text":"<p>The schemas of the Model inputs and outputs can be accessed from the ModelSchema metadata objects.</p> <pre><code>model_schema.input_schema\nmodel_schema.output_schema\n</code></pre>"},{"location":"generated/model-registry/model_schema_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_schema_api/#to_dict","title":"to_dict","text":"<pre><code>Schema.to_dict()\n</code></pre> <p>Get dict representation of the Schema.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_schema_api/#to_dict_1","title":"to_dict","text":"<pre><code>ModelSchema.to_dict()\n</code></pre> <p>Get dict representation of the ModelSchema.</p>"},{"location":"generated/model-serving/deployment_api/","title":"Deployment","text":""},{"location":"generated/model-serving/deployment_api/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_model_serving","title":"get_model_serving","text":"<pre><code>Connection.get_model_serving()\n</code></pre> <p>Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> <p>Returns</p> <p><code>ModelServing</code>. A model serving handle object to perform operations on.</p>"},{"location":"generated/model-serving/deployment_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#create_deployment","title":"create_deployment","text":"<pre><code>ModelServing.create_deployment(predictor, name=None, environment=None)\n</code></pre> <p>Create a Deployment metadata object.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre> <p>Using the model object</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nmy_deployment = my_model.deploy()\n\nmy_deployment.get_state().describe()\n</code></pre> <p>Using the Model Serving handle</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = my_predictor.deploy()\n\nmy_deployment.get_state().describe()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the <code>save()</code> method.</p> <p>Arguments</p> <ul> <li>predictor <code>hsml.predictor.Predictor</code>: predictor to be used in the deployment</li> <li>name <code>str | None</code>: name of the deployment</li> <li>environment <code>str | None</code>: The inference environment to use</li> </ul> <p>Returns</p> <p><code>Deployment</code>. The model metadata object.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#deploy","title":"deploy","text":"<pre><code>Model.deploy(\n    name=None,\n    description=None,\n    artifact_version=\"CREATE\",\n    serving_tool=None,\n    script_file=None,\n    resources=None,\n    inference_logger=None,\n    inference_batcher=None,\n    transformer=None,\n    api_protocol=\"REST\",\n    environment=None,\n)\n</code></pre> <p>Deploy the model.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nmy_deployment = my_model.deploy()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: Name of the deployment.</li> <li>description <code>str | None</code>: Description of the deployment.</li> <li>artifact_version <code>str | None</code>: Version number of the model artifact to deploy, <code>CREATE</code> to create a new model artifact or <code>MODEL-ONLY</code> to reuse the shared artifact containing only the model files.</li> <li>serving_tool <code>str | None</code>: Serving tool used to deploy the model server.</li> <li>script_file <code>str | None</code>: Path to a custom predictor script implementing the Predict class.</li> <li>resources <code>hsml.resources.PredictorResources | dict | None</code>: Resources to be allocated for the predictor.</li> <li>inference_logger <code>hsml.inference_logger.InferenceLogger | dict | None</code>: Inference logger configuration.</li> <li>inference_batcher <code>hsml.inference_batcher.InferenceBatcher | dict | None</code>: Inference batcher configuration.</li> <li>transformer <code>hsml.transformer.Transformer | dict | None</code>: Transformer to be deployed together with the predictor.</li> <li>api_protocol <code>str | None</code>: API protocol to be enabled in the deployment (i.e., 'REST' or 'GRPC'). Defaults to 'REST'.</li> <li>environment <code>str | None</code>: The inference environment to use.</li> </ul> <p>Returns</p> <p><code>Deployment</code>: The deployment metadata object of a new or existing deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#deploy_1","title":"deploy","text":"<pre><code>Predictor.deploy()\n</code></pre> <p>Create a deployment for this predictor and persists it in the Model Serving.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\nmy_deployment = my_predictor.deploy()\n\nprint(my_deployment.get_state())\n</code></pre> <p>Returns</p> <p><code>Deployment</code>. The deployment metadata object of a new or existing deployment.</p>"},{"location":"generated/model-serving/deployment_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_deployment","title":"get_deployment","text":"<pre><code>ModelServing.get_deployment(name=None)\n</code></pre> <p>Get a deployment by name from Model Serving.</p> <p>Example</p> <pre><code># login and get Hopsworks Model Serving handle using .login() and .get_model_serving()\n\n# get a deployment by name\nmy_deployment = ms.get_deployment('deployment_name')\n</code></pre> <p>Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop).</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the deployment to get.</li> </ul> <p>Returns</p> <p><code>Deployment</code>: The deployment metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve deployment from model serving.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_deployment_by_id","title":"get_deployment_by_id","text":"<pre><code>ModelServing.get_deployment_by_id(id)\n</code></pre> <p>Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop).</p> <p>Example</p> <pre><code># login and get Hopsworks Model Serving handle using .login() and .get_model_serving()\n\n# get a deployment by id\nmy_deployment = ms.get_deployment_by_id(1)\n</code></pre> <p>Arguments</p> <ul> <li>id <code>int</code>: Id of the deployment to get.</li> </ul> <p>Returns</p> <p><code>Deployment</code>: The deployment metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve deployment from model serving.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_deployments","title":"get_deployments","text":"<pre><code>ModelServing.get_deployments(model=None, status=None)\n</code></pre> <p>Get all deployments from model serving.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nlist_deployments = ms.get_deployment(my_model)\n\nfor deployment in list_deployments:\n    print(deployment.get_state())\n</code></pre> <p>Arguments</p> <ul> <li>model <code>hsml.model.Model</code>: Filter by model served in the deployments</li> <li>status <code>str</code>: Filter by status of the deployments</li> </ul> <p>Returns</p> <p><code>List[Deployment]</code>: A list of deployments.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve deployments from model serving.</li> </ul>"},{"location":"generated/model-serving/deployment_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#api_protocol","title":"api_protocol","text":"<p>API protocol enabled in the deployment (e.g., HTTP or GRPC).</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#artifact_files_path","title":"artifact_files_path","text":"<p>Path of the artifact files deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#artifact_path","title":"artifact_path","text":"<p>Path of the model artifact deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#artifact_version","title":"artifact_version","text":"<p>Artifact version deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#created_at","title":"created_at","text":"<p>Created at date of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#creator","title":"creator","text":"<p>Creator of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#description","title":"description","text":"<p>Description of the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#environment","title":"environment","text":"<p>Name of inference environment</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#id","title":"id","text":"<p>Id of the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#inference_batcher","title":"inference_batcher","text":"<p>Configuration of the inference batcher attached to this predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#inference_logger","title":"inference_logger","text":"<p>Configuration of the inference logger attached to this predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#model_name","title":"model_name","text":"<p>Name of the model deployed by the predictor</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#model_path","title":"model_path","text":"<p>Model path deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#model_registry_id","title":"model_registry_id","text":"<p>Model Registry Id of the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#model_server","title":"model_server","text":"<p>Model server ran by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#model_version","title":"model_version","text":"<p>Model version deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#name","title":"name","text":"<p>Name of the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#predictor","title":"predictor","text":"<p>Predictor used in the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#project_namespace","title":"project_namespace","text":"<p>Name of inference environment</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#requested_instances","title":"requested_instances","text":"<p>Total number of requested instances in the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#resources","title":"resources","text":"<p>Resource configuration for the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#script_file","title":"script_file","text":"<p>Script file used by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#serving_tool","title":"serving_tool","text":"<p>Serving tool used to run the model server.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#transformer","title":"transformer","text":"<p>Transformer configured in the predictor.</p>"},{"location":"generated/model-serving/deployment_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#delete","title":"delete","text":"<pre><code>Deployment.delete(force=False)\n</code></pre> <p>Delete the deployment</p> <p>Arguments</p> <ul> <li>force: Force the deletion of the deployment.        If the deployment is running, it will be stopped and deleted automatically.        !!! warn A call to this method does not ask for a second confirmation.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#describe","title":"describe","text":"<pre><code>Deployment.describe()\n</code></pre> <p>Print a description of the deployment</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#download_artifact_files","title":"download_artifact_files","text":"<pre><code>Deployment.download_artifact_files(local_path=None)\n</code></pre> <p>Download the artifact files served by the deployment</p> <p>Arguments</p> <ul> <li>local_path: path where to download the artifact files in the local filesystem</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_logs","title":"get_logs","text":"<pre><code>Deployment.get_logs(component=\"predictor\", tail=10)\n</code></pre> <p>Prints the deployment logs of the predictor or transformer.</p> <p>Arguments</p> <ul> <li>component: Deployment component to get the logs from (e.g., predictor or transformer)</li> <li>tail: Number of most recent lines to retrieve from the logs.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_model","title":"get_model","text":"<pre><code>Deployment.get_model()\n</code></pre> <p>Retrieve the metadata object for the model being used by this deployment</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_state","title":"get_state","text":"<pre><code>Deployment.get_state()\n</code></pre> <p>Get the current state of the deployment</p> <p>Returns</p> <p><code>PredictorState</code>. The state of the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_url","title":"get_url","text":"<pre><code>Deployment.get_url()\n</code></pre> <p>Get url to the deployment in Hopsworks</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#is_created","title":"is_created","text":"<pre><code>Deployment.is_created()\n</code></pre> <p>Check whether the deployment is created.</p> <p>Returns</p> <p><code>bool</code>. Whether the deployment is created or not.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#is_running","title":"is_running","text":"<pre><code>Deployment.is_running(or_idle=True, or_updating=True)\n</code></pre> <p>Check whether the deployment is ready to handle inference requests</p> <p>Arguments</p> <ul> <li>or_idle: Whether the idle state is considered as running (default is True)</li> <li>or_updating: Whether the updating state is considered as running (default is True)</li> </ul> <p>Returns</p> <p><code>bool</code>. Whether the deployment is ready or not.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#is_stopped","title":"is_stopped","text":"<pre><code>Deployment.is_stopped(or_created=True)\n</code></pre> <p>Check whether the deployment is stopped</p> <p>Arguments</p> <ul> <li>or_created: Whether the creating and created state is considered as stopped (default is True)</li> </ul> <p>Returns</p> <p><code>bool</code>. Whether the deployment is stopped or not.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#predict","title":"predict","text":"<pre><code>Deployment.predict(data=None, inputs=None)\n</code></pre> <p>Send inference requests to the deployment.    One of data or inputs parameters must be set. If both are set, inputs will be ignored.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# retrieve deployment by name\nmy_deployment = ms.get_deployment(\"my_deployment\")\n\n# (optional) retrieve model input example\nmy_model = project.get_model_registry()                                .get_model(my_deployment.model_name, my_deployment.model_version)\n\n# make predictions using model inputs (single or batch)\npredictions = my_deployment.predict(inputs=my_model.input_example)\n\n# or using more sophisticated inference request payloads\ndata = { \"instances\": [ my_model.input_example ], \"key2\": \"value2\" }\npredictions = my_deployment.predict(data)\n</code></pre> <p>Arguments</p> <ul> <li>data <code>Dict | hopsworks_common.client.istio.utils.infer_type.InferInput | None</code>: Payload dictionary for the inference request including the model input(s)</li> <li>inputs <code>List | Dict | None</code>: Model inputs used in the inference requests</li> </ul> <p>Returns</p> <p><code>dict</code>. Inference response.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#save","title":"save","text":"<pre><code>Deployment.save(await_update=120)\n</code></pre> <p>Persist this deployment including the predictor and metadata to Model Serving.</p> <p>Arguments</p> <ul> <li>await_update <code>int | None</code>: If the deployment is running, awaiting time (seconds) for the running instances to be updated.               If the running instances are not updated within this timespan, the call to this method returns while               the update in the background.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#start","title":"start","text":"<pre><code>Deployment.start(await_running=120)\n</code></pre> <p>Start the deployment</p> <p>Arguments</p> <ul> <li>await_running <code>int | None</code>: Awaiting time (seconds) for the deployment to start.                If the deployment has not started within this timespan, the call to this method returns while                it deploys in the background.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#stop","title":"stop","text":"<pre><code>Deployment.stop(await_stopped=120)\n</code></pre> <p>Stop the deployment</p> <p>Arguments</p> <ul> <li>await_stopped <code>int | None</code>: Awaiting time (seconds) for the deployment to stop.                If the deployment has not stopped within this timespan, the call to this method returns while                it stopping in the background.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#to_dict","title":"to_dict","text":"<pre><code>Deployment.to_dict()\n</code></pre>"},{"location":"generated/model-serving/inference_batcher_api/","title":"Inference batcher","text":""},{"location":"generated/model-serving/inference_batcher_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#inferencebatcher","title":"InferenceBatcher","text":"<pre><code>hsml.inference_batcher.InferenceBatcher(\n    enabled=None, max_batch_size=None, max_latency=None, timeout=None, **kwargs\n)\n</code></pre> <p>Configuration of an inference batcher for a predictor.</p> <p>Arguments</p> <ul> <li>enabled <code>bool | None</code>: Whether the inference batcher is enabled or not. The default value is <code>false</code>.</li> <li>max_batch_size <code>int | None</code>: Maximum requests batch size.</li> <li>max_latency <code>int | None</code>: Maximum latency for request batching.</li> <li>timeout <code>int | None</code>: Maximum waiting time for request batching.</li> </ul> <p>Returns</p> <p><code>InferenceLogger</code>. Configuration of an inference logger.</p>"},{"location":"generated/model-serving/inference_batcher_api/#retrieval","title":"Retrieval","text":""},{"location":"generated/model-serving/inference_batcher_api/#predictorinference_batcher","title":"predictor.inference_batcher","text":"<p>Inference batchers can be accessed from the predictor metadata objects.</p> <pre><code>predictor.inference_batcher\n</code></pre> <p>Predictors can be found in the deployment metadata objects (see Predictor Reference). To retrieve a deployment, see the Deployment Reference.</p>"},{"location":"generated/model-serving/inference_batcher_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#enabled","title":"enabled","text":"<p>Whether the inference batcher is enabled or not.</p> <p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#max_batch_size","title":"max_batch_size","text":"<p>Maximum requests batch size.</p> <p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#max_latency","title":"max_latency","text":"<p>Maximum latency.</p> <p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#timeout","title":"timeout","text":"<p>Maximum timeout.</p>"},{"location":"generated/model-serving/inference_batcher_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#describe","title":"describe","text":"<pre><code>InferenceBatcher.describe()\n</code></pre> <p>Print a description of the inference batcher</p> <p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#to_dict","title":"to_dict","text":"<pre><code>InferenceBatcher.to_dict()\n</code></pre>"},{"location":"generated/model-serving/inference_logger_api/","title":"Inference logger","text":""},{"location":"generated/model-serving/inference_logger_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/model-serving/inference_logger_api/#inferencelogger","title":"InferenceLogger","text":"<pre><code>hsml.inference_logger.InferenceLogger(kafka_topic=DEFAULT, mode=\"ALL\", **kwargs)\n</code></pre> <p>Configuration of an inference logger for a predictor.</p> <p>Arguments</p> <ul> <li>kafka_topic <code>hsml.kafka_topic.KafkaTopic | dict | hopsworks_common.constants.Default | None</code>: Kafka topic to send the inference logs to. By default, a new Kafka topic is configured.</li> <li>mode <code>str | None</code>: Inference logging mode. (e.g., <code>NONE</code>, <code>ALL</code>, <code>PREDICTIONS</code>, or <code>MODEL_INPUTS</code>). By default, <code>ALL</code> inference logs are sent.</li> </ul> <p>Returns</p> <p><code>InferenceLogger</code>. Configuration of an inference logger.</p>"},{"location":"generated/model-serving/inference_logger_api/#retrieval","title":"Retrieval","text":""},{"location":"generated/model-serving/inference_logger_api/#predictorinference_logger","title":"predictor.inference_logger","text":"<p>Inference loggers can be accessed from the predictor metadata objects.</p> <pre><code>predictor.inference_logger\n</code></pre> <p>Predictors can be found in the deployment metadata objects (see Predictor Reference). To retrieve a deployment, see the Deployment Reference.</p>"},{"location":"generated/model-serving/inference_logger_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/inference_logger_api/#kafka_topic","title":"kafka_topic","text":"<p>Kafka topic to send the inference logs to.</p> <p>[source]</p>"},{"location":"generated/model-serving/inference_logger_api/#mode","title":"mode","text":"<p>Inference logging mode (\"NONE\", \"ALL\", \"PREDICTIONS\", or \"MODEL_INPUTS\").</p>"},{"location":"generated/model-serving/inference_logger_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/inference_logger_api/#describe","title":"describe","text":"<pre><code>InferenceLogger.describe()\n</code></pre> <p>Print a description of the inference logger</p> <p>[source]</p>"},{"location":"generated/model-serving/inference_logger_api/#to_dict","title":"to_dict","text":"<pre><code>InferenceLogger.to_dict()\n</code></pre>"},{"location":"generated/model-serving/model_serving_api/","title":"Model Serving","text":""},{"location":"generated/model-serving/model_serving_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#get_model_serving","title":"get_model_serving","text":"<pre><code>Connection.get_model_serving()\n</code></pre> <p>Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> <p>Returns</p> <p><code>ModelServing</code>. A model serving handle object to perform operations on.</p>"},{"location":"generated/model-serving/model_serving_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#project_id","title":"project_id","text":"<p>Id of the project in which Model Serving is located.</p> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#project_name","title":"project_name","text":"<p>Name of the project in which Model Serving is located.</p> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#project_path","title":"project_path","text":"<p>Path of the project the registry is connected to.</p>"},{"location":"generated/model-serving/model_serving_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#create_deployment","title":"create_deployment","text":"<pre><code>ModelServing.create_deployment(predictor, name=None, environment=None)\n</code></pre> <p>Create a Deployment metadata object.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre> <p>Using the model object</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nmy_deployment = my_model.deploy()\n\nmy_deployment.get_state().describe()\n</code></pre> <p>Using the Model Serving handle</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = my_predictor.deploy()\n\nmy_deployment.get_state().describe()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the <code>save()</code> method.</p> <p>Arguments</p> <ul> <li>predictor <code>hsml.predictor.Predictor</code>: predictor to be used in the deployment</li> <li>name <code>str | None</code>: name of the deployment</li> <li>environment <code>str | None</code>: The inference environment to use</li> </ul> <p>Returns</p> <p><code>Deployment</code>. The model metadata object.</p> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#create_predictor","title":"create_predictor","text":"<pre><code>ModelServing.create_predictor(\n    model,\n    name=None,\n    artifact_version=\"CREATE\",\n    serving_tool=None,\n    script_file=None,\n    resources=None,\n    inference_logger=None,\n    inference_batcher=None,\n    transformer=None,\n    api_protocol=\"REST\",\n)\n</code></pre> <p>Create a Predictor metadata object.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = my_predictor.deploy()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the <code>deploy()</code> method.</p> <p>Arguments</p> <ul> <li>model <code>hsml.model.Model</code>: Model to be deployed.</li> <li>name <code>str | None</code>: Name of the predictor.</li> <li>artifact_version <code>str | None</code>: Version number of the model artifact to deploy, <code>CREATE</code> to create a new model artifact or <code>MODEL-ONLY</code> to reuse the shared artifact containing only the model files.</li> <li>serving_tool <code>str | None</code>: Serving tool used to deploy the model server.</li> <li>script_file <code>str | None</code>: Path to a custom predictor script implementing the Predict class.</li> <li>resources <code>hsml.resources.PredictorResources | dict | None</code>: Resources to be allocated for the predictor.</li> <li>inference_logger <code>hsml.inference_logger.InferenceLogger | dict | str | None</code>: Inference logger configuration.</li> <li>inference_batcher <code>hsml.inference_batcher.InferenceBatcher | dict | None</code>: Inference batcher configuration.</li> <li>transformer <code>hsml.transformer.Transformer | dict | None</code>: Transformer to be deployed together with the predictor.</li> <li>api_protocol <code>str | None</code>: API protocol to be enabled in the deployment (i.e., 'REST' or 'GRPC'). Defaults to 'REST'.</li> </ul> <p>Returns</p> <p><code>Predictor</code>. The predictor metadata object.</p> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#create_transformer","title":"create_transformer","text":"<pre><code>ModelServing.create_transformer(script_file=None, resources=None)\n</code></pre> <p>Create a Transformer metadata object.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Dataset API instance\ndataset_api = project.get_dataset_api()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# create my_transformer.py Python script\nclass Transformer(object):\n    def __init__(self):\n        ''' Initialization code goes here '''\n        pass\n\n    def preprocess(self, inputs):\n        ''' Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. '''\n        return inputs\n\n    def postprocess(self, outputs):\n        ''' Transform the predictions computed by the model before returning a response '''\n        return outputs\n\nuploaded_file_path = dataset_api.upload(\"my_transformer.py\", \"Resources\", overwrite=True)\ntransformer_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)\n\nmy_transformer = ms.create_transformer(script_file=uploaded_file_path)\n\n# or\n\nfrom hsml.transformer import Transformer\n\nmy_transformer = Transformer(script_file)\n</code></pre> <p>Create a deployment with the transformer</p> <pre><code>my_predictor = ms.create_predictor(transformer=my_transformer)\nmy_deployment = my_predictor.deploy()\n\n# or\nmy_deployment = ms.create_deployment(my_predictor, transformer=my_transformer)\nmy_deployment.save()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the <code>predictor.transformer</code> property.</p> <p>Arguments</p> <ul> <li>script_file <code>str | None</code>: Path to a custom predictor script implementing the Transformer class.</li> <li>resources <code>hsml.resources.PredictorResources | dict | None</code>: Resources to be allocated for the transformer.</li> </ul> <p>Returns</p> <p><code>Transformer</code>. The model metadata object.</p> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#get_deployment","title":"get_deployment","text":"<pre><code>ModelServing.get_deployment(name=None)\n</code></pre> <p>Get a deployment by name from Model Serving.</p> <p>Example</p> <pre><code># login and get Hopsworks Model Serving handle using .login() and .get_model_serving()\n\n# get a deployment by name\nmy_deployment = ms.get_deployment('deployment_name')\n</code></pre> <p>Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop).</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the deployment to get.</li> </ul> <p>Returns</p> <p><code>Deployment</code>: The deployment metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve deployment from model serving.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#get_deployment_by_id","title":"get_deployment_by_id","text":"<pre><code>ModelServing.get_deployment_by_id(id)\n</code></pre> <p>Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop).</p> <p>Example</p> <pre><code># login and get Hopsworks Model Serving handle using .login() and .get_model_serving()\n\n# get a deployment by id\nmy_deployment = ms.get_deployment_by_id(1)\n</code></pre> <p>Arguments</p> <ul> <li>id <code>int</code>: Id of the deployment to get.</li> </ul> <p>Returns</p> <p><code>Deployment</code>: The deployment metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve deployment from model serving.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#get_deployments","title":"get_deployments","text":"<pre><code>ModelServing.get_deployments(model=None, status=None)\n</code></pre> <p>Get all deployments from model serving.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nlist_deployments = ms.get_deployment(my_model)\n\nfor deployment in list_deployments:\n    print(deployment.get_state())\n</code></pre> <p>Arguments</p> <ul> <li>model <code>hsml.model.Model</code>: Filter by model served in the deployments</li> <li>status <code>str</code>: Filter by status of the deployments</li> </ul> <p>Returns</p> <p><code>List[Deployment]</code>: A list of deployments.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve deployments from model serving.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#get_inference_endpoints","title":"get_inference_endpoints","text":"<pre><code>ModelServing.get_inference_endpoints()\n</code></pre> <p>Get all inference endpoints available in the current project.</p> <p>Returns</p> <p><code>List[InferenceEndpoint]</code>: Inference endpoints for model inference</p>"},{"location":"generated/model-serving/predictor_api/","title":"Predictor","text":""},{"location":"generated/model-serving/predictor_api/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#get_model_serving","title":"get_model_serving","text":"<pre><code>Connection.get_model_serving()\n</code></pre> <p>Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> <p>Returns</p> <p><code>ModelServing</code>. A model serving handle object to perform operations on.</p>"},{"location":"generated/model-serving/predictor_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#create_predictor","title":"create_predictor","text":"<pre><code>ModelServing.create_predictor(\n    model,\n    name=None,\n    artifact_version=\"CREATE\",\n    serving_tool=None,\n    script_file=None,\n    resources=None,\n    inference_logger=None,\n    inference_batcher=None,\n    transformer=None,\n    api_protocol=\"REST\",\n)\n</code></pre> <p>Create a Predictor metadata object.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = my_predictor.deploy()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the <code>deploy()</code> method.</p> <p>Arguments</p> <ul> <li>model <code>hsml.model.Model</code>: Model to be deployed.</li> <li>name <code>str | None</code>: Name of the predictor.</li> <li>artifact_version <code>str | None</code>: Version number of the model artifact to deploy, <code>CREATE</code> to create a new model artifact or <code>MODEL-ONLY</code> to reuse the shared artifact containing only the model files.</li> <li>serving_tool <code>str | None</code>: Serving tool used to deploy the model server.</li> <li>script_file <code>str | None</code>: Path to a custom predictor script implementing the Predict class.</li> <li>resources <code>hsml.resources.PredictorResources | dict | None</code>: Resources to be allocated for the predictor.</li> <li>inference_logger <code>hsml.inference_logger.InferenceLogger | dict | str | None</code>: Inference logger configuration.</li> <li>inference_batcher <code>hsml.inference_batcher.InferenceBatcher | dict | None</code>: Inference batcher configuration.</li> <li>transformer <code>hsml.transformer.Transformer | dict | None</code>: Transformer to be deployed together with the predictor.</li> <li>api_protocol <code>str | None</code>: API protocol to be enabled in the deployment (i.e., 'REST' or 'GRPC'). Defaults to 'REST'.</li> </ul> <p>Returns</p> <p><code>Predictor</code>. The predictor metadata object.</p>"},{"location":"generated/model-serving/predictor_api/#retrieval","title":"Retrieval","text":""},{"location":"generated/model-serving/predictor_api/#deploymentpredictor","title":"deployment.predictor","text":"<p>Predictors can be accessed from the deployment metadata objects.</p> <pre><code>deployment.predictor\n</code></pre> <p>To retrieve a deployment, see the Deployment Reference.</p>"},{"location":"generated/model-serving/predictor_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#api_protocol","title":"api_protocol","text":"<p>API protocol enabled in the predictor (e.g., HTTP or GRPC).</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#artifact_files_path","title":"artifact_files_path","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#artifact_path","title":"artifact_path","text":"<p>Path of the model artifact deployed by the predictor. Resolves to /Projects/{project_name}/Models/{name}/{version}/Artifacts/{artifact_version}/{name}{version}.zip</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#artifact_version","title":"artifact_version","text":"<p>Artifact version deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#created_at","title":"created_at","text":"<p>Created at date of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#creator","title":"creator","text":"<p>Creator of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#description","title":"description","text":"<p>Description of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#environment","title":"environment","text":"<p>Name of the inference environment</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#id","title":"id","text":"<p>Id of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#inference_batcher","title":"inference_batcher","text":"<p>Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer).</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#inference_logger","title":"inference_logger","text":"<p>Configuration of the inference logger attached to this predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#model_framework","title":"model_framework","text":"<p>Model framework of the model to be deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#model_name","title":"model_name","text":"<p>Name of the model deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#model_path","title":"model_path","text":"<p>Model path deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#model_server","title":"model_server","text":"<p>Model server used by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#model_version","title":"model_version","text":"<p>Model version deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#name","title":"name","text":"<p>Name of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#project_namespace","title":"project_namespace","text":"<p>Kubernetes project namespace</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#requested_instances","title":"requested_instances","text":"<p>Total number of requested instances in the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#resources","title":"resources","text":"<p>Resource configuration for the deployment component (i.e., predictor or transformer).</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#script_file","title":"script_file","text":"<p>Script file used to load and run the model.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#serving_tool","title":"serving_tool","text":"<p>Serving tool used to run the model server.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#transformer","title":"transformer","text":"<p>Transformer configuration attached to the predictor.</p>"},{"location":"generated/model-serving/predictor_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#deploy","title":"deploy","text":"<pre><code>Predictor.deploy()\n</code></pre> <p>Create a deployment for this predictor and persists it in the Model Serving.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\nmy_deployment = my_predictor.deploy()\n\nprint(my_deployment.get_state())\n</code></pre> <p>Returns</p> <p><code>Deployment</code>. The deployment metadata object of a new or existing deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#describe","title":"describe","text":"<pre><code>Predictor.describe()\n</code></pre> <p>Print a description of the predictor</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#to_dict","title":"to_dict","text":"<pre><code>Predictor.to_dict()\n</code></pre> <p>To be implemented by the component type</p>"},{"location":"generated/model-serving/predictor_state_api/","title":"Deployment state","text":"<p>The state of a deployment corresponds to the state of the predictor configured in it.</p> <p>Note</p> <p>Currently, only one predictor is supported in a deployment. Support for multiple predictors (the inference graphs) is coming soon.</p>"},{"location":"generated/model-serving/predictor_state_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#get_state","title":"get_state","text":"<pre><code>Deployment.get_state()\n</code></pre> <p>Get the current state of the deployment</p> <p>Returns</p> <p><code>PredictorState</code>. The state of the deployment.</p>"},{"location":"generated/model-serving/predictor_state_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#available_predictor_instances","title":"available_predictor_instances","text":"<p>Available predicotr instances.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#available_transformer_instances","title":"available_transformer_instances","text":"<p>Available transformer instances.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#condition","title":"condition","text":"<p>Condition of the current state of predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#deployed","title":"deployed","text":"<p>Whether the predictor is deployed or not.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#hopsworks_inference_path","title":"hopsworks_inference_path","text":"<p>Inference path in the Hopsworks REST API.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#internal_port","title":"internal_port","text":"<p>Internal port for the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#model_server_inference_path","title":"model_server_inference_path","text":"<p>Inference path in the model server</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#revision","title":"revision","text":"<p>Last revision of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#status","title":"status","text":"<p>Status of the predictor.</p>"},{"location":"generated/model-serving/predictor_state_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#describe","title":"describe","text":"<pre><code>PredictorState.describe()\n</code></pre> <p>Print a description of the deployment state</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#to_dict","title":"to_dict","text":"<pre><code>PredictorState.to_dict()\n</code></pre>"},{"location":"generated/model-serving/predictor_state_condition_api/","title":"Deployment state condition","text":"<p>The state condition of a deployment is a more detailed representation of a deployment state.</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#condition","title":"condition","text":"<p>Condition of the current state of predictor.</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#reason","title":"reason","text":"<p>Condition reason of the predictor state.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#status","title":"status","text":"<p>Condition status of the predictor state.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#type","title":"type","text":"<p>Condition type of the predictor state.</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#describe","title":"describe","text":"<pre><code>PredictorStateCondition.describe()\n</code></pre> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#to_dict","title":"to_dict","text":"<pre><code>PredictorStateCondition.to_dict()\n</code></pre>"},{"location":"generated/model-serving/resources_api/","title":"Resources","text":""},{"location":"generated/model-serving/resources_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/model-serving/resources_api/#resources_1","title":"Resources","text":"<pre><code>hsml.resources.Resources(cores, memory, gpus, **kwargs)\n</code></pre> <p>Resource configuration for a predictor or transformer.</p> <p>Arguments</p> <ul> <li>cores <code>int</code>: Number of CPUs.</li> <li>memory <code>int</code>: Memory (MB) resources.</li> <li>gpus <code>int</code>: Number of GPUs.</li> </ul> <p>Returns</p> <p><code>Resources</code>. Resource configuration for a predictor or transformer.</p>"},{"location":"generated/model-serving/resources_api/#retrieval","title":"Retrieval","text":""},{"location":"generated/model-serving/resources_api/#predictorresources","title":"predictor.resources","text":"<p>Resources allocated for a preditor can be accessed from the predictor metadata object.</p> <pre><code>predictor.resources\n</code></pre> <p>Predictors can be found in the deployment metadata objects (see Predictor Reference). To retrieve a deployment, see the Deployment Reference.</p>"},{"location":"generated/model-serving/resources_api/#transformerresources","title":"transformer.resources","text":"<p>Resources allocated for a transformer can be accessed from the transformer metadata object.</p> <pre><code>transformer.resources\n</code></pre> <p>Transformer can be found in the predictor metadata objects (see Predictor Reference).</p>"},{"location":"generated/model-serving/resources_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/resources_api/#cores","title":"cores","text":"<p>Number of CPUs to be allocated per instance</p> <p>[source]</p>"},{"location":"generated/model-serving/resources_api/#gpus","title":"gpus","text":"<p>Number of GPUs to be allocated per instance</p> <p>[source]</p>"},{"location":"generated/model-serving/resources_api/#memory","title":"memory","text":"<p>Memory resources to be allocated per instance</p>"},{"location":"generated/model-serving/resources_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/resources_api/#describe","title":"describe","text":"<pre><code>Resources.describe()\n</code></pre> <p>Print a description of the resource configuration</p> <p>[source]</p>"},{"location":"generated/model-serving/resources_api/#to_dict","title":"to_dict","text":"<pre><code>Resources.to_dict()\n</code></pre>"},{"location":"generated/model-serving/transformer_api/","title":"Transformer","text":""},{"location":"generated/model-serving/transformer_api/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#get_model_serving","title":"get_model_serving","text":"<pre><code>Connection.get_model_serving()\n</code></pre> <p>Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> <p>Returns</p> <p><code>ModelServing</code>. A model serving handle object to perform operations on.</p>"},{"location":"generated/model-serving/transformer_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#create_transformer","title":"create_transformer","text":"<pre><code>ModelServing.create_transformer(script_file=None, resources=None)\n</code></pre> <p>Create a Transformer metadata object.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Dataset API instance\ndataset_api = project.get_dataset_api()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# create my_transformer.py Python script\nclass Transformer(object):\n    def __init__(self):\n        ''' Initialization code goes here '''\n        pass\n\n    def preprocess(self, inputs):\n        ''' Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. '''\n        return inputs\n\n    def postprocess(self, outputs):\n        ''' Transform the predictions computed by the model before returning a response '''\n        return outputs\n\nuploaded_file_path = dataset_api.upload(\"my_transformer.py\", \"Resources\", overwrite=True)\ntransformer_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)\n\nmy_transformer = ms.create_transformer(script_file=uploaded_file_path)\n\n# or\n\nfrom hsml.transformer import Transformer\n\nmy_transformer = Transformer(script_file)\n</code></pre> <p>Create a deployment with the transformer</p> <pre><code>my_predictor = ms.create_predictor(transformer=my_transformer)\nmy_deployment = my_predictor.deploy()\n\n# or\nmy_deployment = ms.create_deployment(my_predictor, transformer=my_transformer)\nmy_deployment.save()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the <code>predictor.transformer</code> property.</p> <p>Arguments</p> <ul> <li>script_file <code>str | None</code>: Path to a custom predictor script implementing the Transformer class.</li> <li>resources <code>hsml.resources.PredictorResources | dict | None</code>: Resources to be allocated for the transformer.</li> </ul> <p>Returns</p> <p><code>Transformer</code>. The model metadata object.</p>"},{"location":"generated/model-serving/transformer_api/#retrieval","title":"Retrieval","text":""},{"location":"generated/model-serving/transformer_api/#predictortransformer","title":"predictor.transformer","text":"<p>Transformers can be accessed from the predictor metadata objects.</p> <pre><code>predictor.transformer\n</code></pre> <p>Predictors can be found in the deployment metadata objects (see Predictor Reference). To retrieve a deployment, see the Deployment Reference.</p>"},{"location":"generated/model-serving/transformer_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#inference_batcher","title":"inference_batcher","text":"<p>Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer).</p> <p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#resources","title":"resources","text":"<p>Resource configuration for the deployment component (i.e., predictor or transformer).</p> <p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#script_file","title":"script_file","text":"<p>Script file ran by the deployment component (i.e., predictor or transformer).</p>"},{"location":"generated/model-serving/transformer_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#describe","title":"describe","text":"<pre><code>Transformer.describe()\n</code></pre> <p>Print a description of the transformer</p> <p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#to_dict","title":"to_dict","text":"<pre><code>Transformer.to_dict()\n</code></pre> <p>To be implemented by the component type</p>"}]}